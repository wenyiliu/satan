2020-11-16 16:14:24  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:14:24  [ main:831 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:14:24  [ main:832 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:14:25  [ main:1036 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:14:25  [ main:1042 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:14:25  [ main:1057 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:14:25  [ main:1145 ] - [ INFO ]  number of splits:1
2020-11-16 16:14:25  [ main:1225 ] - [ INFO ]  Submitting tokens for job: job_local1337895179_0001
2020-11-16 16:14:25  [ main:1337 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:14:25  [ main:1338 ] - [ INFO ]  Running job: job_local1337895179_0001
2020-11-16 16:14:25  [ Thread-18:1338 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:14:25  [ Thread-18:1343 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:14:25  [ Thread-18:1345 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:14:25  [ Thread-18:1392 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1392 ] - [ INFO ]  Starting task: attempt_local1337895179_0001_m_000000_0
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1410 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1416 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1417 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1419 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/partition/partition.txt:0+163
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1471 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1471 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1471 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1471 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1471 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1473 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1551 ] - [ INFO ]  Starting flush of map output
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1551 ] - [ INFO ]  Spilling map output
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1552 ] - [ INFO ]  bufstart = 0; bufend = 224; bufvoid = 104857600
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1552 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600
2020-11-16 16:14:25  [ LocalJobRunner Map Task Executor #0:1561 ] - [ INFO ]  Finished spill 0
2020-11-16 16:14:25  [ Thread-18:1564 ] - [ INFO ]  map task executor complete.
2020-11-16 16:14:25  [ Thread-18:1577 ] - [ WARN ]  job_local1337895179_0001
java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:26)
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:22)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-11-16 16:14:26  [ main:2342 ] - [ INFO ]  Job job_local1337895179_0001 running in uber mode : false
2020-11-16 16:14:26  [ main:2343 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 16:14:26  [ main:2345 ] - [ INFO ]  Job job_local1337895179_0001 failed with state FAILED due to: NA
2020-11-16 16:14:26  [ main:2346 ] - [ INFO ]  Counters: 0
2020-11-16 16:20:44  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:20:44  [ main:545 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:20:44  [ main:546 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:20:44  [ main:724 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:20:44  [ main:728 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:20:44  [ main:741 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:20:44  [ main:807 ] - [ INFO ]  number of splits:1
2020-11-16 16:20:44  [ main:867 ] - [ INFO ]  Submitting tokens for job: job_local385037745_0001
2020-11-16 16:20:44  [ main:948 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:20:44  [ main:949 ] - [ INFO ]  Running job: job_local385037745_0001
2020-11-16 16:20:44  [ Thread-18:949 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:20:44  [ Thread-18:952 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:20:44  [ Thread-18:953 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:20:44  [ Thread-18:989 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:20:44  [ LocalJobRunner Map Task Executor #0:989 ] - [ INFO ]  Starting task: attempt_local385037745_0001_m_000000_0
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1003 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1006 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1007 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1009 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/partition/partition.txt:0+163
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1054 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1054 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1054 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1054 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1055 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1057 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  Starting flush of map output
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  Spilling map output
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  bufstart = 0; bufend = 224; bufvoid = 104857600
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600
2020-11-16 16:20:45  [ LocalJobRunner Map Task Executor #0:1137 ] - [ INFO ]  Finished spill 0
2020-11-16 16:20:45  [ Thread-18:1139 ] - [ INFO ]  map task executor complete.
2020-11-16 16:20:45  [ Thread-18:1148 ] - [ WARN ]  job_local385037745_0001
java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:28)
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:24)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-11-16 16:20:45  [ main:1953 ] - [ INFO ]  Job job_local385037745_0001 running in uber mode : false
2020-11-16 16:20:45  [ main:1954 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 16:20:45  [ main:1955 ] - [ INFO ]  Job job_local385037745_0001 failed with state FAILED due to: NA
2020-11-16 16:20:45  [ main:1957 ] - [ INFO ]  Counters: 0
2020-11-16 16:21:21  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:21:22  [ main:626 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:21:22  [ main:627 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:21:22  [ main:814 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:21:22  [ main:819 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:21:22  [ main:835 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:21:22  [ main:928 ] - [ INFO ]  number of splits:1
2020-11-16 16:21:22  [ main:996 ] - [ INFO ]  Submitting tokens for job: job_local1527620216_0001
2020-11-16 16:21:22  [ main:1088 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:21:22  [ main:1088 ] - [ INFO ]  Running job: job_local1527620216_0001
2020-11-16 16:21:22  [ Thread-18:1089 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:21:22  [ Thread-18:1092 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:21:22  [ Thread-18:1093 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:21:22  [ Thread-18:1131 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1131 ] - [ INFO ]  Starting task: attempt_local1527620216_0001_m_000000_0
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1145 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1149 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1149 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/partition/partition.txt:0+163
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1197 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1197 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1197 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1197 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1197 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1200 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1278 ] - [ INFO ]  Starting flush of map output
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1278 ] - [ INFO ]  Spilling map output
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1278 ] - [ INFO ]  bufstart = 0; bufend = 224; bufvoid = 104857600
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1278 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600
2020-11-16 16:21:22  [ LocalJobRunner Map Task Executor #0:1289 ] - [ INFO ]  Finished spill 0
2020-11-16 16:21:22  [ Thread-18:1292 ] - [ INFO ]  map task executor complete.
2020-11-16 16:21:22  [ Thread-18:1300 ] - [ WARN ]  job_local1527620216_0001
java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:29)
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:24)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-11-16 16:21:23  [ main:2090 ] - [ INFO ]  Job job_local1527620216_0001 running in uber mode : false
2020-11-16 16:21:23  [ main:2091 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 16:21:23  [ main:2092 ] - [ INFO ]  Job job_local1527620216_0001 failed with state FAILED due to: NA
2020-11-16 16:21:23  [ main:2094 ] - [ INFO ]  Counters: 0
2020-11-16 16:21:54  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:21:55  [ main:559 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:21:55  [ main:559 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:21:55  [ main:767 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:21:55  [ main:773 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:21:55  [ main:786 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:21:55  [ main:872 ] - [ INFO ]  number of splits:1
2020-11-16 16:21:55  [ main:938 ] - [ INFO ]  Submitting tokens for job: job_local1054572483_0001
2020-11-16 16:21:55  [ main:1027 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:21:55  [ main:1028 ] - [ INFO ]  Running job: job_local1054572483_0001
2020-11-16 16:21:55  [ Thread-18:1028 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:21:55  [ Thread-18:1031 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:21:55  [ Thread-18:1032 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:21:55  [ Thread-18:1084 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1085 ] - [ INFO ]  Starting task: attempt_local1054572483_0001_m_000000_0
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1101 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1107 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/partition/partition.txt:0+163
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1160 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1160 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:21:55  [ LocalJobRunner Map Task Executor #0:1163 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 16:21:56  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  Starting flush of map output
2020-11-16 16:21:56  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  Spilling map output
2020-11-16 16:21:56  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  bufstart = 0; bufend = 224; bufvoid = 104857600
2020-11-16 16:21:56  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600
2020-11-16 16:21:56  [ LocalJobRunner Map Task Executor #0:1272 ] - [ INFO ]  Finished spill 0
2020-11-16 16:21:56  [ Thread-18:1275 ] - [ INFO ]  map task executor complete.
2020-11-16 16:21:56  [ Thread-18:1283 ] - [ WARN ]  job_local1054572483_0001
java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:28)
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:24)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-11-16 16:21:56  [ main:2033 ] - [ INFO ]  Job job_local1054572483_0001 running in uber mode : false
2020-11-16 16:21:56  [ main:2034 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 16:21:56  [ main:2035 ] - [ INFO ]  Job job_local1054572483_0001 failed with state FAILED due to: NA
2020-11-16 16:21:56  [ main:2037 ] - [ INFO ]  Counters: 0
2020-11-16 16:22:51  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:22:51  [ main:570 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:22:51  [ main:570 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:22:52  [ main:983 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:22:52  [ main:989 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:22:52  [ main:1023 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:22:52  [ main:1103 ] - [ INFO ]  number of splits:1
2020-11-16 16:22:52  [ main:1164 ] - [ INFO ]  Submitting tokens for job: job_local2136502495_0001
2020-11-16 16:22:52  [ main:1250 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:22:52  [ main:1250 ] - [ INFO ]  Running job: job_local2136502495_0001
2020-11-16 16:22:52  [ Thread-18:1251 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:22:52  [ Thread-18:1254 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:22:52  [ Thread-18:1255 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:22:52  [ Thread-18:1295 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1296 ] - [ INFO ]  Starting task: attempt_local2136502495_0001_m_000000_0
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1311 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1315 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1315 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1317 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/partition/partition.txt:0+163
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1369 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1369 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1369 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1369 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1369 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1372 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1456 ] - [ INFO ]  Starting flush of map output
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1456 ] - [ INFO ]  Spilling map output
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1456 ] - [ INFO ]  bufstart = 0; bufend = 224; bufvoid = 104857600
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1456 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600
2020-11-16 16:22:52  [ LocalJobRunner Map Task Executor #0:1465 ] - [ INFO ]  Finished spill 0
2020-11-16 16:22:52  [ Thread-18:1468 ] - [ INFO ]  map task executor complete.
2020-11-16 16:22:52  [ Thread-18:1493 ] - [ WARN ]  job_local2136502495_0001
java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:29)
	at com.satan.hadoop.mr.ConsumerMapReduceJob$ConsumerMapper.map(MapReduceConsumerJob.java:24)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-11-16 16:22:53  [ main:2253 ] - [ INFO ]  Job job_local2136502495_0001 running in uber mode : false
2020-11-16 16:22:53  [ main:2254 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 16:22:53  [ main:2256 ] - [ INFO ]  Job job_local2136502495_0001 failed with state FAILED due to: NA
2020-11-16 16:22:53  [ main:2258 ] - [ INFO ]  Counters: 0
2020-11-16 16:23:37  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:23:38  [ main:611 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:23:38  [ main:612 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:23:38  [ main:803 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:23:38  [ main:807 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:23:38  [ main:822 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:23:38  [ main:904 ] - [ INFO ]  number of splits:1
2020-11-16 16:23:38  [ main:977 ] - [ INFO ]  Submitting tokens for job: job_local1686498090_0001
2020-11-16 16:23:38  [ main:1071 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:23:38  [ main:1072 ] - [ INFO ]  Running job: job_local1686498090_0001
2020-11-16 16:23:38  [ Thread-18:1072 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:23:38  [ Thread-18:1075 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:23:38  [ Thread-18:1076 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:23:38  [ Thread-18:1119 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1119 ] - [ INFO ]  Starting task: attempt_local1686498090_0001_m_000000_0
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1134 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1138 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1138 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1140 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/partition/partition.txt:0+163
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:23:38  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1300 ] - [ INFO ]  
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1302 ] - [ INFO ]  Starting flush of map output
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1302 ] - [ INFO ]  Spilling map output
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1302 ] - [ INFO ]  bufstart = 0; bufend = 224; bufvoid = 104857600
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1302 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214372(104857488); length = 25/6553600
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1313 ] - [ INFO ]  Finished spill 0
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1316 ] - [ INFO ]  Task:attempt_local1686498090_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1331 ] - [ INFO ]  map
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1331 ] - [ INFO ]  Task 'attempt_local1686498090_0001_m_000000_0' done.
2020-11-16 16:23:39  [ LocalJobRunner Map Task Executor #0:1331 ] - [ INFO ]  Finishing task: attempt_local1686498090_0001_m_000000_0
2020-11-16 16:23:39  [ Thread-18:1331 ] - [ INFO ]  map task executor complete.
2020-11-16 16:23:39  [ Thread-18:1335 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 16:23:39  [ pool-6-thread-1:1335 ] - [ INFO ]  Starting task: attempt_local1686498090_0001_r_000000_0
2020-11-16 16:23:39  [ pool-6-thread-1:1340 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:23:39  [ pool-6-thread-1:1341 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:23:39  [ pool-6-thread-1:1341 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:23:39  [ pool-6-thread-1:1344 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f3fe43c
2020-11-16 16:23:39  [ pool-6-thread-1:1354 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 16:23:39  [ EventFetcher for fetching Map Completion Events:1356 ] - [ INFO ]  attempt_local1686498090_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 16:23:39  [ localfetcher#1:1379 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1686498090_0001_m_000000_0 decomp: 36 len: 40 to MEMORY
2020-11-16 16:23:39  [ localfetcher#1:1383 ] - [ INFO ]  Read 36 bytes from map-output for attempt_local1686498090_0001_m_000000_0
2020-11-16 16:23:39  [ localfetcher#1:1384 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 36, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->36
2020-11-16 16:23:39  [ EventFetcher for fetching Map Completion Events:1385 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 16:23:39  [ pool-6-thread-1:1385 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:39  [ pool-6-thread-1:1385 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 16:23:39  [ pool-6-thread-1:1389 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:39  [ pool-6-thread-1:1390 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 31 bytes
2020-11-16 16:23:39  [ pool-6-thread-1:1390 ] - [ INFO ]  Merged 1 segments, 36 bytes to disk to satisfy reduce memory limit
2020-11-16 16:23:39  [ pool-6-thread-1:1391 ] - [ INFO ]  Merging 1 files, 40 bytes from disk
2020-11-16 16:23:39  [ pool-6-thread-1:1391 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 16:23:39  [ pool-6-thread-1:1391 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:39  [ pool-6-thread-1:1391 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 31 bytes
2020-11-16 16:23:39  [ pool-6-thread-1:1392 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:39  [ pool-6-thread-1:1414 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 16:23:39  [ pool-6-thread-1:1735 ] - [ INFO ]  Task:attempt_local1686498090_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 16:23:39  [ pool-6-thread-1:1804 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:39  [ pool-6-thread-1:1804 ] - [ INFO ]  Task attempt_local1686498090_0001_r_000000_0 is allowed to commit now
2020-11-16 16:23:39  [ pool-6-thread-1:1929 ] - [ INFO ]  Saved output of task 'attempt_local1686498090_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/partition/result/_temporary/0/task_local1686498090_0001_r_000000
2020-11-16 16:23:39  [ pool-6-thread-1:1930 ] - [ INFO ]  reduce > reduce
2020-11-16 16:23:39  [ pool-6-thread-1:1930 ] - [ INFO ]  Task 'attempt_local1686498090_0001_r_000000_0' done.
2020-11-16 16:23:39  [ pool-6-thread-1:1930 ] - [ INFO ]  Finishing task: attempt_local1686498090_0001_r_000000_0
2020-11-16 16:23:39  [ pool-6-thread-1:1931 ] - [ INFO ]  Starting task: attempt_local1686498090_0001_r_000001_0
2020-11-16 16:23:39  [ pool-6-thread-1:1932 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:23:39  [ pool-6-thread-1:1932 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:23:39  [ pool-6-thread-1:1932 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:23:39  [ pool-6-thread-1:1932 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7d2ad7ce
2020-11-16 16:23:39  [ pool-6-thread-1:1933 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 16:23:39  [ EventFetcher for fetching Map Completion Events:1933 ] - [ INFO ]  attempt_local1686498090_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 16:23:39  [ localfetcher#2:1935 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local1686498090_0001_m_000000_0 decomp: 70 len: 74 to MEMORY
2020-11-16 16:23:39  [ localfetcher#2:1935 ] - [ INFO ]  Read 70 bytes from map-output for attempt_local1686498090_0001_m_000000_0
2020-11-16 16:23:39  [ localfetcher#2:1935 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 70, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->70
2020-11-16 16:23:39  [ EventFetcher for fetching Map Completion Events:1935 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 16:23:39  [ pool-6-thread-1:1936 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:39  [ pool-6-thread-1:1936 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 16:23:39  [ pool-6-thread-1:1937 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:39  [ pool-6-thread-1:1937 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 65 bytes
2020-11-16 16:23:39  [ pool-6-thread-1:1938 ] - [ INFO ]  Merged 1 segments, 70 bytes to disk to satisfy reduce memory limit
2020-11-16 16:23:39  [ pool-6-thread-1:1938 ] - [ INFO ]  Merging 1 files, 74 bytes from disk
2020-11-16 16:23:39  [ pool-6-thread-1:1938 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 16:23:39  [ pool-6-thread-1:1938 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:39  [ pool-6-thread-1:1939 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 65 bytes
2020-11-16 16:23:39  [ pool-6-thread-1:1939 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:39  [ main:2074 ] - [ INFO ]  Job job_local1686498090_0001 running in uber mode : false
2020-11-16 16:23:39  [ main:2076 ] - [ INFO ]   map 100% reduce 25%
2020-11-16 16:23:39  [ pool-6-thread-1:2117 ] - [ INFO ]  Task:attempt_local1686498090_0001_r_000001_0 is done. And is in the process of committing
2020-11-16 16:23:39  [ pool-6-thread-1:2129 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:39  [ pool-6-thread-1:2129 ] - [ INFO ]  Task attempt_local1686498090_0001_r_000001_0 is allowed to commit now
2020-11-16 16:23:39  [ pool-6-thread-1:2191 ] - [ INFO ]  Saved output of task 'attempt_local1686498090_0001_r_000001_0' to hdfs://master:9000/user/root/mr/data/partition/result/_temporary/0/task_local1686498090_0001_r_000001
2020-11-16 16:23:39  [ pool-6-thread-1:2191 ] - [ INFO ]  reduce > reduce
2020-11-16 16:23:39  [ pool-6-thread-1:2191 ] - [ INFO ]  Task 'attempt_local1686498090_0001_r_000001_0' done.
2020-11-16 16:23:39  [ pool-6-thread-1:2191 ] - [ INFO ]  Finishing task: attempt_local1686498090_0001_r_000001_0
2020-11-16 16:23:39  [ pool-6-thread-1:2191 ] - [ INFO ]  Starting task: attempt_local1686498090_0001_r_000002_0
2020-11-16 16:23:39  [ pool-6-thread-1:2192 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:23:39  [ pool-6-thread-1:2192 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:23:39  [ pool-6-thread-1:2193 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:23:39  [ pool-6-thread-1:2193 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@99d88c4
2020-11-16 16:23:39  [ pool-6-thread-1:2193 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 16:23:39  [ EventFetcher for fetching Map Completion Events:2193 ] - [ INFO ]  attempt_local1686498090_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 16:23:39  [ localfetcher#3:2195 ] - [ INFO ]  localfetcher#3 about to shuffle output of map attempt_local1686498090_0001_m_000000_0 decomp: 36 len: 40 to MEMORY
2020-11-16 16:23:39  [ localfetcher#3:2195 ] - [ INFO ]  Read 36 bytes from map-output for attempt_local1686498090_0001_m_000000_0
2020-11-16 16:23:39  [ localfetcher#3:2195 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 36, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->36
2020-11-16 16:23:39  [ EventFetcher for fetching Map Completion Events:2195 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 16:23:39  [ pool-6-thread-1:2196 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:39  [ pool-6-thread-1:2196 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 16:23:39  [ pool-6-thread-1:2197 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:39  [ pool-6-thread-1:2197 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 31 bytes
2020-11-16 16:23:39  [ pool-6-thread-1:2198 ] - [ INFO ]  Merged 1 segments, 36 bytes to disk to satisfy reduce memory limit
2020-11-16 16:23:39  [ pool-6-thread-1:2198 ] - [ INFO ]  Merging 1 files, 40 bytes from disk
2020-11-16 16:23:39  [ pool-6-thread-1:2198 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 16:23:39  [ pool-6-thread-1:2198 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:39  [ pool-6-thread-1:2198 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 31 bytes
2020-11-16 16:23:39  [ pool-6-thread-1:2198 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:40  [ pool-6-thread-1:2457 ] - [ INFO ]  Task:attempt_local1686498090_0001_r_000002_0 is done. And is in the process of committing
2020-11-16 16:23:40  [ pool-6-thread-1:2488 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:40  [ pool-6-thread-1:2488 ] - [ INFO ]  Task attempt_local1686498090_0001_r_000002_0 is allowed to commit now
2020-11-16 16:23:40  [ pool-6-thread-1:2535 ] - [ INFO ]  Saved output of task 'attempt_local1686498090_0001_r_000002_0' to hdfs://master:9000/user/root/mr/data/partition/result/_temporary/0/task_local1686498090_0001_r_000002
2020-11-16 16:23:40  [ pool-6-thread-1:2535 ] - [ INFO ]  reduce > reduce
2020-11-16 16:23:40  [ pool-6-thread-1:2536 ] - [ INFO ]  Task 'attempt_local1686498090_0001_r_000002_0' done.
2020-11-16 16:23:40  [ pool-6-thread-1:2536 ] - [ INFO ]  Finishing task: attempt_local1686498090_0001_r_000002_0
2020-11-16 16:23:40  [ pool-6-thread-1:2536 ] - [ INFO ]  Starting task: attempt_local1686498090_0001_r_000003_0
2020-11-16 16:23:40  [ pool-6-thread-1:2536 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:23:40  [ pool-6-thread-1:2537 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:23:40  [ pool-6-thread-1:2537 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:23:40  [ pool-6-thread-1:2537 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@12159f07
2020-11-16 16:23:40  [ pool-6-thread-1:2537 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 16:23:40  [ EventFetcher for fetching Map Completion Events:2538 ] - [ INFO ]  attempt_local1686498090_0001_r_000003_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 16:23:40  [ localfetcher#4:2539 ] - [ INFO ]  localfetcher#4 about to shuffle output of map attempt_local1686498090_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
2020-11-16 16:23:40  [ localfetcher#4:2540 ] - [ INFO ]  Read 2 bytes from map-output for attempt_local1686498090_0001_m_000000_0
2020-11-16 16:23:40  [ localfetcher#4:2540 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
2020-11-16 16:23:40  [ EventFetcher for fetching Map Completion Events:2540 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 16:23:40  [ pool-6-thread-1:2540 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:40  [ pool-6-thread-1:2540 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 16:23:40  [ pool-6-thread-1:2541 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:40  [ pool-6-thread-1:2541 ] - [ INFO ]  Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2020-11-16 16:23:40  [ pool-6-thread-1:2542 ] - [ INFO ]  Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
2020-11-16 16:23:40  [ pool-6-thread-1:2542 ] - [ INFO ]  Merging 1 files, 6 bytes from disk
2020-11-16 16:23:40  [ pool-6-thread-1:2542 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 16:23:40  [ pool-6-thread-1:2542 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:23:40  [ pool-6-thread-1:2542 ] - [ INFO ]  Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2020-11-16 16:23:40  [ pool-6-thread-1:2543 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:40  [ pool-6-thread-1:2563 ] - [ INFO ]  Task:attempt_local1686498090_0001_r_000003_0 is done. And is in the process of committing
2020-11-16 16:23:40  [ pool-6-thread-1:2578 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:23:40  [ pool-6-thread-1:2579 ] - [ INFO ]  Task attempt_local1686498090_0001_r_000003_0 is allowed to commit now
2020-11-16 16:23:40  [ pool-6-thread-1:2606 ] - [ INFO ]  Saved output of task 'attempt_local1686498090_0001_r_000003_0' to hdfs://master:9000/user/root/mr/data/partition/result/_temporary/0/task_local1686498090_0001_r_000003
2020-11-16 16:23:40  [ pool-6-thread-1:2607 ] - [ INFO ]  reduce > reduce
2020-11-16 16:23:40  [ pool-6-thread-1:2607 ] - [ INFO ]  Task 'attempt_local1686498090_0001_r_000003_0' done.
2020-11-16 16:23:40  [ pool-6-thread-1:2607 ] - [ INFO ]  Finishing task: attempt_local1686498090_0001_r_000003_0
2020-11-16 16:23:40  [ Thread-18:2607 ] - [ INFO ]  reduce task executor complete.
2020-11-16 16:23:40  [ main:3079 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 16:23:40  [ main:3079 ] - [ INFO ]  Job job_local1686498090_0001 completed successfully
2020-11-16 16:23:40  [ main:3090 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=3491
		FILE: Number of bytes written=1428593
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=815
		HDFS: Number of bytes written=893
		HDFS: Number of read operations=65
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=30
	Map-Reduce Framework
		Map input records=8
		Map output records=7
		Map output bytes=224
		Map output materialized bytes=160
		Input split bytes=125
		Combine input records=7
		Combine output records=4
		Reduce input groups=4
		Reduce shuffle bytes=160
		Reduce input records=4
		Reduce output records=4
		Spilled Records=8
		Shuffled Maps =4
		Failed Shuffles=0
		Merged Map outputs=4
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=1580728320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=163
	File Output Format Counters 
		Bytes Written=298
2020-11-16 16:47:34  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:48:07  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:49:22  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:49:22  [ main:524 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:49:22  [ main:524 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:49:23  [ main:710 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:49:23  [ main:716 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:49:23  [ main:728 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:49:23  [ main:810 ] - [ INFO ]  number of splits:1
2020-11-16 16:49:23  [ main:883 ] - [ INFO ]  Submitting tokens for job: job_local1076229705_0001
2020-11-16 16:49:23  [ main:981 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:49:23  [ main:982 ] - [ INFO ]  Running job: job_local1076229705_0001
2020-11-16 16:49:23  [ Thread-18:982 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:49:23  [ Thread-18:985 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:49:23  [ Thread-18:986 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:49:23  [ Thread-18:1019 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1019 ] - [ INFO ]  Starting task: attempt_local1076229705_0001_m_000000_0
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1033 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1036 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1037 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1039 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/sort_test/sort.txt:0+113
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1083 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1084 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1084 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1084 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1084 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:49:23  [ LocalJobRunner Map Task Executor #0:1085 ] - [ WARN ]  Unable to initialize MapOutputCollector org.apache.hadoop.mapred.MapTask$MapOutputBuffer
java.lang.RuntimeException: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:134)
	at org.apache.hadoop.io.WritableComparator.newKey(WritableComparator.java:144)
	at org.apache.hadoop.io.WritableComparator.<init>(WritableComparator.java:130)
	at org.apache.hadoop.io.WritableComparator.get(WritableComparator.java:65)
	at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004)
	at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402)
	at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:698)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at java.lang.Class.getConstructor0(Class.java:3082)
	at java.lang.Class.getDeclaredConstructor(Class.java:2178)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:128)
	... 16 more
2020-11-16 16:49:23  [ Thread-18:1087 ] - [ INFO ]  map task executor complete.
2020-11-16 16:49:23  [ Thread-18:1096 ] - [ WARN ]  job_local1076229705_0001
java.lang.Exception: java.io.IOException: Initialization of all the collectors failed. Error in last collector was :java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.io.IOException: Initialization of all the collectors failed. Error in last collector was :java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:415)
	at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:698)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:134)
	at org.apache.hadoop.io.WritableComparator.newKey(WritableComparator.java:144)
	at org.apache.hadoop.io.WritableComparator.<init>(WritableComparator.java:130)
	at org.apache.hadoop.io.WritableComparator.get(WritableComparator.java:65)
	at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004)
	at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402)
	... 10 more
Caused by: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at java.lang.Class.getConstructor0(Class.java:3082)
	at java.lang.Class.getDeclaredConstructor(Class.java:2178)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:128)
	... 16 more
2020-11-16 16:49:24  [ main:1983 ] - [ INFO ]  Job job_local1076229705_0001 running in uber mode : false
2020-11-16 16:49:24  [ main:1984 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 16:49:24  [ main:1986 ] - [ INFO ]  Job job_local1076229705_0001 failed with state FAILED due to: NA
2020-11-16 16:49:24  [ main:1989 ] - [ INFO ]  Counters: 0
2020-11-16 16:49:56  [ main:1 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:49:56  [ main:602 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:49:56  [ main:603 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:49:57  [ main:797 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:49:57  [ main:802 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:49:57  [ main:816 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:49:57  [ main:891 ] - [ INFO ]  number of splits:1
2020-11-16 16:49:57  [ main:960 ] - [ INFO ]  Submitting tokens for job: job_local1242922046_0001
2020-11-16 16:49:57  [ main:1058 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:49:57  [ main:1059 ] - [ INFO ]  Running job: job_local1242922046_0001
2020-11-16 16:49:57  [ Thread-18:1059 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:49:57  [ Thread-18:1062 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:49:57  [ Thread-18:1064 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:49:57  [ Thread-18:1104 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1104 ] - [ INFO ]  Starting task: attempt_local1242922046_0001_m_000000_0
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1120 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1123 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1124 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1126 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/sort_test/sort.txt:0+113
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1175 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1175 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1175 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1176 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1176 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:49:57  [ LocalJobRunner Map Task Executor #0:1176 ] - [ WARN ]  Unable to initialize MapOutputCollector org.apache.hadoop.mapred.MapTask$MapOutputBuffer
java.lang.RuntimeException: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:134)
	at org.apache.hadoop.io.WritableComparator.newKey(WritableComparator.java:144)
	at org.apache.hadoop.io.WritableComparator.<init>(WritableComparator.java:130)
	at org.apache.hadoop.io.WritableComparator.get(WritableComparator.java:65)
	at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004)
	at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402)
	at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:698)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at java.lang.Class.getConstructor0(Class.java:3082)
	at java.lang.Class.getDeclaredConstructor(Class.java:2178)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:128)
	... 16 more
2020-11-16 16:49:57  [ Thread-18:1178 ] - [ INFO ]  map task executor complete.
2020-11-16 16:49:57  [ Thread-18:1197 ] - [ WARN ]  job_local1242922046_0001
java.lang.Exception: java.io.IOException: Initialization of all the collectors failed. Error in last collector was :java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.io.IOException: Initialization of all the collectors failed. Error in last collector was :java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:415)
	at org.apache.hadoop.mapred.MapTask.access$100(MapTask.java:81)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:698)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:134)
	at org.apache.hadoop.io.WritableComparator.newKey(WritableComparator.java:144)
	at org.apache.hadoop.io.WritableComparator.<init>(WritableComparator.java:130)
	at org.apache.hadoop.io.WritableComparator.get(WritableComparator.java:65)
	at org.apache.hadoop.mapred.JobConf.getOutputKeyComparator(JobConf.java:887)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.init(MapTask.java:1004)
	at org.apache.hadoop.mapred.MapTask.createSortingCollector(MapTask.java:402)
	... 10 more
Caused by: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.Movie.<init>()
	at java.lang.Class.getConstructor0(Class.java:3082)
	at java.lang.Class.getDeclaredConstructor(Class.java:2178)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:128)
	... 16 more
2020-11-16 16:49:58  [ main:2061 ] - [ INFO ]  Job job_local1242922046_0001 running in uber mode : false
2020-11-16 16:49:58  [ main:2062 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 16:49:58  [ main:2063 ] - [ INFO ]  Job job_local1242922046_0001 failed with state FAILED due to: NA
2020-11-16 16:49:58  [ main:2066 ] - [ INFO ]  Counters: 0
2020-11-16 16:50:11  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 16:50:11  [ main:574 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 16:50:11  [ main:574 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 16:50:12  [ main:772 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 16:50:12  [ main:778 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 16:50:12  [ main:788 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 16:50:12  [ main:853 ] - [ INFO ]  number of splits:1
2020-11-16 16:50:12  [ main:923 ] - [ INFO ]  Submitting tokens for job: job_local451483396_0001
2020-11-16 16:50:12  [ main:1025 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 16:50:12  [ main:1025 ] - [ INFO ]  Running job: job_local451483396_0001
2020-11-16 16:50:12  [ Thread-18:1026 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 16:50:12  [ Thread-18:1029 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:50:12  [ Thread-18:1030 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 16:50:12  [ Thread-18:1069 ] - [ INFO ]  Waiting for map tasks
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1070 ] - [ INFO ]  Starting task: attempt_local451483396_0001_m_000000_0
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1085 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1089 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1089 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1091 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/sort_test/sort.txt:0+113
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  soft limit at 83886080
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1146 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1394 ] - [ INFO ]  
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1396 ] - [ INFO ]  Starting flush of map output
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1396 ] - [ INFO ]  Spilling map output
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1396 ] - [ INFO ]  bufstart = 0; bufend = 134; bufvoid = 104857600
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1396 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214356(104857424); length = 41/6553600
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1401 ] - [ INFO ]  Finished spill 0
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1404 ] - [ INFO ]  Task:attempt_local451483396_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1425 ] - [ INFO ]  map
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1425 ] - [ INFO ]  Task 'attempt_local451483396_0001_m_000000_0' done.
2020-11-16 16:50:12  [ LocalJobRunner Map Task Executor #0:1426 ] - [ INFO ]  Finishing task: attempt_local451483396_0001_m_000000_0
2020-11-16 16:50:12  [ Thread-18:1426 ] - [ INFO ]  map task executor complete.
2020-11-16 16:50:12  [ Thread-18:1427 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 16:50:12  [ pool-6-thread-1:1428 ] - [ INFO ]  Starting task: attempt_local451483396_0001_r_000000_0
2020-11-16 16:50:12  [ pool-6-thread-1:1432 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 16:50:12  [ pool-6-thread-1:1432 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 16:50:12  [ pool-6-thread-1:1432 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 16:50:12  [ pool-6-thread-1:1434 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@405d619c
2020-11-16 16:50:12  [ pool-6-thread-1:1441 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 16:50:12  [ EventFetcher for fetching Map Completion Events:1443 ] - [ INFO ]  attempt_local451483396_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 16:50:12  [ localfetcher#1:1462 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local451483396_0001_m_000000_0 decomp: 158 len: 162 to MEMORY
2020-11-16 16:50:12  [ localfetcher#1:1466 ] - [ INFO ]  Read 158 bytes from map-output for attempt_local451483396_0001_m_000000_0
2020-11-16 16:50:12  [ localfetcher#1:1467 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 158, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->158
2020-11-16 16:50:12  [ EventFetcher for fetching Map Completion Events:1468 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 16:50:12  [ pool-6-thread-1:1469 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:50:12  [ pool-6-thread-1:1469 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 16:50:12  [ pool-6-thread-1:1473 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:50:12  [ pool-6-thread-1:1474 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 144 bytes
2020-11-16 16:50:12  [ pool-6-thread-1:1474 ] - [ INFO ]  Merged 1 segments, 158 bytes to disk to satisfy reduce memory limit
2020-11-16 16:50:12  [ pool-6-thread-1:1475 ] - [ INFO ]  Merging 1 files, 162 bytes from disk
2020-11-16 16:50:12  [ pool-6-thread-1:1475 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 16:50:12  [ pool-6-thread-1:1475 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 16:50:12  [ pool-6-thread-1:1475 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 144 bytes
2020-11-16 16:50:12  [ pool-6-thread-1:1476 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:50:12  [ pool-6-thread-1:1561 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 16:50:13  [ pool-6-thread-1:1656 ] - [ INFO ]  Task:attempt_local451483396_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 16:50:13  [ pool-6-thread-1:1663 ] - [ INFO ]  1 / 1 copied.
2020-11-16 16:50:13  [ pool-6-thread-1:1663 ] - [ INFO ]  Task attempt_local451483396_0001_r_000000_0 is allowed to commit now
2020-11-16 16:50:13  [ pool-6-thread-1:1683 ] - [ INFO ]  Saved output of task 'attempt_local451483396_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/sort_test/result/_temporary/0/task_local451483396_0001_r_000000
2020-11-16 16:50:13  [ pool-6-thread-1:1683 ] - [ INFO ]  reduce > reduce
2020-11-16 16:50:13  [ pool-6-thread-1:1684 ] - [ INFO ]  Task 'attempt_local451483396_0001_r_000000_0' done.
2020-11-16 16:50:13  [ pool-6-thread-1:1684 ] - [ INFO ]  Finishing task: attempt_local451483396_0001_r_000000_0
2020-11-16 16:50:13  [ Thread-18:1684 ] - [ INFO ]  reduce task executor complete.
2020-11-16 16:50:13  [ main:2029 ] - [ INFO ]  Job job_local451483396_0001 running in uber mode : false
2020-11-16 16:50:13  [ main:2030 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 16:50:13  [ main:2031 ] - [ INFO ]  Job job_local451483396_0001 completed successfully
2020-11-16 16:50:13  [ main:2039 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=698
		FILE: Number of bytes written=563196
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=226
		HDFS: Number of bytes written=321
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=11
		Map output records=11
		Map output bytes=134
		Map output materialized bytes=162
		Input split bytes=120
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=162
		Reduce input records=11
		Reduce output records=11
		Spilled Records=22
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=113
	File Output Format Counters 
		Bytes Written=321
2020-11-16 17:39:43  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 17:40:31  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 17:40:31  [ main:547 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 17:40:31  [ main:548 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 17:40:32  [ main:744 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 17:40:32  [ main:749 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 17:40:32  [ main:763 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 17:40:32  [ main:837 ] - [ INFO ]  number of splits:1
2020-11-16 17:40:32  [ main:900 ] - [ INFO ]  Submitting tokens for job: job_local1121340975_0001
2020-11-16 17:40:32  [ main:998 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 17:40:32  [ main:999 ] - [ INFO ]  Running job: job_local1121340975_0001
2020-11-16 17:40:32  [ Thread-18:999 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 17:40:32  [ Thread-18:1003 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:40:32  [ Thread-18:1004 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 17:40:32  [ Thread-18:1046 ] - [ INFO ]  Waiting for map tasks
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1047 ] - [ INFO ]  Starting task: attempt_local1121340975_0001_m_000000_0
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1062 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1066 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1066 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1068 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/sort_test/sort2.txt:0+200
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1117 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1117 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1117 ] - [ INFO ]  soft limit at 83886080
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1117 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1117 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1119 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1195 ] - [ INFO ]  
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  Starting flush of map output
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  Spilling map output
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  bufstart = 0; bufend = 392; bufvoid = 104857600
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214208(104856832); length = 189/6553600
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1202 ] - [ INFO ]  Finished spill 0
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  Task:attempt_local1121340975_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1222 ] - [ INFO ]  map
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1222 ] - [ INFO ]  Task 'attempt_local1121340975_0001_m_000000_0' done.
2020-11-16 17:40:32  [ LocalJobRunner Map Task Executor #0:1222 ] - [ INFO ]  Finishing task: attempt_local1121340975_0001_m_000000_0
2020-11-16 17:40:32  [ Thread-18:1222 ] - [ INFO ]  map task executor complete.
2020-11-16 17:40:32  [ Thread-18:1224 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 17:40:32  [ pool-6-thread-1:1224 ] - [ INFO ]  Starting task: attempt_local1121340975_0001_r_000000_0
2020-11-16 17:40:32  [ pool-6-thread-1:1228 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:40:32  [ pool-6-thread-1:1228 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 17:40:32  [ pool-6-thread-1:1228 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 17:40:32  [ pool-6-thread-1:1230 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4c0a1e33
2020-11-16 17:40:32  [ pool-6-thread-1:1238 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 17:40:32  [ EventFetcher for fetching Map Completion Events:1239 ] - [ INFO ]  attempt_local1121340975_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 17:40:32  [ localfetcher#1:1259 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1121340975_0001_m_000000_0 decomp: 490 len: 494 to MEMORY
2020-11-16 17:40:32  [ localfetcher#1:1263 ] - [ INFO ]  Read 490 bytes from map-output for attempt_local1121340975_0001_m_000000_0
2020-11-16 17:40:32  [ localfetcher#1:1264 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 490, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->490
2020-11-16 17:40:32  [ EventFetcher for fetching Map Completion Events:1265 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 17:40:32  [ pool-6-thread-1:1266 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:40:32  [ pool-6-thread-1:1266 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 17:40:32  [ pool-6-thread-1:1270 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:40:32  [ pool-6-thread-1:1270 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 487 bytes
2020-11-16 17:40:32  [ pool-6-thread-1:1271 ] - [ INFO ]  Merged 1 segments, 490 bytes to disk to satisfy reduce memory limit
2020-11-16 17:40:32  [ pool-6-thread-1:1271 ] - [ INFO ]  Merging 1 files, 494 bytes from disk
2020-11-16 17:40:32  [ pool-6-thread-1:1271 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 17:40:32  [ pool-6-thread-1:1271 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:40:32  [ pool-6-thread-1:1272 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 487 bytes
2020-11-16 17:40:32  [ pool-6-thread-1:1272 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:40:32  [ pool-6-thread-1:1294 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 17:40:33  [ pool-6-thread-1:1737 ] - [ INFO ]  Task:attempt_local1121340975_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 17:40:33  [ pool-6-thread-1:1746 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:40:33  [ pool-6-thread-1:1746 ] - [ INFO ]  Task attempt_local1121340975_0001_r_000000_0 is allowed to commit now
2020-11-16 17:40:33  [ pool-6-thread-1:1777 ] - [ INFO ]  Saved output of task 'attempt_local1121340975_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/sort_test/result2/_temporary/0/task_local1121340975_0001_r_000000
2020-11-16 17:40:33  [ pool-6-thread-1:1778 ] - [ INFO ]  reduce > reduce
2020-11-16 17:40:33  [ pool-6-thread-1:1778 ] - [ INFO ]  Task 'attempt_local1121340975_0001_r_000000_0' done.
2020-11-16 17:40:33  [ pool-6-thread-1:1778 ] - [ INFO ]  Finishing task: attempt_local1121340975_0001_r_000000_0
2020-11-16 17:40:33  [ Thread-18:1778 ] - [ INFO ]  reduce task executor complete.
2020-11-16 17:40:33  [ main:2001 ] - [ INFO ]  Job job_local1121340975_0001 running in uber mode : false
2020-11-16 17:40:33  [ main:2003 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 17:40:33  [ main:2003 ] - [ INFO ]  Job job_local1121340975_0001 completed successfully
2020-11-16 17:40:33  [ main:2010 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1366
		FILE: Number of bytes written=570656
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=400
		HDFS: Number of bytes written=211
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=17
		Map output records=48
		Map output bytes=392
		Map output materialized bytes=494
		Input split bytes=121
		Combine input records=0
		Combine output records=0
		Reduce input groups=34
		Reduce shuffle bytes=494
		Reduce input records=48
		Reduce output records=34
		Spilled Records=96
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=200
	File Output Format Counters 
		Bytes Written=211
2020-11-16 17:43:39  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 17:43:40  [ main:559 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 17:43:40  [ main:560 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 17:43:40  [ main:757 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 17:43:40  [ main:762 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 17:43:40  [ main:776 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 17:43:40  [ main:859 ] - [ INFO ]  number of splits:1
2020-11-16 17:43:40  [ main:928 ] - [ INFO ]  Submitting tokens for job: job_local1270020731_0001
2020-11-16 17:43:40  [ main:1013 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 17:43:40  [ main:1014 ] - [ INFO ]  Running job: job_local1270020731_0001
2020-11-16 17:43:40  [ Thread-18:1014 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 17:43:40  [ Thread-18:1017 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:43:40  [ Thread-18:1018 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 17:43:40  [ Thread-18:1107 ] - [ INFO ]  Waiting for map tasks
2020-11-16 17:43:40  [ LocalJobRunner Map Task Executor #0:1107 ] - [ INFO ]  Starting task: attempt_local1270020731_0001_m_000000_0
2020-11-16 17:43:40  [ LocalJobRunner Map Task Executor #0:1120 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:43:40  [ LocalJobRunner Map Task Executor #0:1124 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 17:43:40  [ LocalJobRunner Map Task Executor #0:1124 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 17:43:40  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/sort_test/sort2.txt:0+200
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1182 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1182 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1182 ] - [ INFO ]  soft limit at 83886080
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1182 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1182 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1184 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1261 ] - [ INFO ]  
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1262 ] - [ INFO ]  Starting flush of map output
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1262 ] - [ INFO ]  Spilling map output
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1262 ] - [ INFO ]  bufstart = 0; bufend = 392; bufvoid = 104857600
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1262 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214208(104856832); length = 189/6553600
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1269 ] - [ INFO ]  Finished spill 0
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1272 ] - [ INFO ]  Task:attempt_local1270020731_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1288 ] - [ INFO ]  map
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1288 ] - [ INFO ]  Task 'attempt_local1270020731_0001_m_000000_0' done.
2020-11-16 17:43:41  [ LocalJobRunner Map Task Executor #0:1288 ] - [ INFO ]  Finishing task: attempt_local1270020731_0001_m_000000_0
2020-11-16 17:43:41  [ Thread-18:1288 ] - [ INFO ]  map task executor complete.
2020-11-16 17:43:41  [ Thread-18:1290 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 17:43:41  [ pool-6-thread-1:1290 ] - [ INFO ]  Starting task: attempt_local1270020731_0001_r_000000_0
2020-11-16 17:43:41  [ pool-6-thread-1:1295 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:43:41  [ pool-6-thread-1:1295 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 17:43:41  [ pool-6-thread-1:1295 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 17:43:41  [ pool-6-thread-1:1297 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@54c4f60f
2020-11-16 17:43:41  [ pool-6-thread-1:1305 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 17:43:41  [ EventFetcher for fetching Map Completion Events:1306 ] - [ INFO ]  attempt_local1270020731_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 17:43:41  [ localfetcher#1:1326 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1270020731_0001_m_000000_0 decomp: 53 len: 57 to MEMORY
2020-11-16 17:43:41  [ localfetcher#1:1330 ] - [ INFO ]  Read 53 bytes from map-output for attempt_local1270020731_0001_m_000000_0
2020-11-16 17:43:41  [ localfetcher#1:1331 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 53, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->53
2020-11-16 17:43:41  [ EventFetcher for fetching Map Completion Events:1332 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 17:43:41  [ pool-6-thread-1:1332 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1333 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 17:43:41  [ pool-6-thread-1:1337 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:43:41  [ pool-6-thread-1:1337 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 50 bytes
2020-11-16 17:43:41  [ pool-6-thread-1:1338 ] - [ INFO ]  Merged 1 segments, 53 bytes to disk to satisfy reduce memory limit
2020-11-16 17:43:41  [ pool-6-thread-1:1338 ] - [ INFO ]  Merging 1 files, 57 bytes from disk
2020-11-16 17:43:41  [ pool-6-thread-1:1338 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 17:43:41  [ pool-6-thread-1:1339 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:43:41  [ pool-6-thread-1:1339 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 50 bytes
2020-11-16 17:43:41  [ pool-6-thread-1:1339 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1362 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 17:43:41  [ pool-6-thread-1:1468 ] - [ INFO ]  Task:attempt_local1270020731_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 17:43:41  [ pool-6-thread-1:1477 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1477 ] - [ INFO ]  Task attempt_local1270020731_0001_r_000000_0 is allowed to commit now
2020-11-16 17:43:41  [ pool-6-thread-1:1509 ] - [ INFO ]  Saved output of task 'attempt_local1270020731_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/sort_test/result2/_temporary/0/task_local1270020731_0001_r_000000
2020-11-16 17:43:41  [ pool-6-thread-1:1509 ] - [ INFO ]  reduce > reduce
2020-11-16 17:43:41  [ pool-6-thread-1:1509 ] - [ INFO ]  Task 'attempt_local1270020731_0001_r_000000_0' done.
2020-11-16 17:43:41  [ pool-6-thread-1:1509 ] - [ INFO ]  Finishing task: attempt_local1270020731_0001_r_000000_0
2020-11-16 17:43:41  [ pool-6-thread-1:1509 ] - [ INFO ]  Starting task: attempt_local1270020731_0001_r_000001_0
2020-11-16 17:43:41  [ pool-6-thread-1:1510 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:43:41  [ pool-6-thread-1:1510 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 17:43:41  [ pool-6-thread-1:1510 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 17:43:41  [ pool-6-thread-1:1511 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49877d02
2020-11-16 17:43:41  [ pool-6-thread-1:1511 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 17:43:41  [ EventFetcher for fetching Map Completion Events:1511 ] - [ INFO ]  attempt_local1270020731_0001_r_000001_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 17:43:41  [ localfetcher#2:1513 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local1270020731_0001_m_000000_0 decomp: 272 len: 276 to MEMORY
2020-11-16 17:43:41  [ localfetcher#2:1513 ] - [ INFO ]  Read 272 bytes from map-output for attempt_local1270020731_0001_m_000000_0
2020-11-16 17:43:41  [ localfetcher#2:1513 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 272, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->272
2020-11-16 17:43:41  [ EventFetcher for fetching Map Completion Events:1513 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 17:43:41  [ pool-6-thread-1:1514 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1514 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 17:43:41  [ pool-6-thread-1:1515 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:43:41  [ pool-6-thread-1:1515 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 266 bytes
2020-11-16 17:43:41  [ pool-6-thread-1:1515 ] - [ INFO ]  Merged 1 segments, 272 bytes to disk to satisfy reduce memory limit
2020-11-16 17:43:41  [ pool-6-thread-1:1515 ] - [ INFO ]  Merging 1 files, 276 bytes from disk
2020-11-16 17:43:41  [ pool-6-thread-1:1515 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 17:43:41  [ pool-6-thread-1:1516 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:43:41  [ pool-6-thread-1:1516 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 266 bytes
2020-11-16 17:43:41  [ pool-6-thread-1:1516 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1584 ] - [ INFO ]  Task:attempt_local1270020731_0001_r_000001_0 is done. And is in the process of committing
2020-11-16 17:43:41  [ pool-6-thread-1:1594 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1594 ] - [ INFO ]  Task attempt_local1270020731_0001_r_000001_0 is allowed to commit now
2020-11-16 17:43:41  [ pool-6-thread-1:1623 ] - [ INFO ]  Saved output of task 'attempt_local1270020731_0001_r_000001_0' to hdfs://master:9000/user/root/mr/data/sort_test/result2/_temporary/0/task_local1270020731_0001_r_000001
2020-11-16 17:43:41  [ pool-6-thread-1:1624 ] - [ INFO ]  reduce > reduce
2020-11-16 17:43:41  [ pool-6-thread-1:1624 ] - [ INFO ]  Task 'attempt_local1270020731_0001_r_000001_0' done.
2020-11-16 17:43:41  [ pool-6-thread-1:1624 ] - [ INFO ]  Finishing task: attempt_local1270020731_0001_r_000001_0
2020-11-16 17:43:41  [ pool-6-thread-1:1624 ] - [ INFO ]  Starting task: attempt_local1270020731_0001_r_000002_0
2020-11-16 17:43:41  [ pool-6-thread-1:1625 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 17:43:41  [ pool-6-thread-1:1625 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 17:43:41  [ pool-6-thread-1:1625 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 17:43:41  [ pool-6-thread-1:1625 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@32b25adf
2020-11-16 17:43:41  [ pool-6-thread-1:1626 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 17:43:41  [ EventFetcher for fetching Map Completion Events:1626 ] - [ INFO ]  attempt_local1270020731_0001_r_000002_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 17:43:41  [ localfetcher#3:1628 ] - [ INFO ]  localfetcher#3 about to shuffle output of map attempt_local1270020731_0001_m_000000_0 decomp: 169 len: 173 to MEMORY
2020-11-16 17:43:41  [ localfetcher#3:1628 ] - [ INFO ]  Read 169 bytes from map-output for attempt_local1270020731_0001_m_000000_0
2020-11-16 17:43:41  [ localfetcher#3:1628 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 169, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->169
2020-11-16 17:43:41  [ EventFetcher for fetching Map Completion Events:1628 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 17:43:41  [ pool-6-thread-1:1629 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1629 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 17:43:41  [ pool-6-thread-1:1630 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:43:41  [ pool-6-thread-1:1630 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 162 bytes
2020-11-16 17:43:41  [ pool-6-thread-1:1630 ] - [ INFO ]  Merged 1 segments, 169 bytes to disk to satisfy reduce memory limit
2020-11-16 17:43:41  [ pool-6-thread-1:1631 ] - [ INFO ]  Merging 1 files, 173 bytes from disk
2020-11-16 17:43:41  [ pool-6-thread-1:1631 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 17:43:41  [ pool-6-thread-1:1631 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 17:43:41  [ pool-6-thread-1:1631 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 162 bytes
2020-11-16 17:43:41  [ pool-6-thread-1:1631 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1688 ] - [ INFO ]  Task:attempt_local1270020731_0001_r_000002_0 is done. And is in the process of committing
2020-11-16 17:43:41  [ pool-6-thread-1:1698 ] - [ INFO ]  1 / 1 copied.
2020-11-16 17:43:41  [ pool-6-thread-1:1698 ] - [ INFO ]  Task attempt_local1270020731_0001_r_000002_0 is allowed to commit now
2020-11-16 17:43:41  [ main:2015 ] - [ INFO ]  Job job_local1270020731_0001 running in uber mode : false
2020-11-16 17:43:41  [ main:2017 ] - [ INFO ]   map 100% reduce 67%
2020-11-16 17:43:41  [ pool-6-thread-1:2056 ] - [ INFO ]  Saved output of task 'attempt_local1270020731_0001_r_000002_0' to hdfs://master:9000/user/root/mr/data/sort_test/result2/_temporary/0/task_local1270020731_0001_r_000002
2020-11-16 17:43:41  [ pool-6-thread-1:2057 ] - [ INFO ]  reduce > reduce
2020-11-16 17:43:41  [ pool-6-thread-1:2057 ] - [ INFO ]  Task 'attempt_local1270020731_0001_r_000002_0' done.
2020-11-16 17:43:41  [ pool-6-thread-1:2057 ] - [ INFO ]  Finishing task: attempt_local1270020731_0001_r_000002_0
2020-11-16 17:43:41  [ Thread-18:2057 ] - [ INFO ]  reduce task executor complete.
2020-11-16 17:43:42  [ main:3018 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 17:43:42  [ main:3018 ] - [ INFO ]  Job job_local1270020731_0001 completed successfully
2020-11-16 17:43:42  [ main:3027 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=4657
		FILE: Number of bytes written=1142788
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=800
		HDFS: Number of bytes written=349
		HDFS: Number of read operations=46
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=20
	Map-Reduce Framework
		Map input records=17
		Map output records=48
		Map output bytes=392
		Map output materialized bytes=506
		Input split bytes=121
		Combine input records=0
		Combine output records=0
		Reduce input groups=34
		Reduce shuffle bytes=506
		Reduce input records=48
		Reduce output records=34
		Spilled Records=96
		Shuffled Maps =3
		Failed Shuffles=0
		Merged Map outputs=3
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=1266679808
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=200
	File Output Format Counters 
		Bytes Written=211
2020-11-16 18:16:44  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 18:16:45  [ main:578 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 18:16:45  [ main:579 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 18:16:45  [ main:778 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 18:16:45  [ main:784 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 18:16:45  [ main:831 ] - [ INFO ]  Total input paths to process : 3
2020-11-16 18:16:45  [ main:882 ] - [ INFO ]  number of splits:3
2020-11-16 18:16:45  [ main:948 ] - [ INFO ]  Submitting tokens for job: job_local1832728842_0001
2020-11-16 18:16:45  [ main:1039 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 18:16:45  [ main:1040 ] - [ INFO ]  Running job: job_local1832728842_0001
2020-11-16 18:16:45  [ Thread-18:1040 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 18:16:45  [ Thread-18:1043 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:16:45  [ Thread-18:1045 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 18:16:45  [ Thread-18:1085 ] - [ INFO ]  Waiting for map tasks
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1086 ] - [ INFO ]  Starting task: attempt_local1832728842_0001_m_000000_0
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1101 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1108 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/english.txt:0+112
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1164 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1245 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  Starting task: attempt_local1832728842_0001_m_000001_0
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1253 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1253 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1254 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1255 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/math.txt:0+112
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1307 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1307 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1307 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1308 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1308 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1308 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1326 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1329 ] - [ INFO ]  Starting task: attempt_local1832728842_0001_m_000002_0
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1330 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1330 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1330 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1331 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/chinese.txt:0+111
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1381 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1381 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1381 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1381 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1381 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1382 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:16:45  [ LocalJobRunner Map Task Executor #0:1398 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:16:45  [ Thread-18:1400 ] - [ INFO ]  map task executor complete.
2020-11-16 18:16:45  [ Thread-18:1411 ] - [ WARN ]  job_local1832728842_0001
java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at com.satan.hadoop.model.result.Student.write(Student.java:69)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1157)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.satan.hadoop.mr.MultipleFilesCombineMapReduceJob$MultipleFilesCombineMapper.map(MultipleFilesCombineMapReduceJob.java:40)
	at com.satan.hadoop.mr.MultipleFilesCombineMapReduceJob$MultipleFilesCombineMapper.map(MultipleFilesCombineMapReduceJob.java:20)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-11-16 18:16:46  [ main:2045 ] - [ INFO ]  Job job_local1832728842_0001 running in uber mode : false
2020-11-16 18:16:46  [ main:2046 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 18:16:46  [ main:2047 ] - [ INFO ]  Job job_local1832728842_0001 failed with state FAILED due to: NA
2020-11-16 18:16:46  [ main:2050 ] - [ INFO ]  Counters: 0
2020-11-16 18:18:17  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 18:18:18  [ main:550 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 18:18:18  [ main:550 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 18:18:18  [ main:752 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 18:18:18  [ main:757 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 18:18:18  [ main:795 ] - [ INFO ]  Total input paths to process : 3
2020-11-16 18:18:18  [ main:838 ] - [ INFO ]  number of splits:3
2020-11-16 18:18:18  [ main:900 ] - [ INFO ]  Submitting tokens for job: job_local801256216_0001
2020-11-16 18:18:18  [ main:990 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 18:18:18  [ main:991 ] - [ INFO ]  Running job: job_local801256216_0001
2020-11-16 18:18:18  [ Thread-18:991 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 18:18:18  [ Thread-18:994 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:18:18  [ Thread-18:996 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 18:18:18  [ Thread-18:1037 ] - [ INFO ]  Waiting for map tasks
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1037 ] - [ INFO ]  Starting task: attempt_local801256216_0001_m_000000_0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1053 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1057 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1057 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1059 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/english.txt:0+112
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1111 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1111 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1111 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1111 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1111 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1113 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1195 ] - [ INFO ]  
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  Spilling map output
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  bufstart = 0; bufend = 249; bufvoid = 104857600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  Finished spill 0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1208 ] - [ INFO ]  Task:attempt_local801256216_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1220 ] - [ INFO ]  map
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1220 ] - [ INFO ]  Task 'attempt_local801256216_0001_m_000000_0' done.
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1221 ] - [ INFO ]  Finishing task: attempt_local801256216_0001_m_000000_0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1221 ] - [ INFO ]  Starting task: attempt_local801256216_0001_m_000001_0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1221 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1222 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1222 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1223 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/math.txt:0+112
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1266 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1266 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1266 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1267 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1267 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1267 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1285 ] - [ INFO ]  
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1285 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1285 ] - [ INFO ]  Spilling map output
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1285 ] - [ INFO ]  bufstart = 0; bufend = 249; bufvoid = 104857600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1285 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1286 ] - [ INFO ]  Finished spill 0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1288 ] - [ INFO ]  Task:attempt_local801256216_0001_m_000001_0 is done. And is in the process of committing
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1298 ] - [ INFO ]  map
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1298 ] - [ INFO ]  Task 'attempt_local801256216_0001_m_000001_0' done.
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1298 ] - [ INFO ]  Finishing task: attempt_local801256216_0001_m_000001_0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1298 ] - [ INFO ]  Starting task: attempt_local801256216_0001_m_000002_0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1299 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1299 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1299 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1300 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/chinese.txt:0+111
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1340 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1341 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1341 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1341 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1341 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1342 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1359 ] - [ INFO ]  
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1359 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1359 ] - [ INFO ]  Spilling map output
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1359 ] - [ INFO ]  bufstart = 0; bufend = 249; bufvoid = 104857600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1359 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1361 ] - [ INFO ]  Finished spill 0
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1362 ] - [ INFO ]  Task:attempt_local801256216_0001_m_000002_0 is done. And is in the process of committing
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1370 ] - [ INFO ]  map
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1371 ] - [ INFO ]  Task 'attempt_local801256216_0001_m_000002_0' done.
2020-11-16 18:18:18  [ LocalJobRunner Map Task Executor #0:1371 ] - [ INFO ]  Finishing task: attempt_local801256216_0001_m_000002_0
2020-11-16 18:18:18  [ Thread-18:1371 ] - [ INFO ]  map task executor complete.
2020-11-16 18:18:18  [ Thread-18:1372 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 18:18:18  [ pool-6-thread-1:1372 ] - [ INFO ]  Starting task: attempt_local801256216_0001_r_000000_0
2020-11-16 18:18:18  [ pool-6-thread-1:1376 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:18:18  [ pool-6-thread-1:1376 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:18:18  [ pool-6-thread-1:1376 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:18:18  [ pool-6-thread-1:1378 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1a82dc08
2020-11-16 18:18:19  [ pool-6-thread-1:1386 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 18:18:19  [ EventFetcher for fetching Map Completion Events:1387 ] - [ INFO ]  attempt_local801256216_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 18:18:19  [ localfetcher#1:1405 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local801256216_0001_m_000002_0 decomp: 91 len: 95 to MEMORY
2020-11-16 18:18:19  [ localfetcher#1:1409 ] - [ INFO ]  Read 91 bytes from map-output for attempt_local801256216_0001_m_000002_0
2020-11-16 18:18:19  [ localfetcher#1:1410 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 91, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->91
2020-11-16 18:18:19  [ localfetcher#1:1411 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local801256216_0001_m_000001_0 decomp: 91 len: 95 to MEMORY
2020-11-16 18:18:19  [ localfetcher#1:1411 ] - [ INFO ]  Read 91 bytes from map-output for attempt_local801256216_0001_m_000001_0
2020-11-16 18:18:19  [ localfetcher#1:1412 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 91, inMemoryMapOutputs.size() -> 2, commitMemory -> 91, usedMemory ->182
2020-11-16 18:18:19  [ localfetcher#1:1412 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local801256216_0001_m_000000_0 decomp: 91 len: 95 to MEMORY
2020-11-16 18:18:19  [ localfetcher#1:1413 ] - [ INFO ]  Read 91 bytes from map-output for attempt_local801256216_0001_m_000000_0
2020-11-16 18:18:19  [ localfetcher#1:1413 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 91, inMemoryMapOutputs.size() -> 3, commitMemory -> 182, usedMemory ->273
2020-11-16 18:18:19  [ EventFetcher for fetching Map Completion Events:1413 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 18:18:19  [ pool-6-thread-1:1414 ] - [ INFO ]  3 / 3 copied.
2020-11-16 18:18:19  [ pool-6-thread-1:1414 ] - [ INFO ]  finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 18:18:19  [ pool-6-thread-1:1417 ] - [ INFO ]  Merging 3 sorted segments
2020-11-16 18:18:19  [ pool-6-thread-1:1417 ] - [ INFO ]  Down to the last merge-pass, with 3 segments left of total size: 252 bytes
2020-11-16 18:18:19  [ pool-6-thread-1:1418 ] - [ INFO ]  Merged 3 segments, 273 bytes to disk to satisfy reduce memory limit
2020-11-16 18:18:19  [ pool-6-thread-1:1418 ] - [ INFO ]  Merging 1 files, 273 bytes from disk
2020-11-16 18:18:19  [ pool-6-thread-1:1419 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 18:18:19  [ pool-6-thread-1:1419 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 18:18:19  [ pool-6-thread-1:1419 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 262 bytes
2020-11-16 18:18:19  [ pool-6-thread-1:1419 ] - [ INFO ]  3 / 3 copied.
2020-11-16 18:18:19  [ pool-6-thread-1:1438 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 18:18:19  [ pool-6-thread-1:1547 ] - [ INFO ]  Task:attempt_local801256216_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 18:18:19  [ pool-6-thread-1:1554 ] - [ INFO ]  3 / 3 copied.
2020-11-16 18:18:19  [ pool-6-thread-1:1554 ] - [ INFO ]  Task attempt_local801256216_0001_r_000000_0 is allowed to commit now
2020-11-16 18:18:19  [ pool-6-thread-1:1581 ] - [ INFO ]  Saved output of task 'attempt_local801256216_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/m_file_test/result/_temporary/0/task_local801256216_0001_r_000000
2020-11-16 18:18:19  [ pool-6-thread-1:1581 ] - [ INFO ]  reduce > reduce
2020-11-16 18:18:19  [ pool-6-thread-1:1582 ] - [ INFO ]  Task 'attempt_local801256216_0001_r_000000_0' done.
2020-11-16 18:18:19  [ pool-6-thread-1:1582 ] - [ INFO ]  Finishing task: attempt_local801256216_0001_r_000000_0
2020-11-16 18:18:19  [ Thread-18:1582 ] - [ INFO ]  reduce task executor complete.
2020-11-16 18:18:19  [ main:1992 ] - [ INFO ]  Job job_local801256216_0001 running in uber mode : false
2020-11-16 18:18:19  [ main:1993 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 18:18:19  [ main:1994 ] - [ INFO ]  Job job_local801256216_0001 completed successfully
2020-11-16 18:18:19  [ main:2003 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=4389
		FILE: Number of bytes written=1135100
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1006
		HDFS: Number of bytes written=215
		HDFS: Number of read operations=41
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=27
		Map output records=27
		Map output bytes=747
		Map output materialized bytes=285
		Input split bytes=372
		Combine input records=27
		Combine output records=9
		Reduce input groups=3
		Reduce shuffle bytes=285
		Reduce input records=9
		Reduce output records=3
		Spilled Records=18
		Shuffled Maps =3
		Failed Shuffles=0
		Merged Map outputs=3
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=1791492096
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=335
	File Output Format Counters 
		Bytes Written=215
2020-11-16 18:21:16  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 18:21:16  [ main:640 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 18:21:16  [ main:641 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 18:21:17  [ main:851 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 18:21:17  [ main:856 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 18:21:17  [ main:899 ] - [ INFO ]  Total input paths to process : 3
2020-11-16 18:21:17  [ main:952 ] - [ INFO ]  number of splits:3
2020-11-16 18:21:17  [ main:1020 ] - [ INFO ]  Submitting tokens for job: job_local1141227196_0001
2020-11-16 18:21:17  [ main:1120 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 18:21:17  [ main:1120 ] - [ INFO ]  Running job: job_local1141227196_0001
2020-11-16 18:21:17  [ Thread-18:1121 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 18:21:17  [ Thread-18:1124 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:21:17  [ Thread-18:1126 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 18:21:17  [ Thread-18:1171 ] - [ INFO ]  Waiting for map tasks
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1172 ] - [ INFO ]  Starting task: attempt_local1141227196_0001_m_000000_0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1189 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/english.txt:0+112
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1262 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1265 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1345 ] - [ INFO ]  
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1346 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1346 ] - [ INFO ]  Spilling map output
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1346 ] - [ INFO ]  bufstart = 0; bufend = 249; bufvoid = 104857600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1346 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1356 ] - [ INFO ]  Finished spill 0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1359 ] - [ INFO ]  Task:attempt_local1141227196_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1371 ] - [ INFO ]  map
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1371 ] - [ INFO ]  Task 'attempt_local1141227196_0001_m_000000_0' done.
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1371 ] - [ INFO ]  Finishing task: attempt_local1141227196_0001_m_000000_0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1371 ] - [ INFO ]  Starting task: attempt_local1141227196_0001_m_000001_0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1372 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1372 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1372 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1373 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/math.txt:0+112
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1421 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1434 ] - [ INFO ]  
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1434 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1434 ] - [ INFO ]  Spilling map output
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1434 ] - [ INFO ]  bufstart = 0; bufend = 249; bufvoid = 104857600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1434 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1436 ] - [ INFO ]  Finished spill 0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1437 ] - [ INFO ]  Task:attempt_local1141227196_0001_m_000001_0 is done. And is in the process of committing
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1445 ] - [ INFO ]  map
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1445 ] - [ INFO ]  Task 'attempt_local1141227196_0001_m_000001_0' done.
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1445 ] - [ INFO ]  Finishing task: attempt_local1141227196_0001_m_000001_0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1445 ] - [ INFO ]  Starting task: attempt_local1141227196_0001_m_000002_0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1446 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1446 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1446 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1447 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/m_file_test/chinese.txt:0+111
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1492 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1492 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1492 ] - [ INFO ]  soft limit at 83886080
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1492 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1492 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1492 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1508 ] - [ INFO ]  
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1508 ] - [ INFO ]  Starting flush of map output
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1509 ] - [ INFO ]  Spilling map output
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1509 ] - [ INFO ]  bufstart = 0; bufend = 249; bufvoid = 104857600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1509 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1510 ] - [ INFO ]  Finished spill 0
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1512 ] - [ INFO ]  Task:attempt_local1141227196_0001_m_000002_0 is done. And is in the process of committing
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1519 ] - [ INFO ]  map
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1519 ] - [ INFO ]  Task 'attempt_local1141227196_0001_m_000002_0' done.
2020-11-16 18:21:17  [ LocalJobRunner Map Task Executor #0:1519 ] - [ INFO ]  Finishing task: attempt_local1141227196_0001_m_000002_0
2020-11-16 18:21:17  [ Thread-18:1519 ] - [ INFO ]  map task executor complete.
2020-11-16 18:21:17  [ Thread-18:1521 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 18:21:17  [ pool-6-thread-1:1521 ] - [ INFO ]  Starting task: attempt_local1141227196_0001_r_000000_0
2020-11-16 18:21:17  [ pool-6-thread-1:1526 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 18:21:17  [ pool-6-thread-1:1526 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 18:21:17  [ pool-6-thread-1:1526 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 18:21:17  [ pool-6-thread-1:1528 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6dec8a7f
2020-11-16 18:21:17  [ pool-6-thread-1:1537 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 18:21:17  [ EventFetcher for fetching Map Completion Events:1539 ] - [ INFO ]  attempt_local1141227196_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 18:21:17  [ localfetcher#1:1561 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1141227196_0001_m_000000_0 decomp: 91 len: 95 to MEMORY
2020-11-16 18:21:17  [ localfetcher#1:1566 ] - [ INFO ]  Read 91 bytes from map-output for attempt_local1141227196_0001_m_000000_0
2020-11-16 18:21:17  [ localfetcher#1:1567 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 91, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->91
2020-11-16 18:21:17  [ localfetcher#1:1568 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1141227196_0001_m_000002_0 decomp: 91 len: 95 to MEMORY
2020-11-16 18:21:17  [ localfetcher#1:1569 ] - [ INFO ]  Read 91 bytes from map-output for attempt_local1141227196_0001_m_000002_0
2020-11-16 18:21:17  [ localfetcher#1:1569 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 91, inMemoryMapOutputs.size() -> 2, commitMemory -> 91, usedMemory ->182
2020-11-16 18:21:17  [ localfetcher#1:1570 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1141227196_0001_m_000001_0 decomp: 91 len: 95 to MEMORY
2020-11-16 18:21:17  [ localfetcher#1:1571 ] - [ INFO ]  Read 91 bytes from map-output for attempt_local1141227196_0001_m_000001_0
2020-11-16 18:21:17  [ localfetcher#1:1571 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 91, inMemoryMapOutputs.size() -> 3, commitMemory -> 182, usedMemory ->273
2020-11-16 18:21:17  [ EventFetcher for fetching Map Completion Events:1571 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 18:21:17  [ pool-6-thread-1:1571 ] - [ INFO ]  3 / 3 copied.
2020-11-16 18:21:17  [ pool-6-thread-1:1572 ] - [ INFO ]  finalMerge called with 3 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 18:21:17  [ pool-6-thread-1:1577 ] - [ INFO ]  Merging 3 sorted segments
2020-11-16 18:21:17  [ pool-6-thread-1:1577 ] - [ INFO ]  Down to the last merge-pass, with 3 segments left of total size: 252 bytes
2020-11-16 18:21:17  [ pool-6-thread-1:1578 ] - [ INFO ]  Merged 3 segments, 273 bytes to disk to satisfy reduce memory limit
2020-11-16 18:21:17  [ pool-6-thread-1:1578 ] - [ INFO ]  Merging 1 files, 273 bytes from disk
2020-11-16 18:21:17  [ pool-6-thread-1:1579 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 18:21:17  [ pool-6-thread-1:1579 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 18:21:17  [ pool-6-thread-1:1579 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 262 bytes
2020-11-16 18:21:17  [ pool-6-thread-1:1579 ] - [ INFO ]  3 / 3 copied.
2020-11-16 18:21:17  [ pool-6-thread-1:1606 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 18:21:18  [ pool-6-thread-1:1990 ] - [ INFO ]  Task:attempt_local1141227196_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 18:21:18  [ pool-6-thread-1:1998 ] - [ INFO ]  3 / 3 copied.
2020-11-16 18:21:18  [ pool-6-thread-1:1998 ] - [ INFO ]  Task attempt_local1141227196_0001_r_000000_0 is allowed to commit now
2020-11-16 18:21:18  [ pool-6-thread-1:2020 ] - [ INFO ]  Saved output of task 'attempt_local1141227196_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/m_file_test/result/_temporary/0/task_local1141227196_0001_r_000000
2020-11-16 18:21:18  [ pool-6-thread-1:2020 ] - [ INFO ]  reduce > reduce
2020-11-16 18:21:18  [ pool-6-thread-1:2020 ] - [ INFO ]  Task 'attempt_local1141227196_0001_r_000000_0' done.
2020-11-16 18:21:18  [ pool-6-thread-1:2020 ] - [ INFO ]  Finishing task: attempt_local1141227196_0001_r_000000_0
2020-11-16 18:21:18  [ Thread-18:2021 ] - [ INFO ]  reduce task executor complete.
2020-11-16 18:21:18  [ main:2126 ] - [ INFO ]  Job job_local1141227196_0001 running in uber mode : false
2020-11-16 18:21:18  [ main:2127 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 18:21:18  [ main:2128 ] - [ INFO ]  Job job_local1141227196_0001 completed successfully
2020-11-16 18:21:18  [ main:2137 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=4389
		FILE: Number of bytes written=1141172
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1006
		HDFS: Number of bytes written=212
		HDFS: Number of read operations=41
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=10
	Map-Reduce Framework
		Map input records=27
		Map output records=27
		Map output bytes=747
		Map output materialized bytes=285
		Input split bytes=372
		Combine input records=27
		Combine output records=9
		Reduce input groups=3
		Reduce shuffle bytes=285
		Reduce input records=9
		Reduce output records=3
		Spilled Records=18
		Shuffled Maps =3
		Failed Shuffles=0
		Merged Map outputs=3
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=1793589248
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=335
	File Output Format Counters 
		Bytes Written=212
2020-11-16 19:25:02  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:25:03  [ main:552 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:25:03  [ main:553 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:25:03  [ main:768 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:25:03  [ main:773 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:25:03  [ main:786 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:25:03  [ main:855 ] - [ INFO ]  number of splits:1
2020-11-16 19:25:03  [ main:917 ] - [ INFO ]  Submitting tokens for job: job_local822554272_0001
2020-11-16 19:25:03  [ main:1006 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:25:03  [ main:1006 ] - [ INFO ]  Running job: job_local822554272_0001
2020-11-16 19:25:03  [ Thread-18:1007 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:25:03  [ Thread-18:1009 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:25:03  [ Thread-18:1011 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:25:03  [ Thread-18:1052 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1052 ] - [ INFO ]  Starting task: attempt_local822554272_0001_m_000000_0
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1068 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1072 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1072 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1075 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1130 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1214 ] - [ INFO ]  
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  Spilling map output
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1221 ] - [ INFO ]  Finished spill 0
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1224 ] - [ INFO ]  Task:attempt_local822554272_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1239 ] - [ INFO ]  map
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1239 ] - [ INFO ]  Task 'attempt_local822554272_0001_m_000000_0' done.
2020-11-16 19:25:03  [ LocalJobRunner Map Task Executor #0:1239 ] - [ INFO ]  Finishing task: attempt_local822554272_0001_m_000000_0
2020-11-16 19:25:03  [ Thread-18:1239 ] - [ INFO ]  map task executor complete.
2020-11-16 19:25:03  [ Thread-18:1241 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:25:03  [ pool-6-thread-1:1241 ] - [ INFO ]  Starting task: attempt_local822554272_0001_r_000000_0
2020-11-16 19:25:03  [ pool-6-thread-1:1245 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:25:03  [ pool-6-thread-1:1246 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:25:03  [ pool-6-thread-1:1246 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:25:03  [ pool-6-thread-1:1247 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5a14088a
2020-11-16 19:25:03  [ pool-6-thread-1:1255 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:25:03  [ EventFetcher for fetching Map Completion Events:1256 ] - [ INFO ]  attempt_local822554272_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:25:03  [ localfetcher#1:1274 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local822554272_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:25:03  [ localfetcher#1:1278 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local822554272_0001_m_000000_0
2020-11-16 19:25:03  [ localfetcher#1:1279 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:25:03  [ EventFetcher for fetching Map Completion Events:1280 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:25:03  [ pool-6-thread-1:1280 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:25:03  [ pool-6-thread-1:1280 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:25:03  [ pool-6-thread-1:1284 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:25:03  [ pool-6-thread-1:1284 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:25:03  [ pool-6-thread-1:1285 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:25:03  [ pool-6-thread-1:1285 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:25:03  [ pool-6-thread-1:1286 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:25:03  [ pool-6-thread-1:1286 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:25:03  [ pool-6-thread-1:1286 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:25:03  [ pool-6-thread-1:1286 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:25:03  [ pool-6-thread-1:1309 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:25:03  [ pool-6-thread-1:1401 ] - [ INFO ]  Task:attempt_local822554272_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:25:03  [ pool-6-thread-1:1410 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:25:03  [ pool-6-thread-1:1410 ] - [ INFO ]  Task attempt_local822554272_0001_r_000000_0 is allowed to commit now
2020-11-16 19:25:03  [ pool-6-thread-1:1438 ] - [ INFO ]  Saved output of task 'attempt_local822554272_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local822554272_0001_r_000000
2020-11-16 19:25:03  [ pool-6-thread-1:1439 ] - [ INFO ]  reduce > reduce
2020-11-16 19:25:03  [ pool-6-thread-1:1439 ] - [ INFO ]  Task 'attempt_local822554272_0001_r_000000_0' done.
2020-11-16 19:25:03  [ pool-6-thread-1:1439 ] - [ INFO ]  Finishing task: attempt_local822554272_0001_r_000000_0
2020-11-16 19:25:03  [ Thread-18:1439 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:25:04  [ main:2008 ] - [ INFO ]  Job job_local822554272_0001 running in uber mode : false
2020-11-16 19:25:04  [ main:2009 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:25:04  [ main:2009 ] - [ INFO ]  Job job_local822554272_0001 completed successfully
2020-11-16 19:25:04  [ main:2016 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565622
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:25:04  [ main:2042 ] - [ INFO ]  Running job: job_local822554272_0001
2020-11-16 19:25:04  [ main:2042 ] - [ INFO ]  Job job_local822554272_0001 running in uber mode : false
2020-11-16 19:25:04  [ main:2042 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:25:04  [ main:2042 ] - [ INFO ]  Job job_local822554272_0001 completed successfully
2020-11-16 19:25:04  [ main:2044 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565622
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:25:55  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:25:56  [ main:584 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:25:56  [ main:585 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:25:56  [ main:771 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:25:56  [ main:776 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:25:56  [ main:786 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:25:56  [ main:861 ] - [ INFO ]  number of splits:1
2020-11-16 19:25:56  [ main:928 ] - [ INFO ]  Submitting tokens for job: job_local243135361_0001
2020-11-16 19:25:56  [ main:1021 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:25:56  [ main:1021 ] - [ INFO ]  Running job: job_local243135361_0001
2020-11-16 19:25:56  [ Thread-18:1022 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:25:56  [ Thread-18:1025 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:25:56  [ Thread-18:1026 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:25:56  [ Thread-18:1057 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1058 ] - [ INFO ]  Starting task: attempt_local243135361_0001_m_000000_0
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1074 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1078 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1078 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1080 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1129 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1205 ] - [ INFO ]  
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1207 ] - [ INFO ]  Spilling map output
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1207 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:25:56  [ LocalJobRunner Map Task Executor #0:1207 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:25:57  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  Finished spill 0
2020-11-16 19:25:57  [ LocalJobRunner Map Task Executor #0:1215 ] - [ INFO ]  Task:attempt_local243135361_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:25:57  [ LocalJobRunner Map Task Executor #0:1226 ] - [ INFO ]  map
2020-11-16 19:25:57  [ LocalJobRunner Map Task Executor #0:1226 ] - [ INFO ]  Task 'attempt_local243135361_0001_m_000000_0' done.
2020-11-16 19:25:57  [ LocalJobRunner Map Task Executor #0:1226 ] - [ INFO ]  Finishing task: attempt_local243135361_0001_m_000000_0
2020-11-16 19:25:57  [ Thread-18:1226 ] - [ INFO ]  map task executor complete.
2020-11-16 19:25:57  [ Thread-18:1228 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:25:57  [ pool-6-thread-1:1228 ] - [ INFO ]  Starting task: attempt_local243135361_0001_r_000000_0
2020-11-16 19:25:57  [ pool-6-thread-1:1232 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:25:57  [ pool-6-thread-1:1232 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:25:57  [ pool-6-thread-1:1232 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:25:57  [ pool-6-thread-1:1234 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@41a019de
2020-11-16 19:25:57  [ pool-6-thread-1:1241 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:25:57  [ EventFetcher for fetching Map Completion Events:1242 ] - [ INFO ]  attempt_local243135361_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:25:57  [ localfetcher#1:1261 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local243135361_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:25:57  [ localfetcher#1:1265 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local243135361_0001_m_000000_0
2020-11-16 19:25:57  [ localfetcher#1:1265 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:25:57  [ EventFetcher for fetching Map Completion Events:1266 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:25:57  [ pool-6-thread-1:1267 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:25:57  [ pool-6-thread-1:1267 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:25:57  [ pool-6-thread-1:1271 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:25:57  [ pool-6-thread-1:1271 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:25:57  [ pool-6-thread-1:1272 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:25:57  [ pool-6-thread-1:1272 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:25:57  [ pool-6-thread-1:1273 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:25:57  [ pool-6-thread-1:1273 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:25:57  [ pool-6-thread-1:1273 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:25:57  [ pool-6-thread-1:1273 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:25:57  [ pool-6-thread-1:1294 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:25:57  [ pool-6-thread-1:1393 ] - [ INFO ]  Task:attempt_local243135361_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:25:57  [ pool-6-thread-1:1400 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:25:57  [ pool-6-thread-1:1400 ] - [ INFO ]  Task attempt_local243135361_0001_r_000000_0 is allowed to commit now
2020-11-16 19:25:57  [ pool-6-thread-1:1420 ] - [ INFO ]  Saved output of task 'attempt_local243135361_0001_r_000000_0' to hdfs://master:9000/tmp/_temporary/0/task_local243135361_0001_r_000000
2020-11-16 19:25:57  [ pool-6-thread-1:1420 ] - [ INFO ]  reduce > reduce
2020-11-16 19:25:57  [ pool-6-thread-1:1421 ] - [ INFO ]  Task 'attempt_local243135361_0001_r_000000_0' done.
2020-11-16 19:25:57  [ pool-6-thread-1:1421 ] - [ INFO ]  Finishing task: attempt_local243135361_0001_r_000000_0
2020-11-16 19:25:57  [ Thread-18:1421 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:25:57  [ main:2027 ] - [ INFO ]  Job job_local243135361_0001 running in uber mode : false
2020-11-16 19:25:57  [ main:2028 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:25:57  [ main:2029 ] - [ INFO ]  Job job_local243135361_0001 completed successfully
2020-11-16 19:25:57  [ main:2036 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565490
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:25:57  [ main:2049 ] - [ INFO ]  Running job: job_local243135361_0001
2020-11-16 19:25:57  [ main:2049 ] - [ INFO ]  Job job_local243135361_0001 running in uber mode : false
2020-11-16 19:25:57  [ main:2049 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:25:57  [ main:2049 ] - [ INFO ]  Job job_local243135361_0001 completed successfully
2020-11-16 19:25:57  [ main:2052 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565490
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:27:37  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:27:38  [ main:634 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:27:38  [ main:635 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:27:38  [ main:838 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:27:38  [ main:843 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:27:38  [ main:858 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:27:38  [ main:941 ] - [ INFO ]  number of splits:1
2020-11-16 19:27:38  [ main:1013 ] - [ INFO ]  Submitting tokens for job: job_local484197188_0001
2020-11-16 19:27:38  [ main:1116 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:27:38  [ main:1117 ] - [ INFO ]  Running job: job_local484197188_0001
2020-11-16 19:27:38  [ Thread-18:1117 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:27:38  [ Thread-18:1121 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:27:38  [ Thread-18:1122 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:27:38  [ Thread-18:1171 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1171 ] - [ INFO ]  Starting task: attempt_local484197188_0001_m_000000_0
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1189 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1194 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1196 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1250 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1250 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1250 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1250 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1250 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1328 ] - [ INFO ]  
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1330 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1330 ] - [ INFO ]  Spilling map output
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1330 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1330 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1335 ] - [ INFO ]  Finished spill 0
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1338 ] - [ INFO ]  Task:attempt_local484197188_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1353 ] - [ INFO ]  map
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1353 ] - [ INFO ]  Task 'attempt_local484197188_0001_m_000000_0' done.
2020-11-16 19:27:38  [ LocalJobRunner Map Task Executor #0:1353 ] - [ INFO ]  Finishing task: attempt_local484197188_0001_m_000000_0
2020-11-16 19:27:38  [ Thread-18:1353 ] - [ INFO ]  map task executor complete.
2020-11-16 19:27:38  [ Thread-18:1355 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:27:38  [ pool-6-thread-1:1355 ] - [ INFO ]  Starting task: attempt_local484197188_0001_r_000000_0
2020-11-16 19:27:38  [ pool-6-thread-1:1359 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:27:38  [ pool-6-thread-1:1360 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:27:38  [ pool-6-thread-1:1360 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:27:38  [ pool-6-thread-1:1361 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@758e4963
2020-11-16 19:27:38  [ pool-6-thread-1:1369 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:27:38  [ EventFetcher for fetching Map Completion Events:1372 ] - [ INFO ]  attempt_local484197188_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:27:38  [ localfetcher#1:1394 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local484197188_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:27:38  [ localfetcher#1:1398 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local484197188_0001_m_000000_0
2020-11-16 19:27:38  [ localfetcher#1:1399 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:27:38  [ EventFetcher for fetching Map Completion Events:1400 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:27:38  [ pool-6-thread-1:1401 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:27:38  [ pool-6-thread-1:1401 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:27:38  [ pool-6-thread-1:1405 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:27:38  [ pool-6-thread-1:1406 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:27:38  [ pool-6-thread-1:1406 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:27:38  [ pool-6-thread-1:1407 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:27:38  [ pool-6-thread-1:1407 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:27:38  [ pool-6-thread-1:1407 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:27:38  [ pool-6-thread-1:1407 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:27:38  [ pool-6-thread-1:1408 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:27:38  [ pool-6-thread-1:1431 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:27:38  [ pool-6-thread-1:1525 ] - [ INFO ]  Task:attempt_local484197188_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:27:38  [ pool-6-thread-1:1535 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:27:38  [ pool-6-thread-1:1535 ] - [ INFO ]  Task attempt_local484197188_0001_r_000000_0 is allowed to commit now
2020-11-16 19:27:38  [ pool-6-thread-1:1563 ] - [ INFO ]  Saved output of task 'attempt_local484197188_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local484197188_0001_r_000000
2020-11-16 19:27:38  [ pool-6-thread-1:1564 ] - [ INFO ]  reduce > reduce
2020-11-16 19:27:38  [ pool-6-thread-1:1564 ] - [ INFO ]  Task 'attempt_local484197188_0001_r_000000_0' done.
2020-11-16 19:27:38  [ pool-6-thread-1:1564 ] - [ INFO ]  Finishing task: attempt_local484197188_0001_r_000000_0
2020-11-16 19:27:38  [ Thread-18:1564 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:27:39  [ main:2120 ] - [ INFO ]  Job job_local484197188_0001 running in uber mode : false
2020-11-16 19:27:39  [ main:2121 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:27:39  [ main:2121 ] - [ INFO ]  Job job_local484197188_0001 completed successfully
2020-11-16 19:27:39  [ main:2128 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565622
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=635437056
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:27:59  [ main:22270 ] - [ INFO ]  Running job: job_local484197188_0001
2020-11-16 19:27:59  [ main:22271 ] - [ INFO ]  Job job_local484197188_0001 running in uber mode : false
2020-11-16 19:27:59  [ main:22271 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:27:59  [ main:22271 ] - [ INFO ]  Job job_local484197188_0001 completed successfully
2020-11-16 19:27:59  [ main:22273 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565622
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=635437056
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:29:13  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:29:14  [ main:548 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:29:14  [ main:548 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:29:14  [ main:754 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:29:14  [ main:759 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:29:14  [ main:770 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:29:14  [ main:844 ] - [ INFO ]  number of splits:1
2020-11-16 19:29:14  [ main:905 ] - [ INFO ]  Submitting tokens for job: job_local1593973560_0001
2020-11-16 19:29:14  [ main:986 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:29:14  [ main:987 ] - [ INFO ]  Running job: job_local1593973560_0001
2020-11-16 19:29:14  [ Thread-18:987 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:29:14  [ Thread-18:990 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:14  [ Thread-18:991 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:29:14  [ Thread-18:1028 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1028 ] - [ INFO ]  Starting task: attempt_local1593973560_0001_m_000000_0
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1046 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1050 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1050 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1053 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:29:14  [ LocalJobRunner Map Task Executor #0:1107 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1178 ] - [ INFO ]  
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1180 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1180 ] - [ INFO ]  Spilling map output
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1180 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1180 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1186 ] - [ INFO ]  Finished spill 0
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1189 ] - [ INFO ]  Task:attempt_local1593973560_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1201 ] - [ INFO ]  map
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1201 ] - [ INFO ]  Task 'attempt_local1593973560_0001_m_000000_0' done.
2020-11-16 19:29:15  [ LocalJobRunner Map Task Executor #0:1201 ] - [ INFO ]  Finishing task: attempt_local1593973560_0001_m_000000_0
2020-11-16 19:29:15  [ Thread-18:1201 ] - [ INFO ]  map task executor complete.
2020-11-16 19:29:15  [ Thread-18:1203 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:29:15  [ pool-6-thread-1:1203 ] - [ INFO ]  Starting task: attempt_local1593973560_0001_r_000000_0
2020-11-16 19:29:15  [ pool-6-thread-1:1207 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:15  [ pool-6-thread-1:1207 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:29:15  [ pool-6-thread-1:1207 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:29:15  [ pool-6-thread-1:1209 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@60939ea4
2020-11-16 19:29:15  [ pool-6-thread-1:1216 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:29:15  [ EventFetcher for fetching Map Completion Events:1217 ] - [ INFO ]  attempt_local1593973560_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:29:15  [ localfetcher#1:1239 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1593973560_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:29:15  [ localfetcher#1:1244 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local1593973560_0001_m_000000_0
2020-11-16 19:29:15  [ localfetcher#1:1244 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:29:15  [ EventFetcher for fetching Map Completion Events:1245 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:29:15  [ pool-6-thread-1:1246 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:15  [ pool-6-thread-1:1246 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:29:15  [ pool-6-thread-1:1251 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:29:15  [ pool-6-thread-1:1251 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:29:15  [ pool-6-thread-1:1252 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:29:15  [ pool-6-thread-1:1252 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:29:15  [ pool-6-thread-1:1253 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:29:15  [ pool-6-thread-1:1253 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:29:15  [ pool-6-thread-1:1253 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:29:15  [ pool-6-thread-1:1253 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:15  [ pool-6-thread-1:1273 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:29:15  [ pool-6-thread-1:1371 ] - [ INFO ]  Task:attempt_local1593973560_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:29:15  [ pool-6-thread-1:1378 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:15  [ pool-6-thread-1:1378 ] - [ INFO ]  Task attempt_local1593973560_0001_r_000000_0 is allowed to commit now
2020-11-16 19:29:15  [ pool-6-thread-1:1399 ] - [ INFO ]  Saved output of task 'attempt_local1593973560_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local1593973560_0001_r_000000
2020-11-16 19:29:15  [ pool-6-thread-1:1399 ] - [ INFO ]  reduce > reduce
2020-11-16 19:29:15  [ pool-6-thread-1:1399 ] - [ INFO ]  Task 'attempt_local1593973560_0001_r_000000_0' done.
2020-11-16 19:29:15  [ pool-6-thread-1:1400 ] - [ INFO ]  Finishing task: attempt_local1593973560_0001_r_000000_0
2020-11-16 19:29:15  [ Thread-18:1400 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:29:15  [ main:1989 ] - [ INFO ]  Job job_local1593973560_0001 running in uber mode : false
2020-11-16 19:29:15  [ main:1990 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:29:15  [ main:1990 ] - [ INFO ]  Job job_local1593973560_0001 completed successfully
2020-11-16 19:29:15  [ main:1996 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568654
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:29:15  [ main:2017 ] - [ INFO ]  Running job: job_local1593973560_0001
2020-11-16 19:29:15  [ main:2017 ] - [ INFO ]  Job job_local1593973560_0001 running in uber mode : false
2020-11-16 19:29:15  [ main:2017 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:29:15  [ main:2018 ] - [ INFO ]  Job job_local1593973560_0001 completed successfully
2020-11-16 19:29:15  [ main:2020 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568654
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:29:29  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:29:30  [ main:557 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:29:30  [ main:558 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:29:30  [ main:758 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:29:30  [ main:763 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:29:30  [ main:775 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:29:30  [ main:842 ] - [ INFO ]  number of splits:1
2020-11-16 19:29:30  [ main:904 ] - [ INFO ]  Submitting tokens for job: job_local1578431506_0001
2020-11-16 19:29:30  [ main:992 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:29:30  [ main:992 ] - [ INFO ]  Running job: job_local1578431506_0001
2020-11-16 19:29:30  [ Thread-18:993 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:29:30  [ Thread-18:996 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:30  [ Thread-18:997 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:29:30  [ Thread-18:1038 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:29:30  [ LocalJobRunner Map Task Executor #0:1038 ] - [ INFO ]  Starting task: attempt_local1578431506_0001_m_000000_0
2020-11-16 19:29:30  [ LocalJobRunner Map Task Executor #0:1056 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:30  [ LocalJobRunner Map Task Executor #0:1060 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:29:30  [ LocalJobRunner Map Task Executor #0:1060 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:29:30  [ LocalJobRunner Map Task Executor #0:1062 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1116 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1116 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1116 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1116 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1116 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1118 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1188 ] - [ INFO ]  
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1189 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1189 ] - [ INFO ]  Spilling map output
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1189 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1189 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1195 ] - [ INFO ]  Finished spill 0
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1197 ] - [ INFO ]  Task:attempt_local1578431506_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1209 ] - [ INFO ]  map
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1209 ] - [ INFO ]  Task 'attempt_local1578431506_0001_m_000000_0' done.
2020-11-16 19:29:31  [ LocalJobRunner Map Task Executor #0:1209 ] - [ INFO ]  Finishing task: attempt_local1578431506_0001_m_000000_0
2020-11-16 19:29:31  [ Thread-18:1209 ] - [ INFO ]  map task executor complete.
2020-11-16 19:29:31  [ Thread-18:1211 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:29:31  [ pool-6-thread-1:1211 ] - [ INFO ]  Starting task: attempt_local1578431506_0001_r_000000_0
2020-11-16 19:29:31  [ pool-6-thread-1:1215 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:31  [ pool-6-thread-1:1215 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:29:31  [ pool-6-thread-1:1215 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:29:31  [ pool-6-thread-1:1217 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1052077b
2020-11-16 19:29:31  [ pool-6-thread-1:1224 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:29:31  [ EventFetcher for fetching Map Completion Events:1226 ] - [ INFO ]  attempt_local1578431506_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:29:31  [ localfetcher#1:1245 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1578431506_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:29:31  [ localfetcher#1:1249 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local1578431506_0001_m_000000_0
2020-11-16 19:29:31  [ localfetcher#1:1250 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:29:31  [ EventFetcher for fetching Map Completion Events:1251 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:29:31  [ pool-6-thread-1:1251 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:31  [ pool-6-thread-1:1251 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:29:31  [ pool-6-thread-1:1255 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:29:31  [ pool-6-thread-1:1256 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:29:31  [ pool-6-thread-1:1256 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:29:31  [ pool-6-thread-1:1257 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:29:31  [ pool-6-thread-1:1257 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:29:31  [ pool-6-thread-1:1257 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:29:31  [ pool-6-thread-1:1258 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:29:31  [ pool-6-thread-1:1258 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:31  [ pool-6-thread-1:1278 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:29:31  [ pool-6-thread-1:1375 ] - [ INFO ]  Task:attempt_local1578431506_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:29:31  [ pool-6-thread-1:1381 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:31  [ pool-6-thread-1:1381 ] - [ INFO ]  Task attempt_local1578431506_0001_r_000000_0 is allowed to commit now
2020-11-16 19:29:31  [ pool-6-thread-1:1401 ] - [ INFO ]  Saved output of task 'attempt_local1578431506_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local1578431506_0001_r_000000
2020-11-16 19:29:31  [ pool-6-thread-1:1401 ] - [ INFO ]  reduce > reduce
2020-11-16 19:29:31  [ pool-6-thread-1:1401 ] - [ INFO ]  Task 'attempt_local1578431506_0001_r_000000_0' done.
2020-11-16 19:29:31  [ pool-6-thread-1:1402 ] - [ INFO ]  Finishing task: attempt_local1578431506_0001_r_000000_0
2020-11-16 19:29:31  [ Thread-18:1402 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:29:31  [ main:1997 ] - [ INFO ]  Job job_local1578431506_0001 running in uber mode : false
2020-11-16 19:29:31  [ main:1998 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:29:31  [ main:1998 ] - [ INFO ]  Job job_local1578431506_0001 completed successfully
2020-11-16 19:29:31  [ main:2004 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568654
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:29:31  [ main:2022 ] - [ INFO ]  Running job: job_local1578431506_0001
2020-11-16 19:29:31  [ main:2022 ] - [ INFO ]  Job job_local1578431506_0001 running in uber mode : false
2020-11-16 19:29:31  [ main:2023 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:29:31  [ main:2023 ] - [ INFO ]  Job job_local1578431506_0001 completed successfully
2020-11-16 19:29:31  [ main:2025 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568654
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:29:45  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:29:45  [ main:579 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:29:45  [ main:579 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:29:45  [ main:787 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:29:45  [ main:792 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:29:45  [ main:808 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:29:45  [ main:877 ] - [ INFO ]  number of splits:1
2020-11-16 19:29:45  [ main:940 ] - [ INFO ]  Submitting tokens for job: job_local38375339_0001
2020-11-16 19:29:46  [ main:1037 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:29:46  [ main:1037 ] - [ INFO ]  Running job: job_local38375339_0001
2020-11-16 19:29:46  [ Thread-18:1038 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:29:46  [ Thread-18:1041 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:46  [ Thread-18:1043 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:29:46  [ Thread-18:1084 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1085 ] - [ INFO ]  Starting task: attempt_local38375339_0001_m_000000_0
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1102 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1106 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1107 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1109 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1163 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1245 ] - [ INFO ]  
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1247 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1247 ] - [ INFO ]  Spilling map output
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1247 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1247 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  Finished spill 0
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1254 ] - [ INFO ]  Task:attempt_local38375339_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1270 ] - [ INFO ]  map
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1270 ] - [ INFO ]  Task 'attempt_local38375339_0001_m_000000_0' done.
2020-11-16 19:29:46  [ LocalJobRunner Map Task Executor #0:1270 ] - [ INFO ]  Finishing task: attempt_local38375339_0001_m_000000_0
2020-11-16 19:29:46  [ Thread-18:1270 ] - [ INFO ]  map task executor complete.
2020-11-16 19:29:46  [ Thread-18:1271 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:29:46  [ pool-6-thread-1:1272 ] - [ INFO ]  Starting task: attempt_local38375339_0001_r_000000_0
2020-11-16 19:29:46  [ pool-6-thread-1:1275 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:29:46  [ pool-6-thread-1:1276 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:29:46  [ pool-6-thread-1:1276 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:29:46  [ pool-6-thread-1:1277 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@55c537eb
2020-11-16 19:29:46  [ pool-6-thread-1:1285 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:29:46  [ EventFetcher for fetching Map Completion Events:1287 ] - [ INFO ]  attempt_local38375339_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:29:46  [ localfetcher#1:1306 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local38375339_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:29:46  [ localfetcher#1:1310 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local38375339_0001_m_000000_0
2020-11-16 19:29:46  [ localfetcher#1:1311 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:29:46  [ EventFetcher for fetching Map Completion Events:1312 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:29:46  [ pool-6-thread-1:1313 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:46  [ pool-6-thread-1:1313 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:29:46  [ pool-6-thread-1:1318 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:29:46  [ pool-6-thread-1:1318 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:29:46  [ pool-6-thread-1:1319 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:29:46  [ pool-6-thread-1:1319 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:29:46  [ pool-6-thread-1:1319 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:29:46  [ pool-6-thread-1:1319 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:29:46  [ pool-6-thread-1:1320 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:29:46  [ pool-6-thread-1:1320 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:46  [ pool-6-thread-1:1344 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:29:46  [ pool-6-thread-1:1453 ] - [ INFO ]  Task:attempt_local38375339_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:29:46  [ pool-6-thread-1:1466 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:29:46  [ pool-6-thread-1:1466 ] - [ INFO ]  Task attempt_local38375339_0001_r_000000_0 is allowed to commit now
2020-11-16 19:29:46  [ pool-6-thread-1:1502 ] - [ INFO ]  Saved output of task 'attempt_local38375339_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local38375339_0001_r_000000
2020-11-16 19:29:46  [ pool-6-thread-1:1502 ] - [ INFO ]  reduce > reduce
2020-11-16 19:29:46  [ pool-6-thread-1:1502 ] - [ INFO ]  Task 'attempt_local38375339_0001_r_000000_0' done.
2020-11-16 19:29:46  [ pool-6-thread-1:1503 ] - [ INFO ]  Finishing task: attempt_local38375339_0001_r_000000_0
2020-11-16 19:29:46  [ Thread-18:1503 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:29:47  [ main:2041 ] - [ INFO ]  Job job_local38375339_0001 running in uber mode : false
2020-11-16 19:29:47  [ main:2042 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:29:47  [ main:2043 ] - [ INFO ]  Job job_local38375339_0001 completed successfully
2020-11-16 19:29:47  [ main:2049 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=562590
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:29:47  [ main:2086 ] - [ INFO ]  Running job: job_local38375339_0001
2020-11-16 19:29:47  [ main:2086 ] - [ INFO ]  Job job_local38375339_0001 running in uber mode : false
2020-11-16 19:29:47  [ main:2086 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:29:47  [ main:2086 ] - [ INFO ]  Job job_local38375339_0001 completed successfully
2020-11-16 19:29:47  [ main:2089 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=562590
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:30:46  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:30:46  [ main:588 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:30:46  [ main:588 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:30:46  [ main:776 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:30:46  [ main:783 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:30:47  [ main:798 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:30:47  [ main:877 ] - [ INFO ]  number of splits:1
2020-11-16 19:30:47  [ main:950 ] - [ INFO ]  Submitting tokens for job: job_local1616511624_0001
2020-11-16 19:30:47  [ main:1045 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:30:47  [ main:1045 ] - [ INFO ]  Running job: job_local1616511624_0001
2020-11-16 19:30:47  [ Thread-18:1046 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:30:47  [ Thread-18:1049 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:30:47  [ Thread-18:1051 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:30:47  [ Thread-18:1087 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1088 ] - [ INFO ]  Starting task: attempt_local1616511624_0001_m_000000_0
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1103 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1107 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1108 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1110 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1162 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1162 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1162 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1164 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1237 ] - [ INFO ]  
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1238 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1239 ] - [ INFO ]  Spilling map output
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1239 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1239 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1244 ] - [ INFO ]  Finished spill 0
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1247 ] - [ INFO ]  Task:attempt_local1616511624_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1261 ] - [ INFO ]  map
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1261 ] - [ INFO ]  Task 'attempt_local1616511624_0001_m_000000_0' done.
2020-11-16 19:30:47  [ LocalJobRunner Map Task Executor #0:1261 ] - [ INFO ]  Finishing task: attempt_local1616511624_0001_m_000000_0
2020-11-16 19:30:47  [ Thread-18:1261 ] - [ INFO ]  map task executor complete.
2020-11-16 19:30:47  [ Thread-18:1263 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:30:47  [ pool-6-thread-1:1263 ] - [ INFO ]  Starting task: attempt_local1616511624_0001_r_000000_0
2020-11-16 19:30:47  [ pool-6-thread-1:1267 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:30:47  [ pool-6-thread-1:1268 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:30:47  [ pool-6-thread-1:1268 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:30:47  [ pool-6-thread-1:1269 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@475fb1b1
2020-11-16 19:30:47  [ pool-6-thread-1:1277 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:30:47  [ EventFetcher for fetching Map Completion Events:1279 ] - [ INFO ]  attempt_local1616511624_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:30:47  [ localfetcher#1:1300 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1616511624_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:30:47  [ localfetcher#1:1304 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local1616511624_0001_m_000000_0
2020-11-16 19:30:47  [ localfetcher#1:1305 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:30:47  [ EventFetcher for fetching Map Completion Events:1306 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:30:47  [ pool-6-thread-1:1307 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:30:47  [ pool-6-thread-1:1307 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:30:47  [ pool-6-thread-1:1311 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:30:47  [ pool-6-thread-1:1312 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:30:47  [ pool-6-thread-1:1313 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:30:47  [ pool-6-thread-1:1313 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:30:47  [ pool-6-thread-1:1313 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:30:47  [ pool-6-thread-1:1313 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:30:47  [ pool-6-thread-1:1314 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:30:47  [ pool-6-thread-1:1314 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:30:47  [ pool-6-thread-1:1339 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:30:47  [ pool-6-thread-1:1453 ] - [ INFO ]  Task:attempt_local1616511624_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:30:47  [ pool-6-thread-1:1462 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:30:47  [ pool-6-thread-1:1463 ] - [ INFO ]  Task attempt_local1616511624_0001_r_000000_0 is allowed to commit now
2020-11-16 19:30:47  [ pool-6-thread-1:1493 ] - [ INFO ]  Saved output of task 'attempt_local1616511624_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local1616511624_0001_r_000000
2020-11-16 19:30:47  [ pool-6-thread-1:1494 ] - [ INFO ]  reduce > reduce
2020-11-16 19:30:47  [ pool-6-thread-1:1494 ] - [ INFO ]  Task 'attempt_local1616511624_0001_r_000000_0' done.
2020-11-16 19:30:47  [ pool-6-thread-1:1494 ] - [ INFO ]  Finishing task: attempt_local1616511624_0001_r_000000_0
2020-11-16 19:30:47  [ Thread-18:1494 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:30:48  [ main:2048 ] - [ INFO ]  Job job_local1616511624_0001 running in uber mode : false
2020-11-16 19:30:48  [ main:2049 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:30:48  [ main:2050 ] - [ INFO ]  Job job_local1616511624_0001 completed successfully
2020-11-16 19:30:48  [ main:2057 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568654
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:30:48  [ main:2087 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 19:30:48  [ main:2107 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:30:48  [ main:2112 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:30:48  [ main:2121 ] - [ INFO ]  Cleaning up the staging area file:/tmp/hadoop-liuwenyi/mapred/staging/root731656748/.staging/job_local731656748_0002
2020-11-16 19:32:08  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:32:09  [ main:570 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:32:09  [ main:570 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:32:09  [ main:767 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:32:09  [ main:773 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:32:09  [ main:788 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:32:09  [ main:862 ] - [ INFO ]  number of splits:1
2020-11-16 19:32:09  [ main:929 ] - [ INFO ]  Submitting tokens for job: job_local1943216696_0001
2020-11-16 19:32:09  [ main:1017 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:32:09  [ main:1017 ] - [ INFO ]  Running job: job_local1943216696_0001
2020-11-16 19:32:09  [ Thread-18:1018 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:32:09  [ Thread-18:1021 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:32:09  [ Thread-18:1022 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:32:10  [ Thread-18:1052 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1053 ] - [ INFO ]  Starting task: attempt_local1943216696_0001_m_000000_0
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1067 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1070 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1071 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1072 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1121 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1121 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1121 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1121 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1121 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1123 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1190 ] - [ INFO ]  
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1192 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1192 ] - [ INFO ]  Spilling map output
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1192 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1192 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1198 ] - [ INFO ]  Finished spill 0
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1201 ] - [ INFO ]  Task:attempt_local1943216696_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1212 ] - [ INFO ]  map
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1212 ] - [ INFO ]  Task 'attempt_local1943216696_0001_m_000000_0' done.
2020-11-16 19:32:10  [ LocalJobRunner Map Task Executor #0:1212 ] - [ INFO ]  Finishing task: attempt_local1943216696_0001_m_000000_0
2020-11-16 19:32:10  [ Thread-18:1213 ] - [ INFO ]  map task executor complete.
2020-11-16 19:32:10  [ Thread-18:1214 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:32:10  [ pool-6-thread-1:1215 ] - [ INFO ]  Starting task: attempt_local1943216696_0001_r_000000_0
2020-11-16 19:32:10  [ pool-6-thread-1:1218 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:32:10  [ pool-6-thread-1:1219 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:32:10  [ pool-6-thread-1:1219 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:32:10  [ pool-6-thread-1:1221 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@c364ad5
2020-11-16 19:32:10  [ pool-6-thread-1:1229 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:32:10  [ EventFetcher for fetching Map Completion Events:1231 ] - [ INFO ]  attempt_local1943216696_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:32:10  [ localfetcher#1:1252 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1943216696_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:32:10  [ localfetcher#1:1256 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local1943216696_0001_m_000000_0
2020-11-16 19:32:10  [ localfetcher#1:1257 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:32:10  [ EventFetcher for fetching Map Completion Events:1258 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:32:10  [ pool-6-thread-1:1259 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:32:10  [ pool-6-thread-1:1259 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:32:10  [ pool-6-thread-1:1264 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:32:10  [ pool-6-thread-1:1264 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:32:10  [ pool-6-thread-1:1265 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:32:10  [ pool-6-thread-1:1266 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:32:10  [ pool-6-thread-1:1266 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:32:10  [ pool-6-thread-1:1266 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:32:10  [ pool-6-thread-1:1266 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:32:10  [ pool-6-thread-1:1267 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:32:10  [ pool-6-thread-1:1288 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:32:10  [ pool-6-thread-1:1376 ] - [ INFO ]  Task:attempt_local1943216696_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:32:10  [ pool-6-thread-1:1383 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:32:10  [ pool-6-thread-1:1383 ] - [ INFO ]  Task attempt_local1943216696_0001_r_000000_0 is allowed to commit now
2020-11-16 19:32:10  [ pool-6-thread-1:1402 ] - [ INFO ]  Saved output of task 'attempt_local1943216696_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local1943216696_0001_r_000000
2020-11-16 19:32:10  [ pool-6-thread-1:1403 ] - [ INFO ]  reduce > reduce
2020-11-16 19:32:10  [ pool-6-thread-1:1403 ] - [ INFO ]  Task 'attempt_local1943216696_0001_r_000000_0' done.
2020-11-16 19:32:10  [ pool-6-thread-1:1403 ] - [ INFO ]  Finishing task: attempt_local1943216696_0001_r_000000_0
2020-11-16 19:32:10  [ Thread-18:1403 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:32:10  [ main:2020 ] - [ INFO ]  Job job_local1943216696_0001 running in uber mode : false
2020-11-16 19:32:10  [ main:2021 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:32:10  [ main:2022 ] - [ INFO ]  Job job_local1943216696_0001 completed successfully
2020-11-16 19:32:10  [ main:2029 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568654
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:32:10  [ main:2032 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 19:34:15  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:34:15  [ main:539 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:34:15  [ main:540 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:34:16  [ main:719 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:34:16  [ main:724 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:34:16  [ main:736 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:34:16  [ main:803 ] - [ INFO ]  number of splits:1
2020-11-16 19:34:16  [ main:861 ] - [ INFO ]  Submitting tokens for job: job_local598305141_0001
2020-11-16 19:34:16  [ main:948 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:34:16  [ main:948 ] - [ INFO ]  Running job: job_local598305141_0001
2020-11-16 19:34:16  [ Thread-18:949 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:34:16  [ Thread-18:952 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:34:16  [ Thread-18:953 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:34:16  [ Thread-18:985 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:985 ] - [ INFO ]  Starting task: attempt_local598305141_0001_m_000000_0
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1000 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1003 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1004 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1006 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1062 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1062 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1062 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1062 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1062 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1064 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1143 ] - [ INFO ]  
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  Spilling map output
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1144 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1150 ] - [ INFO ]  Finished spill 0
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1153 ] - [ INFO ]  Task:attempt_local598305141_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1164 ] - [ INFO ]  map
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1165 ] - [ INFO ]  Task 'attempt_local598305141_0001_m_000000_0' done.
2020-11-16 19:34:16  [ LocalJobRunner Map Task Executor #0:1165 ] - [ INFO ]  Finishing task: attempt_local598305141_0001_m_000000_0
2020-11-16 19:34:16  [ Thread-18:1165 ] - [ INFO ]  map task executor complete.
2020-11-16 19:34:16  [ Thread-18:1167 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:34:16  [ pool-6-thread-1:1167 ] - [ INFO ]  Starting task: attempt_local598305141_0001_r_000000_0
2020-11-16 19:34:16  [ pool-6-thread-1:1171 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:34:16  [ pool-6-thread-1:1171 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:34:16  [ pool-6-thread-1:1171 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:34:16  [ pool-6-thread-1:1173 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@b4d2e97
2020-11-16 19:34:16  [ pool-6-thread-1:1181 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:34:16  [ EventFetcher for fetching Map Completion Events:1183 ] - [ INFO ]  attempt_local598305141_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:34:16  [ localfetcher#1:1203 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local598305141_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:34:16  [ localfetcher#1:1207 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local598305141_0001_m_000000_0
2020-11-16 19:34:16  [ localfetcher#1:1208 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:34:16  [ EventFetcher for fetching Map Completion Events:1210 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:34:16  [ pool-6-thread-1:1211 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:34:16  [ pool-6-thread-1:1211 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:34:16  [ pool-6-thread-1:1217 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:34:16  [ pool-6-thread-1:1218 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:34:16  [ pool-6-thread-1:1219 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:34:16  [ pool-6-thread-1:1219 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:34:16  [ pool-6-thread-1:1219 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:34:16  [ pool-6-thread-1:1219 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:34:16  [ pool-6-thread-1:1220 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:34:16  [ pool-6-thread-1:1220 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:34:16  [ pool-6-thread-1:1241 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:34:16  [ pool-6-thread-1:1345 ] - [ INFO ]  Task:attempt_local598305141_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:34:16  [ pool-6-thread-1:1351 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:34:16  [ pool-6-thread-1:1352 ] - [ INFO ]  Task attempt_local598305141_0001_r_000000_0 is allowed to commit now
2020-11-16 19:34:16  [ pool-6-thread-1:1371 ] - [ INFO ]  Saved output of task 'attempt_local598305141_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local598305141_0001_r_000000
2020-11-16 19:34:16  [ pool-6-thread-1:1372 ] - [ INFO ]  reduce > reduce
2020-11-16 19:34:16  [ pool-6-thread-1:1372 ] - [ INFO ]  Task 'attempt_local598305141_0001_r_000000_0' done.
2020-11-16 19:34:16  [ pool-6-thread-1:1372 ] - [ INFO ]  Finishing task: attempt_local598305141_0001_r_000000_0
2020-11-16 19:34:16  [ Thread-18:1372 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:34:17  [ main:1951 ] - [ INFO ]  Job job_local598305141_0001 running in uber mode : false
2020-11-16 19:34:17  [ main:1952 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:34:17  [ main:1952 ] - [ INFO ]  Job job_local598305141_0001 completed successfully
2020-11-16 19:34:17  [ main:1959 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565622
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=697303040
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:34:17  [ main:1977 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 19:34:17  [ main:1987 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:34:17  [ main:1992 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:34:17  [ main:1998 ] - [ INFO ]  Cleaning up the staging area file:/tmp/hadoop-liuwenyi/mapred/staging/root49515607/.staging/job_local49515607_0002
2020-11-16 19:37:48  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:37:49  [ main:657 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:37:49  [ main:657 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:37:49  [ main:862 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:37:49  [ main:867 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:37:49  [ main:879 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:37:49  [ main:949 ] - [ INFO ]  number of splits:1
2020-11-16 19:37:49  [ main:1014 ] - [ INFO ]  Submitting tokens for job: job_local147063244_0001
2020-11-16 19:37:49  [ main:1108 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:37:49  [ main:1108 ] - [ INFO ]  Running job: job_local147063244_0001
2020-11-16 19:37:49  [ Thread-18:1109 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:37:49  [ Thread-18:1113 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:37:49  [ Thread-18:1114 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:37:49  [ Thread-18:1150 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:37:49  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]  Starting task: attempt_local147063244_0001_m_000000_0
2020-11-16 19:37:49  [ LocalJobRunner Map Task Executor #0:1169 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:37:49  [ LocalJobRunner Map Task Executor #0:1173 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:37:49  [ LocalJobRunner Map Task Executor #0:1173 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:37:49  [ LocalJobRunner Map Task Executor #0:1176 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1228 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1229 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1229 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1229 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1229 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1231 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1315 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1316 ] - [ INFO ]  Spilling map output
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1316 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1316 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1322 ] - [ INFO ]  Finished spill 0
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1324 ] - [ INFO ]  Task:attempt_local147063244_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1336 ] - [ INFO ]  map
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1337 ] - [ INFO ]  Task 'attempt_local147063244_0001_m_000000_0' done.
2020-11-16 19:37:50  [ LocalJobRunner Map Task Executor #0:1337 ] - [ INFO ]  Finishing task: attempt_local147063244_0001_m_000000_0
2020-11-16 19:37:50  [ Thread-18:1337 ] - [ INFO ]  map task executor complete.
2020-11-16 19:37:50  [ Thread-18:1339 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:37:50  [ pool-6-thread-1:1339 ] - [ INFO ]  Starting task: attempt_local147063244_0001_r_000000_0
2020-11-16 19:37:50  [ pool-6-thread-1:1344 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:37:50  [ pool-6-thread-1:1344 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:37:50  [ pool-6-thread-1:1344 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:37:50  [ pool-6-thread-1:1346 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5fbb144e
2020-11-16 19:37:50  [ pool-6-thread-1:1355 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:37:50  [ EventFetcher for fetching Map Completion Events:1357 ] - [ INFO ]  attempt_local147063244_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:37:50  [ localfetcher#1:1383 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local147063244_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:37:50  [ localfetcher#1:1389 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local147063244_0001_m_000000_0
2020-11-16 19:37:50  [ localfetcher#1:1391 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:37:50  [ EventFetcher for fetching Map Completion Events:1392 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:37:50  [ pool-6-thread-1:1392 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:37:50  [ pool-6-thread-1:1393 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:37:50  [ pool-6-thread-1:1398 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:37:50  [ pool-6-thread-1:1398 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:37:50  [ pool-6-thread-1:1399 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:37:50  [ pool-6-thread-1:1399 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:37:50  [ pool-6-thread-1:1400 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:37:50  [ pool-6-thread-1:1400 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:37:50  [ pool-6-thread-1:1400 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:37:50  [ pool-6-thread-1:1400 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:37:50  [ pool-6-thread-1:1424 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:37:50  [ pool-6-thread-1:1521 ] - [ INFO ]  Task:attempt_local147063244_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:37:50  [ pool-6-thread-1:1528 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:37:50  [ pool-6-thread-1:1528 ] - [ INFO ]  Task attempt_local147063244_0001_r_000000_0 is allowed to commit now
2020-11-16 19:37:50  [ pool-6-thread-1:1548 ] - [ INFO ]  Saved output of task 'attempt_local147063244_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/tmp/_temporary/0/task_local147063244_0001_r_000000
2020-11-16 19:37:50  [ pool-6-thread-1:1549 ] - [ INFO ]  reduce > reduce
2020-11-16 19:37:50  [ pool-6-thread-1:1549 ] - [ INFO ]  Task 'attempt_local147063244_0001_r_000000_0' done.
2020-11-16 19:37:50  [ pool-6-thread-1:1549 ] - [ INFO ]  Finishing task: attempt_local147063244_0001_r_000000_0
2020-11-16 19:37:50  [ Thread-18:1549 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:37:50  [ main:2112 ] - [ INFO ]  Job job_local147063244_0001 running in uber mode : false
2020-11-16 19:37:50  [ main:2113 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:37:50  [ main:2113 ] - [ INFO ]  Job job_local147063244_0001 completed successfully
2020-11-16 19:37:50  [ main:2120 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565622
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=637534208
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:38:32  [ main:44024 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 19:38:32  [ main:44040 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:38:32  [ main:44045 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:38:32  [ main:44054 ] - [ INFO ]  Cleaning up the staging area file:/tmp/hadoop-liuwenyi/mapred/staging/root1024067584/.staging/job_local1024067584_0002
2020-11-16 19:39:35  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 19:39:35  [ main:543 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 19:39:35  [ main:544 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 19:39:36  [ main:743 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:39:36  [ main:752 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:39:36  [ main:764 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:39:36  [ main:841 ] - [ INFO ]  number of splits:1
2020-11-16 19:39:36  [ main:906 ] - [ INFO ]  Submitting tokens for job: job_local606505373_0001
2020-11-16 19:39:36  [ main:992 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:39:36  [ main:992 ] - [ INFO ]  Running job: job_local606505373_0001
2020-11-16 19:39:36  [ Thread-18:993 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:39:36  [ Thread-18:996 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:39:36  [ Thread-18:997 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:39:36  [ Thread-18:1030 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1030 ] - [ INFO ]  Starting task: attempt_local606505373_0001_m_000000_0
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1043 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1047 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1047 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1049 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1100 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1100 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1100 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1100 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1100 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1102 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1200 ] - [ INFO ]  
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1202 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1202 ] - [ INFO ]  Spilling map output
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1202 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1202 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1208 ] - [ INFO ]  Finished spill 0
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1211 ] - [ INFO ]  Task:attempt_local606505373_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1228 ] - [ INFO ]  map
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1228 ] - [ INFO ]  Task 'attempt_local606505373_0001_m_000000_0' done.
2020-11-16 19:39:36  [ LocalJobRunner Map Task Executor #0:1228 ] - [ INFO ]  Finishing task: attempt_local606505373_0001_m_000000_0
2020-11-16 19:39:36  [ Thread-18:1228 ] - [ INFO ]  map task executor complete.
2020-11-16 19:39:36  [ Thread-18:1230 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:39:36  [ pool-6-thread-1:1230 ] - [ INFO ]  Starting task: attempt_local606505373_0001_r_000000_0
2020-11-16 19:39:36  [ pool-6-thread-1:1234 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:39:36  [ pool-6-thread-1:1235 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:39:36  [ pool-6-thread-1:1235 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:39:36  [ pool-6-thread-1:1237 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@25468784
2020-11-16 19:39:36  [ pool-6-thread-1:1245 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:39:36  [ EventFetcher for fetching Map Completion Events:1246 ] - [ INFO ]  attempt_local606505373_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:39:36  [ localfetcher#1:1267 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local606505373_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 19:39:36  [ localfetcher#1:1272 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local606505373_0001_m_000000_0
2020-11-16 19:39:36  [ localfetcher#1:1273 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 19:39:36  [ EventFetcher for fetching Map Completion Events:1274 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:39:36  [ pool-6-thread-1:1274 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:39:36  [ pool-6-thread-1:1275 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:39:36  [ pool-6-thread-1:1279 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:39:36  [ pool-6-thread-1:1279 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:39:36  [ pool-6-thread-1:1280 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 19:39:36  [ pool-6-thread-1:1280 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 19:39:36  [ pool-6-thread-1:1281 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:39:36  [ pool-6-thread-1:1281 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:39:36  [ pool-6-thread-1:1281 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 19:39:36  [ pool-6-thread-1:1281 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:39:36  [ pool-6-thread-1:1311 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 19:39:36  [ pool-6-thread-1:1412 ] - [ INFO ]  Task:attempt_local606505373_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 19:39:36  [ pool-6-thread-1:1419 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:39:36  [ pool-6-thread-1:1419 ] - [ INFO ]  Task attempt_local606505373_0001_r_000000_0 is allowed to commit now
2020-11-16 19:39:36  [ pool-6-thread-1:1438 ] - [ INFO ]  Saved output of task 'attempt_local606505373_0001_r_000000_0' to hdfs://master:9000/tmp960435018/_temporary/0/task_local606505373_0001_r_000000
2020-11-16 19:39:36  [ pool-6-thread-1:1439 ] - [ INFO ]  reduce > reduce
2020-11-16 19:39:36  [ pool-6-thread-1:1439 ] - [ INFO ]  Task 'attempt_local606505373_0001_r_000000_0' done.
2020-11-16 19:39:36  [ pool-6-thread-1:1439 ] - [ INFO ]  Finishing task: attempt_local606505373_0001_r_000000_0
2020-11-16 19:39:36  [ Thread-18:1439 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:39:37  [ main:1996 ] - [ INFO ]  Job job_local606505373_0001 running in uber mode : false
2020-11-16 19:39:37  [ main:1997 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:39:37  [ main:1997 ] - [ INFO ]  Job job_local606505373_0001 completed successfully
2020-11-16 19:39:37  [ main:2004 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565526
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 19:39:37  [ main:2012 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 19:39:37  [ main:2025 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 19:39:37  [ main:2029 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 19:39:37  [ main:2042 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 19:39:37  [ main:2072 ] - [ INFO ]  number of splits:1
2020-11-16 19:39:37  [ main:2090 ] - [ INFO ]  Submitting tokens for job: job_local855234317_0002
2020-11-16 19:39:37  [ main:2130 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 19:39:37  [ main:2130 ] - [ INFO ]  Running job: job_local855234317_0002
2020-11-16 19:39:37  [ Thread-46:2130 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 19:39:37  [ Thread-46:2130 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:39:37  [ Thread-46:2130 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 19:39:37  [ Thread-46:2140 ] - [ INFO ]  Waiting for map tasks
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2140 ] - [ INFO ]  Starting task: attempt_local855234317_0002_m_000000_0
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2141 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2141 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2141 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2142 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp960435018/part-r-00000:0+141
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2181 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2182 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2182 ] - [ INFO ]  soft limit at 83886080
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2182 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2182 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2182 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2199 ] - [ INFO ]  
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2199 ] - [ INFO ]  Starting flush of map output
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2199 ] - [ INFO ]  Spilling map output
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2199 ] - [ INFO ]  bufstart = 0; bufend = 141; bufvoid = 104857600
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2199 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2201 ] - [ INFO ]  Finished spill 0
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2202 ] - [ INFO ]  Task:attempt_local855234317_0002_m_000000_0 is done. And is in the process of committing
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2208 ] - [ INFO ]  map
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2208 ] - [ INFO ]  Task 'attempt_local855234317_0002_m_000000_0' done.
2020-11-16 19:39:37  [ LocalJobRunner Map Task Executor #0:2209 ] - [ INFO ]  Finishing task: attempt_local855234317_0002_m_000000_0
2020-11-16 19:39:37  [ Thread-46:2209 ] - [ INFO ]  map task executor complete.
2020-11-16 19:39:37  [ Thread-46:2209 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 19:39:37  [ pool-9-thread-1:2209 ] - [ INFO ]  Starting task: attempt_local855234317_0002_r_000000_0
2020-11-16 19:39:37  [ pool-9-thread-1:2210 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 19:39:37  [ pool-9-thread-1:2210 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 19:39:37  [ pool-9-thread-1:2210 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 19:39:37  [ pool-9-thread-1:2210 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6b469c79
2020-11-16 19:39:37  [ pool-9-thread-1:2211 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 19:39:37  [ EventFetcher for fetching Map Completion Events:2211 ] - [ INFO ]  attempt_local855234317_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 19:39:37  [ localfetcher#2:2212 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local855234317_0002_m_000000_0 decomp: 161 len: 165 to MEMORY
2020-11-16 19:39:37  [ localfetcher#2:2212 ] - [ INFO ]  Read 161 bytes from map-output for attempt_local855234317_0002_m_000000_0
2020-11-16 19:39:37  [ localfetcher#2:2212 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 161, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->161
2020-11-16 19:39:37  [ EventFetcher for fetching Map Completion Events:2212 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 19:39:37  [ pool-9-thread-1:2213 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:39:37  [ pool-9-thread-1:2213 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 19:39:37  [ pool-9-thread-1:2213 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:39:37  [ pool-9-thread-1:2214 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 19:39:37  [ pool-9-thread-1:2214 ] - [ INFO ]  Merged 1 segments, 161 bytes to disk to satisfy reduce memory limit
2020-11-16 19:39:37  [ pool-9-thread-1:2214 ] - [ INFO ]  Merging 1 files, 165 bytes from disk
2020-11-16 19:39:37  [ pool-9-thread-1:2214 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 19:39:37  [ pool-9-thread-1:2214 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 19:39:37  [ pool-9-thread-1:2214 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 19:39:37  [ pool-9-thread-1:2215 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:39:37  [ pool-9-thread-1:2258 ] - [ INFO ]  Task:attempt_local855234317_0002_r_000000_0 is done. And is in the process of committing
2020-11-16 19:39:37  [ pool-9-thread-1:2264 ] - [ INFO ]  1 / 1 copied.
2020-11-16 19:39:37  [ pool-9-thread-1:2264 ] - [ INFO ]  Task attempt_local855234317_0002_r_000000_0 is allowed to commit now
2020-11-16 19:39:37  [ pool-9-thread-1:2283 ] - [ INFO ]  Saved output of task 'attempt_local855234317_0002_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/_temporary/0/task_local855234317_0002_r_000000
2020-11-16 19:39:37  [ pool-9-thread-1:2284 ] - [ INFO ]  reduce > reduce
2020-11-16 19:39:37  [ pool-9-thread-1:2284 ] - [ INFO ]  Task 'attempt_local855234317_0002_r_000000_0' done.
2020-11-16 19:39:37  [ pool-9-thread-1:2284 ] - [ INFO ]  Finishing task: attempt_local855234317_0002_r_000000_0
2020-11-16 19:39:37  [ Thread-46:2284 ] - [ INFO ]  reduce task executor complete.
2020-11-16 19:39:38  [ main:3130 ] - [ INFO ]  Job job_local855234317_0002 running in uber mode : false
2020-11-16 19:39:38  [ main:3131 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 19:39:38  [ main:3131 ] - [ INFO ]  Job job_local855234317_0002 completed successfully
2020-11-16 19:39:38  [ main:3134 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1648
		FILE: Number of bytes written=1128789
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=600
		HDFS: Number of bytes written=621
		HDFS: Number of read operations=43
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=16
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=141
		Map output materialized bytes=165
		Input split bytes=109
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=165
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=842006528
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=141
	File Output Format Counters 
		Bytes Written=339
2020-11-16 22:07:03  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:07:04  [ main:797 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:07:04  [ main:798 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:07:04  [ main:1012 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:07:04  [ main:1016 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:07:04  [ main:1090 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:07:04  [ main:1246 ] - [ INFO ]  number of splits:1
2020-11-16 22:07:04  [ main:1312 ] - [ INFO ]  Submitting tokens for job: job_local2127748155_0001
2020-11-16 22:07:04  [ main:1395 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:07:04  [ main:1396 ] - [ INFO ]  Running job: job_local2127748155_0001
2020-11-16 22:07:04  [ Thread-18:1396 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:07:04  [ Thread-18:1400 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:07:04  [ Thread-18:1401 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:07:04  [ Thread-18:1472 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1473 ] - [ INFO ]  Starting task: attempt_local2127748155_0001_m_000000_0
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1490 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1495 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1496 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1497 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1544 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1544 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1544 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1544 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1544 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1547 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1696 ] - [ INFO ]  
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1697 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1698 ] - [ INFO ]  Spilling map output
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1698 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1698 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1705 ] - [ INFO ]  Finished spill 0
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1707 ] - [ INFO ]  Task:attempt_local2127748155_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1730 ] - [ INFO ]  map
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1730 ] - [ INFO ]  Task 'attempt_local2127748155_0001_m_000000_0' done.
2020-11-16 22:07:04  [ LocalJobRunner Map Task Executor #0:1730 ] - [ INFO ]  Finishing task: attempt_local2127748155_0001_m_000000_0
2020-11-16 22:07:04  [ Thread-18:1730 ] - [ INFO ]  map task executor complete.
2020-11-16 22:07:04  [ Thread-18:1732 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:07:04  [ pool-6-thread-1:1732 ] - [ INFO ]  Starting task: attempt_local2127748155_0001_r_000000_0
2020-11-16 22:07:04  [ pool-6-thread-1:1736 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:07:04  [ pool-6-thread-1:1737 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:07:04  [ pool-6-thread-1:1737 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:07:04  [ pool-6-thread-1:1739 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@750fa169
2020-11-16 22:07:04  [ pool-6-thread-1:1747 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:07:04  [ EventFetcher for fetching Map Completion Events:1749 ] - [ INFO ]  attempt_local2127748155_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:07:04  [ localfetcher#1:1766 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local2127748155_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 22:07:04  [ localfetcher#1:1769 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local2127748155_0001_m_000000_0
2020-11-16 22:07:04  [ localfetcher#1:1770 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 22:07:04  [ EventFetcher for fetching Map Completion Events:1770 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:07:04  [ pool-6-thread-1:1771 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:07:04  [ pool-6-thread-1:1771 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:07:04  [ pool-6-thread-1:1775 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:07:04  [ pool-6-thread-1:1775 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:07:04  [ pool-6-thread-1:1776 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 22:07:04  [ pool-6-thread-1:1776 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 22:07:04  [ pool-6-thread-1:1776 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:07:04  [ pool-6-thread-1:1776 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:07:04  [ pool-6-thread-1:1777 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:07:04  [ pool-6-thread-1:1777 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:07:05  [ pool-6-thread-1:1887 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:07:05  [ pool-6-thread-1:2059 ] - [ INFO ]  Task:attempt_local2127748155_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:07:05  [ pool-6-thread-1:2079 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:07:05  [ pool-6-thread-1:2079 ] - [ INFO ]  Task attempt_local2127748155_0001_r_000000_0 is allowed to commit now
2020-11-16 22:07:05  [ pool-6-thread-1:2166 ] - [ INFO ]  Saved output of task 'attempt_local2127748155_0001_r_000000_0' to hdfs://master:9000/tmp60919072/output1605535622783/_temporary/0/task_local2127748155_0001_r_000000
2020-11-16 22:07:05  [ pool-6-thread-1:2167 ] - [ INFO ]  reduce > reduce
2020-11-16 22:07:05  [ pool-6-thread-1:2167 ] - [ INFO ]  Task 'attempt_local2127748155_0001_r_000000_0' done.
2020-11-16 22:07:05  [ pool-6-thread-1:2167 ] - [ INFO ]  Finishing task: attempt_local2127748155_0001_r_000000_0
2020-11-16 22:07:05  [ Thread-18:2167 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:07:05  [ main:2398 ] - [ INFO ]  Job job_local2127748155_0001 running in uber mode : false
2020-11-16 22:07:05  [ main:2399 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:07:05  [ main:2400 ] - [ INFO ]  Job job_local2127748155_0001 completed successfully
2020-11-16 22:07:05  [ main:2407 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568634
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 22:07:05  [ main:2487 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 22:07:05  [ main:2507 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:07:05  [ main:2512 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:07:05  [ main:2544 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:07:05  [ main:2582 ] - [ INFO ]  number of splits:1
2020-11-16 22:07:05  [ main:2605 ] - [ INFO ]  Submitting tokens for job: job_local365859837_0002
2020-11-16 22:07:05  [ main:2652 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:07:05  [ main:2652 ] - [ INFO ]  Running job: job_local365859837_0002
2020-11-16 22:07:05  [ Thread-46:2652 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:07:05  [ Thread-46:2652 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:07:05  [ Thread-46:2653 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:07:05  [ Thread-46:2677 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2677 ] - [ INFO ]  Starting task: attempt_local365859837_0002_m_000000_0
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2678 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2678 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2678 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2679 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp60919072/output1605535622783/part-r-00000:0+141
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2724 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2724 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2724 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2724 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2724 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:07:05  [ LocalJobRunner Map Task Executor #0:2725 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2795 ] - [ INFO ]  
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2795 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2795 ] - [ INFO ]  Spilling map output
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2795 ] - [ INFO ]  bufstart = 0; bufend = 141; bufvoid = 104857600
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2795 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2796 ] - [ INFO ]  Finished spill 0
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2797 ] - [ INFO ]  Task:attempt_local365859837_0002_m_000000_0 is done. And is in the process of committing
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2835 ] - [ INFO ]  map
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2835 ] - [ INFO ]  Task 'attempt_local365859837_0002_m_000000_0' done.
2020-11-16 22:07:06  [ LocalJobRunner Map Task Executor #0:2835 ] - [ INFO ]  Finishing task: attempt_local365859837_0002_m_000000_0
2020-11-16 22:07:06  [ Thread-46:2835 ] - [ INFO ]  map task executor complete.
2020-11-16 22:07:06  [ Thread-46:2836 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:07:06  [ pool-9-thread-1:2836 ] - [ INFO ]  Starting task: attempt_local365859837_0002_r_000000_0
2020-11-16 22:07:06  [ pool-9-thread-1:2836 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:07:06  [ pool-9-thread-1:2837 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:07:06  [ pool-9-thread-1:2837 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:07:06  [ pool-9-thread-1:2837 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@18adfef4
2020-11-16 22:07:06  [ pool-9-thread-1:2837 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:07:06  [ EventFetcher for fetching Map Completion Events:2837 ] - [ INFO ]  attempt_local365859837_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:07:06  [ localfetcher#2:2838 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local365859837_0002_m_000000_0 decomp: 161 len: 165 to MEMORY
2020-11-16 22:07:06  [ localfetcher#2:2839 ] - [ INFO ]  Read 161 bytes from map-output for attempt_local365859837_0002_m_000000_0
2020-11-16 22:07:06  [ localfetcher#2:2839 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 161, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->161
2020-11-16 22:07:06  [ EventFetcher for fetching Map Completion Events:2839 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:07:06  [ pool-9-thread-1:2839 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:07:06  [ pool-9-thread-1:2839 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:07:06  [ pool-9-thread-1:2840 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:07:06  [ pool-9-thread-1:2840 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 22:07:06  [ pool-9-thread-1:2841 ] - [ INFO ]  Merged 1 segments, 161 bytes to disk to satisfy reduce memory limit
2020-11-16 22:07:06  [ pool-9-thread-1:2841 ] - [ INFO ]  Merging 1 files, 165 bytes from disk
2020-11-16 22:07:06  [ pool-9-thread-1:2841 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:07:06  [ pool-9-thread-1:2841 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:07:06  [ pool-9-thread-1:2841 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 22:07:06  [ pool-9-thread-1:2841 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:07:06  [ pool-9-thread-1:3053 ] - [ INFO ]  Task:attempt_local365859837_0002_r_000000_0 is done. And is in the process of committing
2020-11-16 22:07:06  [ pool-9-thread-1:3063 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:07:06  [ pool-9-thread-1:3063 ] - [ INFO ]  Task attempt_local365859837_0002_r_000000_0 is allowed to commit now
2020-11-16 22:07:06  [ pool-9-thread-1:3092 ] - [ INFO ]  Saved output of task 'attempt_local365859837_0002_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/_temporary/0/task_local365859837_0002_r_000000
2020-11-16 22:07:06  [ pool-9-thread-1:3093 ] - [ INFO ]  reduce > reduce
2020-11-16 22:07:06  [ pool-9-thread-1:3093 ] - [ INFO ]  Task 'attempt_local365859837_0002_r_000000_0' done.
2020-11-16 22:07:06  [ pool-9-thread-1:3093 ] - [ INFO ]  Finishing task: attempt_local365859837_0002_r_000000_0
2020-11-16 22:07:06  [ Thread-46:3093 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:07:06  [ main:3653 ] - [ INFO ]  Job job_local365859837_0002 running in uber mode : false
2020-11-16 22:07:06  [ main:3653 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:07:06  [ main:3653 ] - [ INFO ]  Job job_local365859837_0002 completed successfully
2020-11-16 22:07:06  [ main:3656 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1686
		FILE: Number of bytes written=1132011
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=600
		HDFS: Number of bytes written=621
		HDFS: Number of read operations=43
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=141
		Map output materialized bytes=165
		Input split bytes=128
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=165
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=844103680
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=141
	File Output Format Counters 
		Bytes Written=339
2020-11-16 22:08:49  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:08:50  [ main:586 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:08:50  [ main:587 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:08:50  [ main:801 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:08:50  [ main:805 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:08:50  [ main:839 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:08:50  [ main:935 ] - [ INFO ]  number of splits:1
2020-11-16 22:08:50  [ main:993 ] - [ INFO ]  Submitting tokens for job: job_local402295240_0001
2020-11-16 22:08:50  [ main:1071 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:08:50  [ main:1071 ] - [ INFO ]  Running job: job_local402295240_0001
2020-11-16 22:08:50  [ Thread-18:1072 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:08:50  [ Thread-18:1074 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:08:50  [ Thread-18:1075 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:08:50  [ Thread-18:1108 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1108 ] - [ INFO ]  Starting task: attempt_local402295240_0001_m_000000_0
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1121 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1124 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1125 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1126 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1171 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1171 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1172 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1172 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1172 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1173 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1304 ] - [ INFO ]  
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1305 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1305 ] - [ INFO ]  Spilling map output
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1305 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1305 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1311 ] - [ INFO ]  Finished spill 0
2020-11-16 22:08:50  [ LocalJobRunner Map Task Executor #0:1313 ] - [ INFO ]  Task:attempt_local402295240_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:08:51  [ LocalJobRunner Map Task Executor #0:1366 ] - [ INFO ]  map
2020-11-16 22:08:51  [ LocalJobRunner Map Task Executor #0:1366 ] - [ INFO ]  Task 'attempt_local402295240_0001_m_000000_0' done.
2020-11-16 22:08:51  [ LocalJobRunner Map Task Executor #0:1366 ] - [ INFO ]  Finishing task: attempt_local402295240_0001_m_000000_0
2020-11-16 22:08:51  [ Thread-18:1366 ] - [ INFO ]  map task executor complete.
2020-11-16 22:08:51  [ Thread-18:1368 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:08:51  [ pool-6-thread-1:1368 ] - [ INFO ]  Starting task: attempt_local402295240_0001_r_000000_0
2020-11-16 22:08:51  [ pool-6-thread-1:1372 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:08:51  [ pool-6-thread-1:1373 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:08:51  [ pool-6-thread-1:1373 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:08:51  [ pool-6-thread-1:1374 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@e359024
2020-11-16 22:08:51  [ pool-6-thread-1:1382 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:08:51  [ EventFetcher for fetching Map Completion Events:1384 ] - [ INFO ]  attempt_local402295240_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:08:51  [ localfetcher#1:1406 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local402295240_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 22:08:51  [ localfetcher#1:1410 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local402295240_0001_m_000000_0
2020-11-16 22:08:51  [ localfetcher#1:1411 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 22:08:51  [ EventFetcher for fetching Map Completion Events:1411 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:08:51  [ pool-6-thread-1:1412 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:08:51  [ pool-6-thread-1:1412 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:08:51  [ pool-6-thread-1:1417 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:08:51  [ pool-6-thread-1:1417 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:08:51  [ pool-6-thread-1:1418 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 22:08:51  [ pool-6-thread-1:1418 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 22:08:51  [ pool-6-thread-1:1418 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:08:51  [ pool-6-thread-1:1418 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:08:51  [ pool-6-thread-1:1419 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:08:51  [ pool-6-thread-1:1419 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:08:51  [ pool-6-thread-1:1447 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:08:51  [ pool-6-thread-1:1585 ] - [ INFO ]  Task:attempt_local402295240_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:08:51  [ pool-6-thread-1:1602 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:08:51  [ pool-6-thread-1:1602 ] - [ INFO ]  Task attempt_local402295240_0001_r_000000_0 is allowed to commit now
2020-11-16 22:08:51  [ pool-6-thread-1:1667 ] - [ INFO ]  Saved output of task 'attempt_local402295240_0001_r_000000_0' to hdfs://master:9000/tmp583239924/output1605535729284/_temporary/0/task_local402295240_0001_r_000000
2020-11-16 22:08:51  [ pool-6-thread-1:1667 ] - [ INFO ]  reduce > reduce
2020-11-16 22:08:51  [ pool-6-thread-1:1667 ] - [ INFO ]  Task 'attempt_local402295240_0001_r_000000_0' done.
2020-11-16 22:08:51  [ pool-6-thread-1:1667 ] - [ INFO ]  Finishing task: attempt_local402295240_0001_r_000000_0
2020-11-16 22:08:51  [ Thread-18:1667 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:08:51  [ main:2076 ] - [ INFO ]  Job job_local402295240_0001 running in uber mode : false
2020-11-16 22:08:51  [ main:2077 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:08:51  [ main:2077 ] - [ INFO ]  Job job_local402295240_0001 completed successfully
2020-11-16 22:08:51  [ main:2083 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565606
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 22:08:51  [ main:2150 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 22:08:51  [ main:2166 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:08:51  [ main:2170 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:08:51  [ main:2190 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:08:51  [ main:2222 ] - [ INFO ]  number of splits:1
2020-11-16 22:08:51  [ main:2244 ] - [ INFO ]  Submitting tokens for job: job_local1045078716_0002
2020-11-16 22:08:51  [ main:2287 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:08:51  [ main:2287 ] - [ INFO ]  Running job: job_local1045078716_0002
2020-11-16 22:08:51  [ Thread-46:2287 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:08:51  [ Thread-46:2287 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:08:51  [ Thread-46:2288 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:08:51  [ Thread-46:2321 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:08:51  [ LocalJobRunner Map Task Executor #0:2322 ] - [ INFO ]  Starting task: attempt_local1045078716_0002_m_000000_0
2020-11-16 22:08:51  [ LocalJobRunner Map Task Executor #0:2322 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2323 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2323 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2324 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp583239924/output1605535729284/part-r-00000:0+141
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2365 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2365 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2365 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2365 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2365 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2365 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2397 ] - [ INFO ]  
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2397 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2397 ] - [ INFO ]  Spilling map output
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2397 ] - [ INFO ]  bufstart = 0; bufend = 141; bufvoid = 104857600
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2397 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2398 ] - [ INFO ]  Finished spill 0
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2399 ] - [ INFO ]  Task:attempt_local1045078716_0002_m_000000_0 is done. And is in the process of committing
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2414 ] - [ INFO ]  map
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2414 ] - [ INFO ]  Task 'attempt_local1045078716_0002_m_000000_0' done.
2020-11-16 22:08:52  [ LocalJobRunner Map Task Executor #0:2414 ] - [ INFO ]  Finishing task: attempt_local1045078716_0002_m_000000_0
2020-11-16 22:08:52  [ Thread-46:2414 ] - [ INFO ]  map task executor complete.
2020-11-16 22:08:52  [ Thread-46:2415 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:08:52  [ pool-9-thread-1:2415 ] - [ INFO ]  Starting task: attempt_local1045078716_0002_r_000000_0
2020-11-16 22:08:52  [ pool-9-thread-1:2415 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:08:52  [ pool-9-thread-1:2416 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:08:52  [ pool-9-thread-1:2416 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:08:52  [ pool-9-thread-1:2416 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3cc05ee9
2020-11-16 22:08:52  [ pool-9-thread-1:2416 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:08:52  [ EventFetcher for fetching Map Completion Events:2416 ] - [ INFO ]  attempt_local1045078716_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:08:52  [ localfetcher#2:2417 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local1045078716_0002_m_000000_0 decomp: 161 len: 165 to MEMORY
2020-11-16 22:08:52  [ localfetcher#2:2418 ] - [ INFO ]  Read 161 bytes from map-output for attempt_local1045078716_0002_m_000000_0
2020-11-16 22:08:52  [ localfetcher#2:2418 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 161, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->161
2020-11-16 22:08:52  [ EventFetcher for fetching Map Completion Events:2418 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:08:52  [ pool-9-thread-1:2418 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:08:52  [ pool-9-thread-1:2418 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:08:52  [ pool-9-thread-1:2419 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:08:52  [ pool-9-thread-1:2419 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 22:08:52  [ pool-9-thread-1:2420 ] - [ INFO ]  Merged 1 segments, 161 bytes to disk to satisfy reduce memory limit
2020-11-16 22:08:52  [ pool-9-thread-1:2420 ] - [ INFO ]  Merging 1 files, 165 bytes from disk
2020-11-16 22:08:52  [ pool-9-thread-1:2420 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:08:52  [ pool-9-thread-1:2420 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:08:52  [ pool-9-thread-1:2420 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 22:08:52  [ pool-9-thread-1:2420 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:08:52  [ pool-9-thread-1:2619 ] - [ INFO ]  Task:attempt_local1045078716_0002_r_000000_0 is done. And is in the process of committing
2020-11-16 22:08:52  [ pool-9-thread-1:2636 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:08:52  [ pool-9-thread-1:2636 ] - [ INFO ]  Task attempt_local1045078716_0002_r_000000_0 is allowed to commit now
2020-11-16 22:08:52  [ pool-9-thread-1:2712 ] - [ INFO ]  Saved output of task 'attempt_local1045078716_0002_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/_temporary/0/task_local1045078716_0002_r_000000
2020-11-16 22:08:52  [ pool-9-thread-1:2712 ] - [ INFO ]  reduce > reduce
2020-11-16 22:08:52  [ pool-9-thread-1:2713 ] - [ INFO ]  Task 'attempt_local1045078716_0002_r_000000_0' done.
2020-11-16 22:08:52  [ pool-9-thread-1:2713 ] - [ INFO ]  Finishing task: attempt_local1045078716_0002_r_000000_0
2020-11-16 22:08:52  [ Thread-46:2713 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:08:52  [ main:3290 ] - [ INFO ]  Job job_local1045078716_0002 running in uber mode : false
2020-11-16 22:08:52  [ main:3290 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:08:52  [ main:3290 ] - [ INFO ]  Job job_local1045078716_0002 completed successfully
2020-11-16 22:08:52  [ main:3294 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1688
		FILE: Number of bytes written=1132009
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=600
		HDFS: Number of bytes written=621
		HDFS: Number of read operations=43
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=141
		Map output materialized bytes=165
		Input split bytes=129
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=165
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=844103680
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=141
	File Output Format Counters 
		Bytes Written=339
2020-11-16 22:09:51  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:09:51  [ main:568 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:09:51  [ main:569 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:09:51  [ main:748 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:09:51  [ main:753 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:09:51  [ main:772 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:09:52  [ main:879 ] - [ INFO ]  number of splits:1
2020-11-16 22:09:52  [ main:937 ] - [ INFO ]  Submitting tokens for job: job_local2008150091_0001
2020-11-16 22:09:52  [ main:1020 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:09:52  [ main:1020 ] - [ INFO ]  Running job: job_local2008150091_0001
2020-11-16 22:09:52  [ Thread-18:1020 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:09:52  [ Thread-18:1024 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:09:52  [ Thread-18:1025 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:09:52  [ Thread-18:1067 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1067 ] - [ INFO ]  Starting task: attempt_local2008150091_0001_m_000000_0
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1082 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1086 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1086 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1088 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1141 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1141 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1141 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1141 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1141 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1143 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1251 ] - [ INFO ]  
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  Spilling map output
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1258 ] - [ INFO ]  Finished spill 0
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1260 ] - [ INFO ]  Task:attempt_local2008150091_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1282 ] - [ INFO ]  map
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1283 ] - [ INFO ]  Task 'attempt_local2008150091_0001_m_000000_0' done.
2020-11-16 22:09:52  [ LocalJobRunner Map Task Executor #0:1283 ] - [ INFO ]  Finishing task: attempt_local2008150091_0001_m_000000_0
2020-11-16 22:09:52  [ Thread-18:1283 ] - [ INFO ]  map task executor complete.
2020-11-16 22:09:52  [ Thread-18:1285 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:09:52  [ pool-6-thread-1:1285 ] - [ INFO ]  Starting task: attempt_local2008150091_0001_r_000000_0
2020-11-16 22:09:52  [ pool-6-thread-1:1288 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:09:52  [ pool-6-thread-1:1289 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:09:52  [ pool-6-thread-1:1289 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:09:52  [ pool-6-thread-1:1291 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@52a6e9fd
2020-11-16 22:09:52  [ pool-6-thread-1:1298 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:09:52  [ EventFetcher for fetching Map Completion Events:1300 ] - [ INFO ]  attempt_local2008150091_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:09:52  [ localfetcher#1:1321 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local2008150091_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 22:09:52  [ localfetcher#1:1325 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local2008150091_0001_m_000000_0
2020-11-16 22:09:52  [ localfetcher#1:1326 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 22:09:52  [ EventFetcher for fetching Map Completion Events:1326 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:09:52  [ pool-6-thread-1:1327 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:09:52  [ pool-6-thread-1:1327 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:09:52  [ pool-6-thread-1:1332 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:09:52  [ pool-6-thread-1:1332 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:09:52  [ pool-6-thread-1:1333 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 22:09:52  [ pool-6-thread-1:1334 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 22:09:52  [ pool-6-thread-1:1334 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:09:52  [ pool-6-thread-1:1334 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:09:52  [ pool-6-thread-1:1334 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:09:52  [ pool-6-thread-1:1335 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:09:52  [ pool-6-thread-1:1357 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:09:52  [ pool-6-thread-1:1522 ] - [ INFO ]  Task:attempt_local2008150091_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:09:52  [ pool-6-thread-1:1572 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:09:52  [ pool-6-thread-1:1572 ] - [ INFO ]  Task attempt_local2008150091_0001_r_000000_0 is allowed to commit now
2020-11-16 22:09:52  [ pool-6-thread-1:1626 ] - [ INFO ]  Saved output of task 'attempt_local2008150091_0001_r_000000_0' to hdfs://master:9000/tmp1184734434/output1605535790750/_temporary/0/task_local2008150091_0001_r_000000
2020-11-16 22:09:52  [ pool-6-thread-1:1627 ] - [ INFO ]  reduce > reduce
2020-11-16 22:09:52  [ pool-6-thread-1:1627 ] - [ INFO ]  Task 'attempt_local2008150091_0001_r_000000_0' done.
2020-11-16 22:09:52  [ pool-6-thread-1:1627 ] - [ INFO ]  Finishing task: attempt_local2008150091_0001_r_000000_0
2020-11-16 22:09:52  [ Thread-18:1627 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:09:53  [ main:2022 ] - [ INFO ]  Job job_local2008150091_0001 running in uber mode : false
2020-11-16 22:09:53  [ main:2023 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:09:53  [ main:2024 ] - [ INFO ]  Job job_local2008150091_0001 completed successfully
2020-11-16 22:09:53  [ main:2031 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568642
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 22:09:53  [ main:2063 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 22:09:53  [ main:2076 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:09:53  [ main:2080 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:09:53  [ main:2114 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:09:53  [ main:2147 ] - [ INFO ]  number of splits:1
2020-11-16 22:09:53  [ main:2165 ] - [ INFO ]  Submitting tokens for job: job_local139508969_0002
2020-11-16 22:09:53  [ main:2208 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:09:53  [ main:2208 ] - [ INFO ]  Running job: job_local139508969_0002
2020-11-16 22:09:53  [ Thread-46:2208 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:09:53  [ Thread-46:2208 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:09:53  [ Thread-46:2209 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:09:53  [ Thread-46:2229 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2229 ] - [ INFO ]  Starting task: attempt_local139508969_0002_m_000000_0
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2230 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2230 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2230 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2231 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp1184734434/output1605535790750/part-r-00000:0+141
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2273 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2273 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2273 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2273 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2273 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2274 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2294 ] - [ INFO ]  
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2294 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2294 ] - [ INFO ]  Spilling map output
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2294 ] - [ INFO ]  bufstart = 0; bufend = 141; bufvoid = 104857600
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2294 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2295 ] - [ INFO ]  Finished spill 0
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2296 ] - [ INFO ]  Task:attempt_local139508969_0002_m_000000_0 is done. And is in the process of committing
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2310 ] - [ INFO ]  map
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2310 ] - [ INFO ]  Task 'attempt_local139508969_0002_m_000000_0' done.
2020-11-16 22:09:53  [ LocalJobRunner Map Task Executor #0:2310 ] - [ INFO ]  Finishing task: attempt_local139508969_0002_m_000000_0
2020-11-16 22:09:53  [ Thread-46:2310 ] - [ INFO ]  map task executor complete.
2020-11-16 22:09:53  [ Thread-46:2310 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:09:53  [ pool-9-thread-1:2310 ] - [ INFO ]  Starting task: attempt_local139508969_0002_r_000000_0
2020-11-16 22:09:53  [ pool-9-thread-1:2311 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:09:53  [ pool-9-thread-1:2311 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:09:53  [ pool-9-thread-1:2311 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:09:53  [ pool-9-thread-1:2312 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7957a7a7
2020-11-16 22:09:53  [ pool-9-thread-1:2312 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:09:53  [ EventFetcher for fetching Map Completion Events:2312 ] - [ INFO ]  attempt_local139508969_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:09:53  [ localfetcher#2:2313 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local139508969_0002_m_000000_0 decomp: 161 len: 165 to MEMORY
2020-11-16 22:09:53  [ localfetcher#2:2313 ] - [ INFO ]  Read 161 bytes from map-output for attempt_local139508969_0002_m_000000_0
2020-11-16 22:09:53  [ localfetcher#2:2313 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 161, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->161
2020-11-16 22:09:53  [ EventFetcher for fetching Map Completion Events:2314 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:09:53  [ pool-9-thread-1:2314 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:09:53  [ pool-9-thread-1:2314 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:09:53  [ pool-9-thread-1:2315 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:09:53  [ pool-9-thread-1:2315 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 22:09:53  [ pool-9-thread-1:2315 ] - [ INFO ]  Merged 1 segments, 161 bytes to disk to satisfy reduce memory limit
2020-11-16 22:09:53  [ pool-9-thread-1:2316 ] - [ INFO ]  Merging 1 files, 165 bytes from disk
2020-11-16 22:09:53  [ pool-9-thread-1:2316 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:09:53  [ pool-9-thread-1:2316 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:09:53  [ pool-9-thread-1:2316 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 159 bytes
2020-11-16 22:09:53  [ pool-9-thread-1:2316 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:09:53  [ pool-9-thread-1:2446 ] - [ INFO ]  Task:attempt_local139508969_0002_r_000000_0 is done. And is in the process of committing
2020-11-16 22:09:53  [ pool-9-thread-1:2455 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:09:53  [ pool-9-thread-1:2455 ] - [ INFO ]  Task attempt_local139508969_0002_r_000000_0 is allowed to commit now
2020-11-16 22:09:53  [ pool-9-thread-1:2497 ] - [ INFO ]  Saved output of task 'attempt_local139508969_0002_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/_temporary/0/task_local139508969_0002_r_000000
2020-11-16 22:09:53  [ pool-9-thread-1:2497 ] - [ INFO ]  reduce > reduce
2020-11-16 22:09:53  [ pool-9-thread-1:2497 ] - [ INFO ]  Task 'attempt_local139508969_0002_r_000000_0' done.
2020-11-16 22:09:53  [ pool-9-thread-1:2497 ] - [ INFO ]  Finishing task: attempt_local139508969_0002_r_000000_0
2020-11-16 22:09:53  [ Thread-46:2497 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:09:54  [ main:3209 ] - [ INFO ]  Job job_local139508969_0002 running in uber mode : false
2020-11-16 22:09:54  [ main:3209 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:09:54  [ main:3210 ] - [ INFO ]  Job job_local139508969_0002 completed successfully
2020-11-16 22:09:54  [ main:3212 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1690
		FILE: Number of bytes written=1132031
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=600
		HDFS: Number of bytes written=621
		HDFS: Number of read operations=43
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=141
		Map output materialized bytes=165
		Input split bytes=130
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=165
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=843055104
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=141
	File Output Format Counters 
		Bytes Written=339
2020-11-16 22:10:48  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:10:49  [ main:601 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:10:49  [ main:602 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:10:49  [ main:791 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:10:49  [ main:796 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:10:49  [ main:817 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:10:49  [ main:895 ] - [ INFO ]  number of splits:1
2020-11-16 22:10:49  [ main:968 ] - [ INFO ]  Submitting tokens for job: job_local388625918_0001
2020-11-16 22:10:49  [ main:1064 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:10:49  [ main:1064 ] - [ INFO ]  Running job: job_local388625918_0001
2020-11-16 22:10:49  [ Thread-18:1065 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:10:49  [ Thread-18:1068 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:10:49  [ Thread-18:1069 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:10:49  [ Thread-18:1142 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1142 ] - [ INFO ]  Starting task: attempt_local388625918_0001_m_000000_0
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1157 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1164 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:10:49  [ LocalJobRunner Map Task Executor #0:1215 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1343 ] - [ INFO ]  
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1345 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1345 ] - [ INFO ]  Spilling map output
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1345 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1345 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1351 ] - [ INFO ]  Finished spill 0
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1354 ] - [ INFO ]  Task:attempt_local388625918_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1463 ] - [ INFO ]  map
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1463 ] - [ INFO ]  Task 'attempt_local388625918_0001_m_000000_0' done.
2020-11-16 22:10:50  [ LocalJobRunner Map Task Executor #0:1463 ] - [ INFO ]  Finishing task: attempt_local388625918_0001_m_000000_0
2020-11-16 22:10:50  [ Thread-18:1464 ] - [ INFO ]  map task executor complete.
2020-11-16 22:10:50  [ Thread-18:1465 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:10:50  [ pool-6-thread-1:1465 ] - [ INFO ]  Starting task: attempt_local388625918_0001_r_000000_0
2020-11-16 22:10:50  [ pool-6-thread-1:1469 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:10:50  [ pool-6-thread-1:1469 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:10:50  [ pool-6-thread-1:1469 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:10:50  [ pool-6-thread-1:1471 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@619f5e2f
2020-11-16 22:10:50  [ pool-6-thread-1:1478 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:10:50  [ EventFetcher for fetching Map Completion Events:1480 ] - [ INFO ]  attempt_local388625918_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:10:50  [ localfetcher#1:1497 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local388625918_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 22:10:50  [ localfetcher#1:1501 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local388625918_0001_m_000000_0
2020-11-16 22:10:50  [ localfetcher#1:1502 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 22:10:50  [ EventFetcher for fetching Map Completion Events:1503 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:10:50  [ pool-6-thread-1:1503 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:10:50  [ pool-6-thread-1:1503 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:10:50  [ pool-6-thread-1:1507 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:10:50  [ pool-6-thread-1:1507 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:10:50  [ pool-6-thread-1:1508 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 22:10:50  [ pool-6-thread-1:1508 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 22:10:50  [ pool-6-thread-1:1509 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:10:50  [ pool-6-thread-1:1509 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:10:50  [ pool-6-thread-1:1509 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:10:50  [ pool-6-thread-1:1509 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:10:50  [ pool-6-thread-1:1637 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:10:50  [ pool-6-thread-1:1796 ] - [ INFO ]  Task:attempt_local388625918_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:10:50  [ pool-6-thread-1:1802 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:10:50  [ pool-6-thread-1:1802 ] - [ INFO ]  Task attempt_local388625918_0001_r_000000_0 is allowed to commit now
2020-11-16 22:10:50  [ pool-6-thread-1:1867 ] - [ INFO ]  Saved output of task 'attempt_local388625918_0001_r_000000_0' to hdfs://master:9000/tmp1991690926/output1605535848383/_temporary/0/task_local388625918_0001_r_000000
2020-11-16 22:10:50  [ pool-6-thread-1:1868 ] - [ INFO ]  reduce > reduce
2020-11-16 22:10:50  [ pool-6-thread-1:1868 ] - [ INFO ]  Task 'attempt_local388625918_0001_r_000000_0' done.
2020-11-16 22:10:50  [ pool-6-thread-1:1868 ] - [ INFO ]  Finishing task: attempt_local388625918_0001_r_000000_0
2020-11-16 22:10:50  [ Thread-18:1868 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:10:50  [ main:2067 ] - [ INFO ]  Job job_local388625918_0001 running in uber mode : false
2020-11-16 22:10:50  [ main:2068 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:10:50  [ main:2068 ] - [ INFO ]  Job job_local388625918_0001 completed successfully
2020-11-16 22:10:50  [ main:2075 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565610
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 22:10:50  [ main:2098 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 22:10:50  [ main:2117 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:10:50  [ main:2121 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:10:50  [ main:2195 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:10:50  [ main:2229 ] - [ INFO ]  number of splits:1
2020-11-16 22:10:51  [ main:2250 ] - [ INFO ]  Submitting tokens for job: job_local1769682995_0002
2020-11-16 22:10:51  [ main:2295 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:10:51  [ main:2296 ] - [ INFO ]  Running job: job_local1769682995_0002
2020-11-16 22:10:51  [ Thread-46:2296 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:10:51  [ Thread-46:2296 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:10:51  [ Thread-46:2296 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:10:51  [ Thread-46:2323 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2323 ] - [ INFO ]  Starting task: attempt_local1769682995_0002_m_000000_0
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2324 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2325 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2325 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2326 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp1991690926/output1605535848383/part-r-00000:0+141
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2372 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2372 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2372 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2372 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2372 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2373 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:10:51  [ LocalJobRunner Map Task Executor #0:2447 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:10:51  [ Thread-46:2449 ] - [ INFO ]  map task executor complete.
2020-11-16 22:10:51  [ Thread-46:2633 ] - [ WARN ]  job_local1769682995_0002
java.lang.Exception: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.NullWritable, received com.satan.hadoop.model.result.Company
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.io.IOException: Type mismatch in key from map: expected org.apache.hadoop.io.NullWritable, received com.satan.hadoop.model.result.Company
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1074)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.satan.hadoop.mr.MultipleMapReduceJob$MultipleMapper2.map(MultipleMapReduceJob.java:62)
	at com.satan.hadoop.mr.MultipleMapReduceJob$MultipleMapper2.map(MultipleMapReduceJob.java:51)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-11-16 22:10:52  [ main:3299 ] - [ INFO ]  Job job_local1769682995_0002 running in uber mode : false
2020-11-16 22:10:52  [ main:3299 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 22:10:52  [ main:3299 ] - [ INFO ]  Job job_local1769682995_0002 failed with state FAILED due to: NA
2020-11-16 22:10:52  [ main:3299 ] - [ INFO ]  Counters: 0
2020-11-16 22:11:14  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:11:15  [ main:630 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:11:15  [ main:630 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:11:15  [ main:828 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:11:15  [ main:834 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:11:15  [ main:846 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:11:15  [ main:920 ] - [ INFO ]  number of splits:1
2020-11-16 22:11:15  [ main:985 ] - [ INFO ]  Submitting tokens for job: job_local1846976345_0001
2020-11-16 22:11:15  [ main:1079 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:11:15  [ main:1079 ] - [ INFO ]  Running job: job_local1846976345_0001
2020-11-16 22:11:15  [ Thread-18:1080 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:11:15  [ Thread-18:1083 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:11:15  [ Thread-18:1084 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:11:15  [ Thread-18:1132 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1132 ] - [ INFO ]  Starting task: attempt_local1846976345_0001_m_000000_0
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1147 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1153 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1208 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1312 ] - [ INFO ]  
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  Spilling map output
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1320 ] - [ INFO ]  Finished spill 0
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1323 ] - [ INFO ]  Task:attempt_local1846976345_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1342 ] - [ INFO ]  map
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1342 ] - [ INFO ]  Task 'attempt_local1846976345_0001_m_000000_0' done.
2020-11-16 22:11:15  [ LocalJobRunner Map Task Executor #0:1342 ] - [ INFO ]  Finishing task: attempt_local1846976345_0001_m_000000_0
2020-11-16 22:11:15  [ Thread-18:1342 ] - [ INFO ]  map task executor complete.
2020-11-16 22:11:15  [ Thread-18:1344 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:11:15  [ pool-6-thread-1:1344 ] - [ INFO ]  Starting task: attempt_local1846976345_0001_r_000000_0
2020-11-16 22:11:15  [ pool-6-thread-1:1349 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:11:15  [ pool-6-thread-1:1350 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:11:15  [ pool-6-thread-1:1350 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:11:15  [ pool-6-thread-1:1351 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@71d204e2
2020-11-16 22:11:15  [ pool-6-thread-1:1361 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:11:15  [ EventFetcher for fetching Map Completion Events:1362 ] - [ INFO ]  attempt_local1846976345_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:11:15  [ localfetcher#1:1382 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1846976345_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 22:11:15  [ localfetcher#1:1386 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local1846976345_0001_m_000000_0
2020-11-16 22:11:15  [ localfetcher#1:1387 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 22:11:15  [ EventFetcher for fetching Map Completion Events:1387 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:11:15  [ pool-6-thread-1:1388 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:11:15  [ pool-6-thread-1:1388 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:11:15  [ pool-6-thread-1:1392 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:11:15  [ pool-6-thread-1:1392 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:11:15  [ pool-6-thread-1:1393 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 22:11:15  [ pool-6-thread-1:1393 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 22:11:15  [ pool-6-thread-1:1393 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:11:15  [ pool-6-thread-1:1393 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:11:15  [ pool-6-thread-1:1394 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:11:15  [ pool-6-thread-1:1394 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:11:16  [ pool-6-thread-1:1416 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:11:16  [ pool-6-thread-1:1567 ] - [ INFO ]  Task:attempt_local1846976345_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:11:16  [ pool-6-thread-1:1586 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:11:16  [ pool-6-thread-1:1586 ] - [ INFO ]  Task attempt_local1846976345_0001_r_000000_0 is allowed to commit now
2020-11-16 22:11:16  [ pool-6-thread-1:1628 ] - [ INFO ]  Saved output of task 'attempt_local1846976345_0001_r_000000_0' to hdfs://master:9000/tmp2037160262/output1605535874185/_temporary/0/task_local1846976345_0001_r_000000
2020-11-16 22:11:16  [ pool-6-thread-1:1629 ] - [ INFO ]  reduce > reduce
2020-11-16 22:11:16  [ pool-6-thread-1:1629 ] - [ INFO ]  Task 'attempt_local1846976345_0001_r_000000_0' done.
2020-11-16 22:11:16  [ pool-6-thread-1:1629 ] - [ INFO ]  Finishing task: attempt_local1846976345_0001_r_000000_0
2020-11-16 22:11:16  [ Thread-18:1629 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:11:16  [ main:2085 ] - [ INFO ]  Job job_local1846976345_0001 running in uber mode : false
2020-11-16 22:11:16  [ main:2086 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:11:16  [ main:2087 ] - [ INFO ]  Job job_local1846976345_0001 completed successfully
2020-11-16 22:11:16  [ main:2095 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=568642
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 22:11:16  [ main:2130 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 22:11:16  [ main:2146 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:11:16  [ main:2151 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:11:16  [ main:2179 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:11:16  [ main:2214 ] - [ INFO ]  number of splits:1
2020-11-16 22:11:16  [ main:2234 ] - [ INFO ]  Submitting tokens for job: job_local1038981215_0002
2020-11-16 22:11:16  [ main:2274 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:11:16  [ main:2274 ] - [ INFO ]  Running job: job_local1038981215_0002
2020-11-16 22:11:16  [ Thread-46:2274 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:11:16  [ Thread-46:2274 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:11:16  [ Thread-46:2275 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:11:16  [ Thread-46:2282 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2282 ] - [ INFO ]  Starting task: attempt_local1038981215_0002_m_000000_0
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2283 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2283 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2283 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2284 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp2037160262/output1605535874185/part-r-00000:0+141
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2324 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2324 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2324 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2324 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2324 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2325 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2396 ] - [ INFO ]  
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2396 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2396 ] - [ INFO ]  Spilling map output
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2396 ] - [ INFO ]  bufstart = 0; bufend = 141; bufvoid = 104857600
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2396 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2397 ] - [ INFO ]  Finished spill 0
2020-11-16 22:11:16  [ LocalJobRunner Map Task Executor #0:2398 ] - [ INFO ]  Task:attempt_local1038981215_0002_m_000000_0 is done. And is in the process of committing
2020-11-16 22:11:17  [ LocalJobRunner Map Task Executor #0:2418 ] - [ INFO ]  map
2020-11-16 22:11:17  [ LocalJobRunner Map Task Executor #0:2418 ] - [ INFO ]  Task 'attempt_local1038981215_0002_m_000000_0' done.
2020-11-16 22:11:17  [ LocalJobRunner Map Task Executor #0:2418 ] - [ INFO ]  Finishing task: attempt_local1038981215_0002_m_000000_0
2020-11-16 22:11:17  [ Thread-46:2418 ] - [ INFO ]  map task executor complete.
2020-11-16 22:11:17  [ Thread-46:2419 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:11:17  [ pool-9-thread-1:2419 ] - [ INFO ]  Starting task: attempt_local1038981215_0002_r_000000_0
2020-11-16 22:11:17  [ pool-9-thread-1:2420 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:11:17  [ pool-9-thread-1:2420 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:11:17  [ pool-9-thread-1:2420 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:11:17  [ pool-9-thread-1:2420 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6c59b8e2
2020-11-16 22:11:17  [ pool-9-thread-1:2420 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:11:17  [ EventFetcher for fetching Map Completion Events:2421 ] - [ INFO ]  attempt_local1038981215_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:11:17  [ localfetcher#2:2422 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local1038981215_0002_m_000000_0 decomp: 161 len: 165 to MEMORY
2020-11-16 22:11:17  [ localfetcher#2:2422 ] - [ INFO ]  Read 161 bytes from map-output for attempt_local1038981215_0002_m_000000_0
2020-11-16 22:11:17  [ localfetcher#2:2422 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 161, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->161
2020-11-16 22:11:17  [ EventFetcher for fetching Map Completion Events:2422 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:11:17  [ pool-9-thread-1:2423 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:11:17  [ pool-9-thread-1:2423 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:11:17  [ pool-9-thread-1:2423 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:11:17  [ pool-9-thread-1:2424 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 143 bytes
2020-11-16 22:11:17  [ pool-9-thread-1:2424 ] - [ INFO ]  Merged 1 segments, 161 bytes to disk to satisfy reduce memory limit
2020-11-16 22:11:17  [ pool-9-thread-1:2424 ] - [ INFO ]  Merging 1 files, 165 bytes from disk
2020-11-16 22:11:17  [ pool-9-thread-1:2424 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:11:17  [ pool-9-thread-1:2424 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:11:17  [ pool-9-thread-1:2424 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 143 bytes
2020-11-16 22:11:17  [ pool-9-thread-1:2425 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:11:17  [ pool-9-thread-1:2588 ] - [ INFO ]  Task:attempt_local1038981215_0002_r_000000_0 is done. And is in the process of committing
2020-11-16 22:11:17  [ pool-9-thread-1:2595 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:11:17  [ pool-9-thread-1:2595 ] - [ INFO ]  Task attempt_local1038981215_0002_r_000000_0 is allowed to commit now
2020-11-16 22:11:17  [ pool-9-thread-1:2639 ] - [ INFO ]  Saved output of task 'attempt_local1038981215_0002_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/_temporary/0/task_local1038981215_0002_r_000000
2020-11-16 22:11:17  [ pool-9-thread-1:2640 ] - [ INFO ]  reduce > reduce
2020-11-16 22:11:17  [ pool-9-thread-1:2640 ] - [ INFO ]  Task 'attempt_local1038981215_0002_r_000000_0' done.
2020-11-16 22:11:17  [ pool-9-thread-1:2640 ] - [ INFO ]  Finishing task: attempt_local1038981215_0002_r_000000_0
2020-11-16 22:11:17  [ Thread-46:2640 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:11:17  [ main:3274 ] - [ INFO ]  Job job_local1038981215_0002 running in uber mode : false
2020-11-16 22:11:17  [ main:3275 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:11:17  [ main:3275 ] - [ INFO ]  Job job_local1038981215_0002 completed successfully
2020-11-16 22:11:17  [ main:3277 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1690
		FILE: Number of bytes written=1135051
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=600
		HDFS: Number of bytes written=621
		HDFS: Number of read operations=43
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=141
		Map output materialized bytes=165
		Input split bytes=130
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=165
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=844103680
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=141
	File Output Format Counters 
		Bytes Written=339
2020-11-16 22:12:31  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:12:32  [ main:690 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:12:32  [ main:690 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:12:32  [ main:904 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:12:32  [ main:908 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:12:32  [ main:945 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:12:32  [ main:1033 ] - [ INFO ]  number of splits:1
2020-11-16 22:12:32  [ main:1094 ] - [ INFO ]  Submitting tokens for job: job_local384440125_0001
2020-11-16 22:12:33  [ main:1186 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:12:33  [ main:1187 ] - [ INFO ]  Running job: job_local384440125_0001
2020-11-16 22:12:33  [ Thread-18:1187 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:12:33  [ Thread-18:1190 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:12:33  [ Thread-18:1191 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:12:33  [ Thread-18:1237 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1237 ] - [ INFO ]  Starting task: attempt_local384440125_0001_m_000000_0
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1254 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1258 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1258 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1260 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1314 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1316 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1427 ] - [ INFO ]  
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1429 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1429 ] - [ INFO ]  Spilling map output
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1429 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1429 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1435 ] - [ INFO ]  Finished spill 0
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1437 ] - [ INFO ]  Task:attempt_local384440125_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1472 ] - [ INFO ]  map
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1472 ] - [ INFO ]  Task 'attempt_local384440125_0001_m_000000_0' done.
2020-11-16 22:12:33  [ LocalJobRunner Map Task Executor #0:1472 ] - [ INFO ]  Finishing task: attempt_local384440125_0001_m_000000_0
2020-11-16 22:12:33  [ Thread-18:1472 ] - [ INFO ]  map task executor complete.
2020-11-16 22:12:33  [ Thread-18:1473 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:12:33  [ pool-6-thread-1:1474 ] - [ INFO ]  Starting task: attempt_local384440125_0001_r_000000_0
2020-11-16 22:12:33  [ pool-6-thread-1:1477 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:12:33  [ pool-6-thread-1:1478 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:12:33  [ pool-6-thread-1:1478 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:12:33  [ pool-6-thread-1:1479 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@54c4f60f
2020-11-16 22:12:33  [ pool-6-thread-1:1486 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:12:33  [ EventFetcher for fetching Map Completion Events:1487 ] - [ INFO ]  attempt_local384440125_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:12:33  [ localfetcher#1:1505 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local384440125_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 22:12:33  [ localfetcher#1:1508 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local384440125_0001_m_000000_0
2020-11-16 22:12:33  [ localfetcher#1:1509 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 22:12:33  [ EventFetcher for fetching Map Completion Events:1509 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:12:33  [ pool-6-thread-1:1510 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:12:33  [ pool-6-thread-1:1510 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:12:33  [ pool-6-thread-1:1514 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:12:33  [ pool-6-thread-1:1514 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:12:33  [ pool-6-thread-1:1515 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 22:12:33  [ pool-6-thread-1:1515 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 22:12:33  [ pool-6-thread-1:1515 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:12:33  [ pool-6-thread-1:1516 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:12:33  [ pool-6-thread-1:1516 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:12:33  [ pool-6-thread-1:1516 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:12:33  [ pool-6-thread-1:1540 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:12:33  [ pool-6-thread-1:1767 ] - [ INFO ]  Task:attempt_local384440125_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:12:33  [ pool-6-thread-1:1796 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:12:33  [ pool-6-thread-1:1796 ] - [ INFO ]  Task attempt_local384440125_0001_r_000000_0 is allowed to commit now
2020-11-16 22:12:33  [ pool-6-thread-1:1855 ] - [ INFO ]  Saved output of task 'attempt_local384440125_0001_r_000000_0' to hdfs://master:9000/tmp992207020/output1605535951437/_temporary/0/task_local384440125_0001_r_000000
2020-11-16 22:12:33  [ pool-6-thread-1:1856 ] - [ INFO ]  reduce > reduce
2020-11-16 22:12:33  [ pool-6-thread-1:1856 ] - [ INFO ]  Task 'attempt_local384440125_0001_r_000000_0' done.
2020-11-16 22:12:33  [ pool-6-thread-1:1856 ] - [ INFO ]  Finishing task: attempt_local384440125_0001_r_000000_0
2020-11-16 22:12:33  [ Thread-18:1857 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:12:34  [ main:2188 ] - [ INFO ]  Job job_local384440125_0001 running in uber mode : false
2020-11-16 22:12:34  [ main:2189 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:12:34  [ main:2189 ] - [ INFO ]  Job job_local384440125_0001 completed successfully
2020-11-16 22:12:34  [ main:2197 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565606
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 22:12:34  [ main:2222 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 22:12:34  [ main:2242 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:12:34  [ main:2248 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:12:34  [ main:2356 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:12:34  [ main:2390 ] - [ INFO ]  number of splits:1
2020-11-16 22:12:34  [ main:2412 ] - [ INFO ]  Submitting tokens for job: job_local77078678_0002
2020-11-16 22:12:34  [ main:2457 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:12:34  [ main:2457 ] - [ INFO ]  Running job: job_local77078678_0002
2020-11-16 22:12:34  [ Thread-46:2457 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:12:34  [ Thread-46:2457 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:12:34  [ Thread-46:2458 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:12:34  [ Thread-46:2479 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2479 ] - [ INFO ]  Starting task: attempt_local77078678_0002_m_000000_0
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2479 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2480 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2480 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2481 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp992207020/output1605535951437/part-r-00000:0+141
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2525 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2525 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2525 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2525 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2525 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2525 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2565 ] - [ INFO ]  
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2565 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2565 ] - [ INFO ]  Spilling map output
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2565 ] - [ INFO ]  bufstart = 0; bufend = 141; bufvoid = 104857600
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2565 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2566 ] - [ INFO ]  Finished spill 0
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2567 ] - [ INFO ]  Task:attempt_local77078678_0002_m_000000_0 is done. And is in the process of committing
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2580 ] - [ INFO ]  map
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2580 ] - [ INFO ]  Task 'attempt_local77078678_0002_m_000000_0' done.
2020-11-16 22:12:34  [ LocalJobRunner Map Task Executor #0:2580 ] - [ INFO ]  Finishing task: attempt_local77078678_0002_m_000000_0
2020-11-16 22:12:34  [ Thread-46:2580 ] - [ INFO ]  map task executor complete.
2020-11-16 22:12:34  [ Thread-46:2581 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:12:34  [ pool-9-thread-1:2581 ] - [ INFO ]  Starting task: attempt_local77078678_0002_r_000000_0
2020-11-16 22:12:34  [ pool-9-thread-1:2581 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:12:34  [ pool-9-thread-1:2582 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:12:34  [ pool-9-thread-1:2582 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:12:34  [ pool-9-thread-1:2582 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@bdc8251
2020-11-16 22:12:34  [ pool-9-thread-1:2582 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:12:34  [ EventFetcher for fetching Map Completion Events:2583 ] - [ INFO ]  attempt_local77078678_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:12:34  [ localfetcher#2:2583 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local77078678_0002_m_000000_0 decomp: 161 len: 165 to MEMORY
2020-11-16 22:12:34  [ localfetcher#2:2584 ] - [ INFO ]  Read 161 bytes from map-output for attempt_local77078678_0002_m_000000_0
2020-11-16 22:12:34  [ localfetcher#2:2584 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 161, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->161
2020-11-16 22:12:34  [ EventFetcher for fetching Map Completion Events:2584 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:12:34  [ pool-9-thread-1:2584 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:12:34  [ pool-9-thread-1:2584 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:12:34  [ pool-9-thread-1:2585 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:12:34  [ pool-9-thread-1:2585 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 143 bytes
2020-11-16 22:12:34  [ pool-9-thread-1:2586 ] - [ INFO ]  Merged 1 segments, 161 bytes to disk to satisfy reduce memory limit
2020-11-16 22:12:34  [ pool-9-thread-1:2586 ] - [ INFO ]  Merging 1 files, 165 bytes from disk
2020-11-16 22:12:34  [ pool-9-thread-1:2586 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:12:34  [ pool-9-thread-1:2586 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:12:34  [ pool-9-thread-1:2586 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 143 bytes
2020-11-16 22:12:34  [ pool-9-thread-1:2586 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:12:34  [ pool-9-thread-1:2789 ] - [ INFO ]  Task:attempt_local77078678_0002_r_000000_0 is done. And is in the process of committing
2020-11-16 22:12:34  [ pool-9-thread-1:2801 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:12:34  [ pool-9-thread-1:2802 ] - [ INFO ]  Task attempt_local77078678_0002_r_000000_0 is allowed to commit now
2020-11-16 22:12:34  [ pool-9-thread-1:2852 ] - [ INFO ]  Saved output of task 'attempt_local77078678_0002_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/_temporary/0/task_local77078678_0002_r_000000
2020-11-16 22:12:34  [ pool-9-thread-1:2853 ] - [ INFO ]  reduce > reduce
2020-11-16 22:12:34  [ pool-9-thread-1:2853 ] - [ INFO ]  Task 'attempt_local77078678_0002_r_000000_0' done.
2020-11-16 22:12:34  [ pool-9-thread-1:2853 ] - [ INFO ]  Finishing task: attempt_local77078678_0002_r_000000_0
2020-11-16 22:12:34  [ Thread-46:2853 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:12:35  [ main:3460 ] - [ INFO ]  Job job_local77078678_0002 running in uber mode : false
2020-11-16 22:12:35  [ main:3460 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:12:35  [ main:3460 ] - [ INFO ]  Job job_local77078678_0002 completed successfully
2020-11-16 22:12:35  [ main:3464 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1688
		FILE: Number of bytes written=1125969
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=600
		HDFS: Number of bytes written=621
		HDFS: Number of read operations=43
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=141
		Map output materialized bytes=165
		Input split bytes=129
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=165
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=843055104
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=141
	File Output Format Counters 
		Bytes Written=339
2020-11-16 22:13:05  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:13:06  [ main:622 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:13:06  [ main:622 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:13:06  [ main:828 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:13:06  [ main:833 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:13:06  [ main:847 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:13:06  [ main:988 ] - [ INFO ]  number of splits:1
2020-11-16 22:13:06  [ main:1052 ] - [ INFO ]  Submitting tokens for job: job_local869250062_0001
2020-11-16 22:13:06  [ main:1144 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:13:06  [ main:1144 ] - [ INFO ]  Running job: job_local869250062_0001
2020-11-16 22:13:06  [ Thread-18:1145 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:13:06  [ Thread-18:1147 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:13:06  [ Thread-18:1149 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:13:06  [ Thread-18:1207 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1207 ] - [ INFO ]  Starting task: attempt_local869250062_0001_m_000000_0
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1222 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1226 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1226 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1229 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/muti_mr/mutil_mr.txt:0+159
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1279 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1279 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1279 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1279 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1279 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:13:06  [ LocalJobRunner Map Task Executor #0:1281 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1908 ] - [ INFO ]  
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1909 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1909 ] - [ INFO ]  Spilling map output
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1909 ] - [ INFO ]  bufstart = 0; bufend = 114; bufvoid = 104857600
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1910 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1915 ] - [ INFO ]  Finished spill 0
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1917 ] - [ INFO ]  Task:attempt_local869250062_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1932 ] - [ INFO ]  map
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1932 ] - [ INFO ]  Task 'attempt_local869250062_0001_m_000000_0' done.
2020-11-16 22:13:07  [ LocalJobRunner Map Task Executor #0:1932 ] - [ INFO ]  Finishing task: attempt_local869250062_0001_m_000000_0
2020-11-16 22:13:07  [ Thread-18:1932 ] - [ INFO ]  map task executor complete.
2020-11-16 22:13:07  [ Thread-18:1934 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:13:07  [ pool-6-thread-1:1934 ] - [ INFO ]  Starting task: attempt_local869250062_0001_r_000000_0
2020-11-16 22:13:07  [ pool-6-thread-1:1937 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:13:07  [ pool-6-thread-1:1938 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:13:07  [ pool-6-thread-1:1938 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:13:07  [ pool-6-thread-1:1939 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@52a6e9fd
2020-11-16 22:13:07  [ pool-6-thread-1:1946 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:13:07  [ EventFetcher for fetching Map Completion Events:1948 ] - [ INFO ]  attempt_local869250062_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:13:07  [ localfetcher#1:1966 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local869250062_0001_m_000000_0 decomp: 134 len: 138 to MEMORY
2020-11-16 22:13:07  [ localfetcher#1:1970 ] - [ INFO ]  Read 134 bytes from map-output for attempt_local869250062_0001_m_000000_0
2020-11-16 22:13:07  [ localfetcher#1:1971 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 134, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->134
2020-11-16 22:13:07  [ EventFetcher for fetching Map Completion Events:1972 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:13:07  [ pool-6-thread-1:1972 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:13:07  [ pool-6-thread-1:1972 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:13:07  [ pool-6-thread-1:1976 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:13:07  [ pool-6-thread-1:1976 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:13:07  [ pool-6-thread-1:1977 ] - [ INFO ]  Merged 1 segments, 134 bytes to disk to satisfy reduce memory limit
2020-11-16 22:13:07  [ pool-6-thread-1:1977 ] - [ INFO ]  Merging 1 files, 138 bytes from disk
2020-11-16 22:13:07  [ pool-6-thread-1:1978 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:13:07  [ pool-6-thread-1:1978 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:13:07  [ pool-6-thread-1:1978 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 124 bytes
2020-11-16 22:13:07  [ pool-6-thread-1:1978 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:13:07  [ pool-6-thread-1:2000 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:13:07  [ main:2147 ] - [ INFO ]  Job job_local869250062_0001 running in uber mode : false
2020-11-16 22:13:07  [ main:2148 ] - [ INFO ]   map 100% reduce 0%
2020-11-16 22:13:07  [ pool-6-thread-1:2163 ] - [ INFO ]  Task:attempt_local869250062_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:13:07  [ pool-6-thread-1:2191 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:13:07  [ pool-6-thread-1:2191 ] - [ INFO ]  Task attempt_local869250062_0001_r_000000_0 is allowed to commit now
2020-11-16 22:13:07  [ pool-6-thread-1:2285 ] - [ INFO ]  Saved output of task 'attempt_local869250062_0001_r_000000_0' to hdfs://master:9000/tmp1355794600/output1605535985307/_temporary/0/task_local869250062_0001_r_000000
2020-11-16 22:13:07  [ pool-6-thread-1:2286 ] - [ INFO ]  reduce > reduce
2020-11-16 22:13:07  [ pool-6-thread-1:2286 ] - [ INFO ]  Task 'attempt_local869250062_0001_r_000000_0' done.
2020-11-16 22:13:07  [ pool-6-thread-1:2286 ] - [ INFO ]  Finishing task: attempt_local869250062_0001_r_000000_0
2020-11-16 22:13:07  [ Thread-18:2286 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:13:08  [ main:3153 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:13:08  [ main:3154 ] - [ INFO ]  Job job_local869250062_0001 completed successfully
2020-11-16 22:13:08  [ main:3161 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=656
		FILE: Number of bytes written=565610
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=318
		HDFS: Number of bytes written=141
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=114
		Map output materialized bytes=138
		Input split bytes=122
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=138
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=630194176
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=159
	File Output Format Counters 
		Bytes Written=141
2020-11-16 22:13:08  [ main:3227 ] - [ INFO ]  Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
2020-11-16 22:13:08  [ main:3248 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:13:08  [ main:3252 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:13:08  [ main:3277 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:13:09  [ main:3312 ] - [ INFO ]  number of splits:1
2020-11-16 22:13:09  [ main:3332 ] - [ INFO ]  Submitting tokens for job: job_local1775655760_0002
2020-11-16 22:13:09  [ main:3372 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:13:09  [ main:3373 ] - [ INFO ]  Running job: job_local1775655760_0002
2020-11-16 22:13:09  [ Thread-46:3373 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:13:09  [ Thread-46:3373 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:13:09  [ Thread-46:3373 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:13:09  [ Thread-46:3383 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3383 ] - [ INFO ]  Starting task: attempt_local1775655760_0002_m_000000_0
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3384 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3384 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3384 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3385 ] - [ INFO ]  Processing split: hdfs://master:9000/tmp1355794600/output1605535985307/part-r-00000:0+141
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3429 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3429 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3429 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3429 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3429 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3430 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3467 ] - [ INFO ]  
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3467 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3467 ] - [ INFO ]  Spilling map output
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3467 ] - [ INFO ]  bufstart = 0; bufend = 141; bufvoid = 104857600
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3467 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214364(104857456); length = 33/6553600
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3469 ] - [ INFO ]  Finished spill 0
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3469 ] - [ INFO ]  Task:attempt_local1775655760_0002_m_000000_0 is done. And is in the process of committing
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3493 ] - [ INFO ]  map
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3493 ] - [ INFO ]  Task 'attempt_local1775655760_0002_m_000000_0' done.
2020-11-16 22:13:09  [ LocalJobRunner Map Task Executor #0:3494 ] - [ INFO ]  Finishing task: attempt_local1775655760_0002_m_000000_0
2020-11-16 22:13:09  [ Thread-46:3494 ] - [ INFO ]  map task executor complete.
2020-11-16 22:13:09  [ Thread-46:3494 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:13:09  [ pool-9-thread-1:3494 ] - [ INFO ]  Starting task: attempt_local1775655760_0002_r_000000_0
2020-11-16 22:13:09  [ pool-9-thread-1:3495 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:13:09  [ pool-9-thread-1:3496 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:13:09  [ pool-9-thread-1:3496 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:13:09  [ pool-9-thread-1:3496 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4d056fef
2020-11-16 22:13:09  [ pool-9-thread-1:3496 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:13:09  [ EventFetcher for fetching Map Completion Events:3497 ] - [ INFO ]  attempt_local1775655760_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:13:09  [ localfetcher#2:3497 ] - [ INFO ]  localfetcher#2 about to shuffle output of map attempt_local1775655760_0002_m_000000_0 decomp: 161 len: 165 to MEMORY
2020-11-16 22:13:09  [ localfetcher#2:3498 ] - [ INFO ]  Read 161 bytes from map-output for attempt_local1775655760_0002_m_000000_0
2020-11-16 22:13:09  [ localfetcher#2:3498 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 161, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->161
2020-11-16 22:13:09  [ EventFetcher for fetching Map Completion Events:3498 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:13:09  [ pool-9-thread-1:3498 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:13:09  [ pool-9-thread-1:3498 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:13:09  [ pool-9-thread-1:3499 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:13:09  [ pool-9-thread-1:3499 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 143 bytes
2020-11-16 22:13:09  [ pool-9-thread-1:3500 ] - [ INFO ]  Merged 1 segments, 161 bytes to disk to satisfy reduce memory limit
2020-11-16 22:13:09  [ pool-9-thread-1:3500 ] - [ INFO ]  Merging 1 files, 165 bytes from disk
2020-11-16 22:13:09  [ pool-9-thread-1:3500 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:13:09  [ pool-9-thread-1:3500 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:13:09  [ pool-9-thread-1:3500 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 143 bytes
2020-11-16 22:13:09  [ pool-9-thread-1:3501 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:13:09  [ pool-9-thread-1:3687 ] - [ INFO ]  Task:attempt_local1775655760_0002_r_000000_0 is done. And is in the process of committing
2020-11-16 22:13:09  [ pool-9-thread-1:3697 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:13:09  [ pool-9-thread-1:3697 ] - [ INFO ]  Task attempt_local1775655760_0002_r_000000_0 is allowed to commit now
2020-11-16 22:13:09  [ pool-9-thread-1:3754 ] - [ INFO ]  Saved output of task 'attempt_local1775655760_0002_r_000000_0' to hdfs://master:9000/user/root/mr/data/muti_mr/result/_temporary/0/task_local1775655760_0002_r_000000
2020-11-16 22:13:09  [ pool-9-thread-1:3754 ] - [ INFO ]  reduce > reduce
2020-11-16 22:13:09  [ pool-9-thread-1:3754 ] - [ INFO ]  Task 'attempt_local1775655760_0002_r_000000_0' done.
2020-11-16 22:13:09  [ pool-9-thread-1:3754 ] - [ INFO ]  Finishing task: attempt_local1775655760_0002_r_000000_0
2020-11-16 22:13:09  [ Thread-46:3754 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:13:10  [ main:4375 ] - [ INFO ]  Job job_local1775655760_0002 running in uber mode : false
2020-11-16 22:13:10  [ main:4375 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:13:10  [ main:4375 ] - [ INFO ]  Job job_local1775655760_0002 completed successfully
2020-11-16 22:13:10  [ main:4379 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1690
		FILE: Number of bytes written=1132019
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=600
		HDFS: Number of bytes written=594
		HDFS: Number of read operations=43
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=18
	Map-Reduce Framework
		Map input records=9
		Map output records=9
		Map output bytes=141
		Map output materialized bytes=165
		Input split bytes=130
		Combine input records=0
		Combine output records=0
		Reduce input groups=9
		Reduce shuffle bytes=165
		Reduce input records=9
		Reduce output records=9
		Spilled Records=18
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=840957952
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=141
	File Output Format Counters 
		Bytes Written=312
2020-11-16 22:54:18  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:54:19  [ main:554 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:54:19  [ main:555 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:54:19  [ main:760 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:54:19  [ main:766 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:54:19  [ main:779 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:54:19  [ main:851 ] - [ INFO ]  number of splits:1
2020-11-16 22:54:19  [ main:914 ] - [ INFO ]  Submitting tokens for job: job_local1000365914_0001
2020-11-16 22:54:19  [ main:1005 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:54:19  [ main:1005 ] - [ INFO ]  Running job: job_local1000365914_0001
2020-11-16 22:54:19  [ Thread-18:1006 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:54:19  [ Thread-18:1009 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:54:19  [ Thread-18:1010 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:54:19  [ Thread-18:1051 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:54:19  [ LocalJobRunner Map Task Executor #0:1051 ] - [ INFO ]  Starting task: attempt_local1000365914_0001_m_000000_0
2020-11-16 22:54:19  [ LocalJobRunner Map Task Executor #0:1068 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:54:19  [ LocalJobRunner Map Task Executor #0:1072 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:54:19  [ LocalJobRunner Map Task Executor #0:1072 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:54:19  [ LocalJobRunner Map Task Executor #0:1074 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/findFriend/friend.txt:0+66740
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1128 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1130 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1251 ] - [ INFO ]  
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1252 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1258 ] - [ INFO ]  Task:attempt_local1000365914_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1269 ] - [ INFO ]  map
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1269 ] - [ INFO ]  Task 'attempt_local1000365914_0001_m_000000_0' done.
2020-11-16 22:54:20  [ LocalJobRunner Map Task Executor #0:1269 ] - [ INFO ]  Finishing task: attempt_local1000365914_0001_m_000000_0
2020-11-16 22:54:20  [ Thread-18:1269 ] - [ INFO ]  map task executor complete.
2020-11-16 22:54:20  [ Thread-18:1271 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:54:20  [ pool-6-thread-1:1271 ] - [ INFO ]  Starting task: attempt_local1000365914_0001_r_000000_0
2020-11-16 22:54:20  [ pool-6-thread-1:1274 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:54:20  [ pool-6-thread-1:1275 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:54:20  [ pool-6-thread-1:1275 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:54:20  [ pool-6-thread-1:1276 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@59a1492b
2020-11-16 22:54:20  [ pool-6-thread-1:1284 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:54:20  [ EventFetcher for fetching Map Completion Events:1285 ] - [ INFO ]  attempt_local1000365914_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:54:20  [ localfetcher#1:1302 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1000365914_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
2020-11-16 22:54:20  [ localfetcher#1:1306 ] - [ INFO ]  Read 2 bytes from map-output for attempt_local1000365914_0001_m_000000_0
2020-11-16 22:54:20  [ localfetcher#1:1307 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
2020-11-16 22:54:20  [ EventFetcher for fetching Map Completion Events:1308 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:54:20  [ pool-6-thread-1:1308 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:54:20  [ pool-6-thread-1:1308 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:54:20  [ pool-6-thread-1:1312 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:54:20  [ pool-6-thread-1:1312 ] - [ INFO ]  Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2020-11-16 22:54:20  [ pool-6-thread-1:1313 ] - [ INFO ]  Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
2020-11-16 22:54:20  [ pool-6-thread-1:1313 ] - [ INFO ]  Merging 1 files, 6 bytes from disk
2020-11-16 22:54:20  [ pool-6-thread-1:1313 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:54:20  [ pool-6-thread-1:1314 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:54:20  [ pool-6-thread-1:1314 ] - [ INFO ]  Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2020-11-16 22:54:20  [ pool-6-thread-1:1314 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:54:20  [ pool-6-thread-1:1334 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:54:20  [ pool-6-thread-1:1347 ] - [ INFO ]  Task:attempt_local1000365914_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:54:20  [ pool-6-thread-1:1367 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:54:20  [ pool-6-thread-1:1367 ] - [ INFO ]  Task attempt_local1000365914_0001_r_000000_0 is allowed to commit now
2020-11-16 22:54:20  [ pool-6-thread-1:1393 ] - [ INFO ]  Saved output of task 'attempt_local1000365914_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/findFriend/result2/_temporary/0/task_local1000365914_0001_r_000000
2020-11-16 22:54:20  [ pool-6-thread-1:1393 ] - [ INFO ]  reduce > reduce
2020-11-16 22:54:20  [ pool-6-thread-1:1393 ] - [ INFO ]  Task 'attempt_local1000365914_0001_r_000000_0' done.
2020-11-16 22:54:20  [ pool-6-thread-1:1393 ] - [ INFO ]  Finishing task: attempt_local1000365914_0001_r_000000_0
2020-11-16 22:54:20  [ Thread-18:1394 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:54:20  [ main:2007 ] - [ INFO ]  Job job_local1000365914_0001 running in uber mode : false
2020-11-16 22:54:20  [ main:2008 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:54:20  [ main:2008 ] - [ INFO ]  Job job_local1000365914_0001 completed successfully
2020-11-16 22:54:20  [ main:2015 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=398
		FILE: Number of bytes written=568812
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=133480
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=4
	Map-Reduce Framework
		Map input records=1000
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=123
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=66740
	File Output Format Counters 
		Bytes Written=0
2020-11-16 22:55:01  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:55:01  [ main:731 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:55:01  [ main:732 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:55:02  [ main:948 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:55:02  [ main:953 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:55:02  [ main:969 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:55:02  [ main:1053 ] - [ INFO ]  number of splits:1
2020-11-16 22:55:02  [ main:1120 ] - [ INFO ]  Submitting tokens for job: job_local1578340310_0001
2020-11-16 22:55:02  [ main:1219 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:55:02  [ main:1219 ] - [ INFO ]  Running job: job_local1578340310_0001
2020-11-16 22:55:02  [ Thread-18:1220 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:55:02  [ Thread-18:1223 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:02  [ Thread-18:1225 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:55:02  [ Thread-18:1264 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1264 ] - [ INFO ]  Starting task: attempt_local1578340310_0001_m_000000_0
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1283 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1287 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1287 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1289 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/findFriend/friend.txt:0+66740
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1339 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1339 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1339 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1340 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1340 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1341 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1457 ] - [ INFO ]  
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1459 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1464 ] - [ INFO ]  Task:attempt_local1578340310_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1481 ] - [ INFO ]  map
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1481 ] - [ INFO ]  Task 'attempt_local1578340310_0001_m_000000_0' done.
2020-11-16 22:55:02  [ LocalJobRunner Map Task Executor #0:1481 ] - [ INFO ]  Finishing task: attempt_local1578340310_0001_m_000000_0
2020-11-16 22:55:02  [ Thread-18:1481 ] - [ INFO ]  map task executor complete.
2020-11-16 22:55:02  [ Thread-18:1483 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:55:02  [ pool-6-thread-1:1483 ] - [ INFO ]  Starting task: attempt_local1578340310_0001_r_000000_0
2020-11-16 22:55:02  [ pool-6-thread-1:1487 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:02  [ pool-6-thread-1:1488 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:55:02  [ pool-6-thread-1:1488 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:55:02  [ pool-6-thread-1:1490 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50001741
2020-11-16 22:55:02  [ pool-6-thread-1:1498 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:55:02  [ EventFetcher for fetching Map Completion Events:1500 ] - [ INFO ]  attempt_local1578340310_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:55:02  [ localfetcher#1:1520 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1578340310_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
2020-11-16 22:55:02  [ localfetcher#1:1524 ] - [ INFO ]  Read 2 bytes from map-output for attempt_local1578340310_0001_m_000000_0
2020-11-16 22:55:02  [ localfetcher#1:1525 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
2020-11-16 22:55:02  [ EventFetcher for fetching Map Completion Events:1526 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:55:02  [ pool-6-thread-1:1527 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:55:02  [ pool-6-thread-1:1527 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:55:02  [ pool-6-thread-1:1531 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:55:02  [ pool-6-thread-1:1531 ] - [ INFO ]  Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2020-11-16 22:55:02  [ pool-6-thread-1:1532 ] - [ INFO ]  Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
2020-11-16 22:55:02  [ pool-6-thread-1:1532 ] - [ INFO ]  Merging 1 files, 6 bytes from disk
2020-11-16 22:55:02  [ pool-6-thread-1:1533 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:55:02  [ pool-6-thread-1:1533 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:55:02  [ pool-6-thread-1:1533 ] - [ INFO ]  Down to the last merge-pass, with 0 segments left of total size: 0 bytes
2020-11-16 22:55:02  [ pool-6-thread-1:1533 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:55:02  [ pool-6-thread-1:1559 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:55:02  [ pool-6-thread-1:1574 ] - [ INFO ]  Task:attempt_local1578340310_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:55:02  [ pool-6-thread-1:1584 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:55:02  [ pool-6-thread-1:1584 ] - [ INFO ]  Task attempt_local1578340310_0001_r_000000_0 is allowed to commit now
2020-11-16 22:55:02  [ pool-6-thread-1:1616 ] - [ INFO ]  Saved output of task 'attempt_local1578340310_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/findFriend/result2/_temporary/0/task_local1578340310_0001_r_000000
2020-11-16 22:55:02  [ pool-6-thread-1:1616 ] - [ INFO ]  reduce > reduce
2020-11-16 22:55:02  [ pool-6-thread-1:1617 ] - [ INFO ]  Task 'attempt_local1578340310_0001_r_000000_0' done.
2020-11-16 22:55:02  [ pool-6-thread-1:1617 ] - [ INFO ]  Finishing task: attempt_local1578340310_0001_r_000000_0
2020-11-16 22:55:02  [ Thread-18:1617 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:55:03  [ main:2224 ] - [ INFO ]  Job job_local1578340310_0001 running in uber mode : false
2020-11-16 22:55:03  [ main:2225 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:55:03  [ main:2226 ] - [ INFO ]  Job job_local1578340310_0001 completed successfully
2020-11-16 22:55:03  [ main:2236 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=398
		FILE: Number of bytes written=568812
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=133480
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=1000
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=123
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=635437056
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=66740
	File Output Format Counters 
		Bytes Written=0
2020-11-16 22:55:31  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:55:32  [ main:733 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:55:32  [ main:734 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:55:32  [ main:950 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:55:32  [ main:956 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:55:32  [ main:971 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:55:32  [ main:1051 ] - [ INFO ]  number of splits:1
2020-11-16 22:55:32  [ main:1117 ] - [ INFO ]  Submitting tokens for job: job_local2069759594_0001
2020-11-16 22:55:32  [ main:1220 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:55:32  [ main:1221 ] - [ INFO ]  Running job: job_local2069759594_0001
2020-11-16 22:55:32  [ Thread-18:1221 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:55:32  [ Thread-18:1225 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:32  [ Thread-18:1226 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:55:32  [ Thread-18:1274 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1275 ] - [ INFO ]  Starting task: attempt_local2069759594_0001_m_000000_0
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1292 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1297 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1297 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1302 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/findFriend/friend.txt:0+66740
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1355 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1355 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1355 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1355 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1356 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:55:32  [ LocalJobRunner Map Task Executor #0:1357 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:55:44  [ main:13145 ] - [ INFO ]  Job job_local2069759594_0001 running in uber mode : false
2020-11-16 22:55:44  [ main:13147 ] - [ INFO ]   map 0% reduce 0%
2020-11-16 22:55:44  [ LocalJobRunner Map Task Executor #0:13158 ] - [ INFO ]  
2020-11-16 22:55:44  [ LocalJobRunner Map Task Executor #0:13159 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:55:44  [ LocalJobRunner Map Task Executor #0:13166 ] - [ INFO ]  Task:attempt_local2069759594_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:55:53  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 22:55:54  [ main:597 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 22:55:54  [ main:597 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 22:55:54  [ main:791 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 22:55:54  [ main:796 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 22:55:54  [ main:809 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 22:55:54  [ main:879 ] - [ INFO ]  number of splits:1
2020-11-16 22:55:54  [ main:943 ] - [ INFO ]  Submitting tokens for job: job_local1700734124_0001
2020-11-16 22:55:54  [ main:1043 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 22:55:54  [ main:1044 ] - [ INFO ]  Running job: job_local1700734124_0001
2020-11-16 22:55:54  [ Thread-18:1044 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 22:55:54  [ Thread-18:1048 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:54  [ Thread-18:1049 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 22:55:54  [ Thread-18:1089 ] - [ INFO ]  Waiting for map tasks
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1090 ] - [ INFO ]  Starting task: attempt_local1700734124_0001_m_000000_0
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1105 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1109 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1109 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1112 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/findFriend/friend.txt:0+66740
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1164 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1165 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1165 ] - [ INFO ]  soft limit at 83886080
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1165 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1165 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1166 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1285 ] - [ INFO ]  
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1286 ] - [ INFO ]  Starting flush of map output
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1286 ] - [ INFO ]  Spilling map output
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1286 ] - [ INFO ]  bufstart = 0; bufend = 459480; bufvoid = 104857600
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1286 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26153136(104612544); length = 61261/6553600
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1331 ] - [ INFO ]  Finished spill 0
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1334 ] - [ INFO ]  Task:attempt_local1700734124_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1346 ] - [ INFO ]  map
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1346 ] - [ INFO ]  Task 'attempt_local1700734124_0001_m_000000_0' done.
2020-11-16 22:55:54  [ LocalJobRunner Map Task Executor #0:1347 ] - [ INFO ]  Finishing task: attempt_local1700734124_0001_m_000000_0
2020-11-16 22:55:54  [ Thread-18:1347 ] - [ INFO ]  map task executor complete.
2020-11-16 22:55:54  [ Thread-18:1348 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 22:55:54  [ pool-6-thread-1:1348 ] - [ INFO ]  Starting task: attempt_local1700734124_0001_r_000000_0
2020-11-16 22:55:54  [ pool-6-thread-1:1353 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 22:55:54  [ pool-6-thread-1:1354 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 22:55:54  [ pool-6-thread-1:1354 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 22:55:54  [ pool-6-thread-1:1355 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@211057c3
2020-11-16 22:55:54  [ pool-6-thread-1:1364 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 22:55:54  [ EventFetcher for fetching Map Completion Events:1366 ] - [ INFO ]  attempt_local1700734124_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 22:55:54  [ localfetcher#1:1390 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1700734124_0001_m_000000_0 decomp: 490114 len: 490118 to MEMORY
2020-11-16 22:55:54  [ localfetcher#1:1395 ] - [ INFO ]  Read 490114 bytes from map-output for attempt_local1700734124_0001_m_000000_0
2020-11-16 22:55:54  [ localfetcher#1:1396 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 490114, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->490114
2020-11-16 22:55:54  [ EventFetcher for fetching Map Completion Events:1397 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 22:55:54  [ pool-6-thread-1:1398 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:55:54  [ pool-6-thread-1:1398 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 22:55:54  [ pool-6-thread-1:1403 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:55:54  [ pool-6-thread-1:1403 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 490092 bytes
2020-11-16 22:55:54  [ pool-6-thread-1:1420 ] - [ INFO ]  Merged 1 segments, 490114 bytes to disk to satisfy reduce memory limit
2020-11-16 22:55:54  [ pool-6-thread-1:1420 ] - [ INFO ]  Merging 1 files, 490118 bytes from disk
2020-11-16 22:55:54  [ pool-6-thread-1:1421 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 22:55:54  [ pool-6-thread-1:1421 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 22:55:54  [ pool-6-thread-1:1421 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 490092 bytes
2020-11-16 22:55:54  [ pool-6-thread-1:1422 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:55:54  [ pool-6-thread-1:1444 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 22:55:55  [ pool-6-thread-1:1677 ] - [ INFO ]  Task:attempt_local1700734124_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 22:55:55  [ pool-6-thread-1:1683 ] - [ INFO ]  1 / 1 copied.
2020-11-16 22:55:55  [ pool-6-thread-1:1683 ] - [ INFO ]  Task attempt_local1700734124_0001_r_000000_0 is allowed to commit now
2020-11-16 22:55:55  [ pool-6-thread-1:1705 ] - [ INFO ]  Saved output of task 'attempt_local1700734124_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/findFriend/result2/_temporary/0/task_local1700734124_0001_r_000000
2020-11-16 22:55:55  [ pool-6-thread-1:1706 ] - [ INFO ]  reduce > reduce
2020-11-16 22:55:55  [ pool-6-thread-1:1706 ] - [ INFO ]  Task 'attempt_local1700734124_0001_r_000000_0' done.
2020-11-16 22:55:55  [ pool-6-thread-1:1706 ] - [ INFO ]  Finishing task: attempt_local1700734124_0001_r_000000_0
2020-11-16 22:55:55  [ Thread-18:1706 ] - [ INFO ]  reduce task executor complete.
2020-11-16 22:55:55  [ main:2050 ] - [ INFO ]  Job job_local1700734124_0001 running in uber mode : false
2020-11-16 22:55:55  [ main:2051 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 22:55:55  [ main:2052 ] - [ INFO ]  Job job_local1700734124_0001 completed successfully
2020-11-16 22:55:55  [ main:2058 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=980622
		FILE: Number of bytes written=2039148
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=133480
		HDFS: Number of bytes written=369200
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=1000
		Map output records=15316
		Map output bytes=459480
		Map output materialized bytes=490118
		Input split bytes=123
		Combine input records=0
		Combine output records=0
		Reduce input groups=11004
		Reduce shuffle bytes=490118
		Reduce input records=15316
		Reduce output records=11004
		Spilled Records=30632
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=66740
	File Output Format Counters 
		Bytes Written=369200
2020-11-16 23:07:41  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 23:07:42  [ main:540 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 23:07:42  [ main:540 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 23:07:42  [ main:738 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 23:07:42  [ main:749 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 23:07:42  [ main:762 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 23:07:42  [ main:837 ] - [ INFO ]  number of splits:1
2020-11-16 23:07:42  [ main:902 ] - [ INFO ]  Submitting tokens for job: job_local2042881851_0001
2020-11-16 23:07:42  [ main:984 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 23:07:42  [ main:985 ] - [ INFO ]  Running job: job_local2042881851_0001
2020-11-16 23:07:42  [ Thread-18:985 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 23:07:42  [ Thread-18:988 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 23:07:42  [ Thread-18:989 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 23:07:42  [ Thread-18:1053 ] - [ INFO ]  Waiting for map tasks
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1053 ] - [ INFO ]  Starting task: attempt_local2042881851_0001_m_000000_0
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1068 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1072 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1073 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1075 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/findFriend/friedn_test.txt:0+142
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1126 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  soft limit at 83886080
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1127 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1129 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1211 ] - [ INFO ]  
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  Starting flush of map output
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  Spilling map output
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  bufstart = 0; bufend = 606; bufvoid = 104857600
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26213996(104855984); length = 401/6553600
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1220 ] - [ INFO ]  Finished spill 0
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1223 ] - [ INFO ]  Task:attempt_local2042881851_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1234 ] - [ INFO ]  map
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1234 ] - [ INFO ]  Task 'attempt_local2042881851_0001_m_000000_0' done.
2020-11-16 23:07:42  [ LocalJobRunner Map Task Executor #0:1234 ] - [ INFO ]  Finishing task: attempt_local2042881851_0001_m_000000_0
2020-11-16 23:07:42  [ Thread-18:1234 ] - [ INFO ]  map task executor complete.
2020-11-16 23:07:42  [ Thread-18:1236 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 23:07:42  [ pool-6-thread-1:1236 ] - [ INFO ]  Starting task: attempt_local2042881851_0001_r_000000_0
2020-11-16 23:07:42  [ pool-6-thread-1:1240 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 23:07:42  [ pool-6-thread-1:1241 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 23:07:42  [ pool-6-thread-1:1241 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 23:07:42  [ pool-6-thread-1:1243 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@a000c43
2020-11-16 23:07:42  [ pool-6-thread-1:1250 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 23:07:42  [ EventFetcher for fetching Map Completion Events:1251 ] - [ INFO ]  attempt_local2042881851_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 23:07:42  [ localfetcher#1:1270 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local2042881851_0001_m_000000_0 decomp: 810 len: 814 to MEMORY
2020-11-16 23:07:42  [ localfetcher#1:1274 ] - [ INFO ]  Read 810 bytes from map-output for attempt_local2042881851_0001_m_000000_0
2020-11-16 23:07:42  [ localfetcher#1:1275 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 810, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->810
2020-11-16 23:07:42  [ EventFetcher for fetching Map Completion Events:1276 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 23:07:42  [ pool-6-thread-1:1276 ] - [ INFO ]  1 / 1 copied.
2020-11-16 23:07:42  [ pool-6-thread-1:1276 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 23:07:42  [ pool-6-thread-1:1280 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 23:07:42  [ pool-6-thread-1:1280 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 804 bytes
2020-11-16 23:07:42  [ pool-6-thread-1:1282 ] - [ INFO ]  Merged 1 segments, 810 bytes to disk to satisfy reduce memory limit
2020-11-16 23:07:42  [ pool-6-thread-1:1282 ] - [ INFO ]  Merging 1 files, 814 bytes from disk
2020-11-16 23:07:42  [ pool-6-thread-1:1282 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 23:07:42  [ pool-6-thread-1:1282 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 23:07:42  [ pool-6-thread-1:1283 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 804 bytes
2020-11-16 23:07:42  [ pool-6-thread-1:1283 ] - [ INFO ]  1 / 1 copied.
2020-11-16 23:07:42  [ pool-6-thread-1:1302 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 23:07:43  [ pool-6-thread-1:1401 ] - [ INFO ]  Task:attempt_local2042881851_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 23:07:43  [ pool-6-thread-1:1407 ] - [ INFO ]  1 / 1 copied.
2020-11-16 23:07:43  [ pool-6-thread-1:1407 ] - [ INFO ]  Task attempt_local2042881851_0001_r_000000_0 is allowed to commit now
2020-11-16 23:07:43  [ pool-6-thread-1:1430 ] - [ INFO ]  Saved output of task 'attempt_local2042881851_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/findFriend/result2/_temporary/0/task_local2042881851_0001_r_000000
2020-11-16 23:07:43  [ pool-6-thread-1:1431 ] - [ INFO ]  reduce > reduce
2020-11-16 23:07:43  [ pool-6-thread-1:1431 ] - [ INFO ]  Task 'attempt_local2042881851_0001_r_000000_0' done.
2020-11-16 23:07:43  [ pool-6-thread-1:1431 ] - [ INFO ]  Finishing task: attempt_local2042881851_0001_r_000000_0
2020-11-16 23:07:43  [ Thread-18:1431 ] - [ INFO ]  reduce task executor complete.
2020-11-16 23:07:43  [ main:1988 ] - [ INFO ]  Job job_local2042881851_0001 running in uber mode : false
2020-11-16 23:07:43  [ main:1989 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 23:07:43  [ main:1990 ] - [ INFO ]  Job job_local2042881851_0001 completed successfully
2020-11-16 23:07:43  [ main:1997 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=2020
		FILE: Number of bytes written=571262
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=284
		HDFS: Number of bytes written=390
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=14
		Map output records=101
		Map output bytes=606
		Map output materialized bytes=814
		Input split bytes=128
		Combine input records=0
		Combine output records=0
		Reduce input groups=47
		Reduce shuffle bytes=814
		Reduce input records=101
		Reduce output records=47
		Spilled Records=202
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=142
	File Output Format Counters 
		Bytes Written=390
2020-11-16 23:10:48  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-16 23:10:48  [ main:555 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-16 23:10:48  [ main:556 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-16 23:10:48  [ main:741 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-16 23:10:48  [ main:746 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-16 23:10:48  [ main:761 ] - [ INFO ]  Total input paths to process : 1
2020-11-16 23:10:48  [ main:832 ] - [ INFO ]  number of splits:1
2020-11-16 23:10:48  [ main:890 ] - [ INFO ]  Submitting tokens for job: job_local1339555029_0001
2020-11-16 23:10:49  [ main:965 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-16 23:10:49  [ main:966 ] - [ INFO ]  Running job: job_local1339555029_0001
2020-11-16 23:10:49  [ Thread-18:966 ] - [ INFO ]  OutputCommitter set in config null
2020-11-16 23:10:49  [ Thread-18:969 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 23:10:49  [ Thread-18:970 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-16 23:10:49  [ Thread-18:1005 ] - [ INFO ]  Waiting for map tasks
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1005 ] - [ INFO ]  Starting task: attempt_local1339555029_0001_m_000000_0
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1018 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1021 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1022 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1023 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/findFriend/friedn_test.txt:0+142
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1069 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1069 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1069 ] - [ INFO ]  soft limit at 83886080
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1069 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1070 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1071 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1150 ] - [ INFO ]  
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]  Starting flush of map output
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]  Spilling map output
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]  bufstart = 0; bufend = 606; bufvoid = 104857600
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1151 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26213996(104855984); length = 401/6553600
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1159 ] - [ INFO ]  Finished spill 0
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  Task:attempt_local1339555029_0001_m_000000_0 is done. And is in the process of committing
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1176 ] - [ INFO ]  map
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1176 ] - [ INFO ]  Task 'attempt_local1339555029_0001_m_000000_0' done.
2020-11-16 23:10:49  [ LocalJobRunner Map Task Executor #0:1176 ] - [ INFO ]  Finishing task: attempt_local1339555029_0001_m_000000_0
2020-11-16 23:10:49  [ Thread-18:1176 ] - [ INFO ]  map task executor complete.
2020-11-16 23:10:49  [ Thread-18:1178 ] - [ INFO ]  Waiting for reduce tasks
2020-11-16 23:10:49  [ pool-6-thread-1:1178 ] - [ INFO ]  Starting task: attempt_local1339555029_0001_r_000000_0
2020-11-16 23:10:49  [ pool-6-thread-1:1182 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-16 23:10:49  [ pool-6-thread-1:1182 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-16 23:10:49  [ pool-6-thread-1:1183 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-16 23:10:49  [ pool-6-thread-1:1184 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@54dc89e9
2020-11-16 23:10:49  [ pool-6-thread-1:1191 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-16 23:10:49  [ EventFetcher for fetching Map Completion Events:1193 ] - [ INFO ]  attempt_local1339555029_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-16 23:10:49  [ localfetcher#1:1215 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1339555029_0001_m_000000_0 decomp: 810 len: 814 to MEMORY
2020-11-16 23:10:49  [ localfetcher#1:1220 ] - [ INFO ]  Read 810 bytes from map-output for attempt_local1339555029_0001_m_000000_0
2020-11-16 23:10:49  [ localfetcher#1:1221 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 810, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->810
2020-11-16 23:10:49  [ EventFetcher for fetching Map Completion Events:1222 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-16 23:10:49  [ pool-6-thread-1:1223 ] - [ INFO ]  1 / 1 copied.
2020-11-16 23:10:49  [ pool-6-thread-1:1223 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-16 23:10:49  [ pool-6-thread-1:1227 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 23:10:49  [ pool-6-thread-1:1227 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 804 bytes
2020-11-16 23:10:49  [ pool-6-thread-1:1229 ] - [ INFO ]  Merged 1 segments, 810 bytes to disk to satisfy reduce memory limit
2020-11-16 23:10:49  [ pool-6-thread-1:1229 ] - [ INFO ]  Merging 1 files, 814 bytes from disk
2020-11-16 23:10:49  [ pool-6-thread-1:1230 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-16 23:10:49  [ pool-6-thread-1:1230 ] - [ INFO ]  Merging 1 sorted segments
2020-11-16 23:10:49  [ pool-6-thread-1:1230 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 804 bytes
2020-11-16 23:10:49  [ pool-6-thread-1:1230 ] - [ INFO ]  1 / 1 copied.
2020-11-16 23:10:49  [ pool-6-thread-1:1255 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-16 23:10:49  [ pool-6-thread-1:1359 ] - [ INFO ]  Task:attempt_local1339555029_0001_r_000000_0 is done. And is in the process of committing
2020-11-16 23:10:49  [ pool-6-thread-1:1369 ] - [ INFO ]  1 / 1 copied.
2020-11-16 23:10:49  [ pool-6-thread-1:1370 ] - [ INFO ]  Task attempt_local1339555029_0001_r_000000_0 is allowed to commit now
2020-11-16 23:10:49  [ pool-6-thread-1:1409 ] - [ INFO ]  Saved output of task 'attempt_local1339555029_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/findFriend/result2/_temporary/0/task_local1339555029_0001_r_000000
2020-11-16 23:10:49  [ pool-6-thread-1:1410 ] - [ INFO ]  reduce > reduce
2020-11-16 23:10:49  [ pool-6-thread-1:1410 ] - [ INFO ]  Task 'attempt_local1339555029_0001_r_000000_0' done.
2020-11-16 23:10:49  [ pool-6-thread-1:1410 ] - [ INFO ]  Finishing task: attempt_local1339555029_0001_r_000000_0
2020-11-16 23:10:49  [ Thread-18:1410 ] - [ INFO ]  reduce task executor complete.
2020-11-16 23:10:50  [ main:1968 ] - [ INFO ]  Job job_local1339555029_0001 running in uber mode : false
2020-11-16 23:10:50  [ main:1969 ] - [ INFO ]   map 100% reduce 100%
2020-11-16 23:10:50  [ main:1970 ] - [ INFO ]  Job job_local1339555029_0001 completed successfully
2020-11-16 23:10:50  [ main:1976 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=2020
		FILE: Number of bytes written=571262
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=284
		HDFS: Number of bytes written=390
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=14
		Map output records=101
		Map output bytes=606
		Map output materialized bytes=814
		Input split bytes=128
		Combine input records=0
		Combine output records=0
		Reduce input groups=47
		Reduce shuffle bytes=814
		Reduce input records=101
		Reduce output records=47
		Spilled Records=202
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=142
	File Output Format Counters 
		Bytes Written=390
