2020-11-15 13:50:33  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-15 13:50:34  [ main:810 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-15 13:50:34  [ main:811 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-15 13:50:34  [ main:1001 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-15 13:50:34  [ main:1005 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-15 13:50:34  [ main:1017 ] - [ INFO ]  Total input paths to process : 1
2020-11-15 13:50:34  [ main:1104 ] - [ INFO ]  number of splits:1
2020-11-15 13:50:34  [ main:1179 ] - [ INFO ]  Submitting tokens for job: job_local893144635_0001
2020-11-15 13:50:34  [ main:1284 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-15 13:50:34  [ main:1284 ] - [ INFO ]  Running job: job_local893144635_0001
2020-11-15 13:50:34  [ Thread-18:1285 ] - [ INFO ]  OutputCommitter set in config null
2020-11-15 13:50:34  [ Thread-18:1289 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:50:34  [ Thread-18:1290 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-15 13:50:34  [ Thread-18:1324 ] - [ INFO ]  Waiting for map tasks
2020-11-15 13:50:34  [ LocalJobRunner Map Task Executor #0:1324 ] - [ INFO ]  Starting task: attempt_local893144635_0001_m_000000_0
2020-11-15 13:50:34  [ LocalJobRunner Map Task Executor #0:1344 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:50:34  [ LocalJobRunner Map Task Executor #0:1350 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:50:34  [ LocalJobRunner Map Task Executor #0:1350 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:50:34  [ LocalJobRunner Map Task Executor #0:1352 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/word_count/input.txt:0+120
2020-11-15 13:50:35  [ LocalJobRunner Map Task Executor #0:1416 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 13:50:35  [ LocalJobRunner Map Task Executor #0:1416 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 13:50:35  [ LocalJobRunner Map Task Executor #0:1416 ] - [ INFO ]  soft limit at 83886080
2020-11-15 13:50:35  [ LocalJobRunner Map Task Executor #0:1416 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 13:50:35  [ LocalJobRunner Map Task Executor #0:1416 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 13:50:35  [ LocalJobRunner Map Task Executor #0:1418 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 13:50:35  [ main:2289 ] - [ INFO ]  Job job_local893144635_0001 running in uber mode : false
2020-11-15 13:50:35  [ main:2290 ] - [ INFO ]   map 0% reduce 0%
2020-11-15 13:52:17  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-15 13:52:18  [ main:666 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-15 13:52:18  [ main:666 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-15 13:52:18  [ main:852 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-15 13:52:18  [ main:856 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-15 13:52:18  [ main:870 ] - [ INFO ]  Total input paths to process : 1
2020-11-15 13:52:18  [ main:952 ] - [ INFO ]  number of splits:1
2020-11-15 13:52:18  [ main:1012 ] - [ INFO ]  Submitting tokens for job: job_local233224313_0001
2020-11-15 13:52:19  [ main:1107 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-15 13:52:19  [ main:1107 ] - [ INFO ]  Running job: job_local233224313_0001
2020-11-15 13:52:19  [ Thread-18:1108 ] - [ INFO ]  OutputCommitter set in config null
2020-11-15 13:52:19  [ Thread-18:1111 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:52:19  [ Thread-18:1112 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-15 13:52:19  [ Thread-18:1146 ] - [ INFO ]  Waiting for map tasks
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1146 ] - [ INFO ]  Starting task: attempt_local233224313_0001_m_000000_0
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1161 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1164 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1165 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1167 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/word_count/input.txt:0+120
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  soft limit at 83886080
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1216 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 13:52:19  [ LocalJobRunner Map Task Executor #0:1218 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 13:52:20  [ main:2109 ] - [ INFO ]  Job job_local233224313_0001 running in uber mode : false
2020-11-15 13:52:20  [ main:2110 ] - [ INFO ]   map 0% reduce 0%
2020-11-15 13:52:57  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-15 13:52:57  [ main:932 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-15 13:52:57  [ main:932 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-15 13:52:58  [ main:1124 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-15 13:52:58  [ main:1129 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-15 13:52:58  [ main:1160 ] - [ INFO ]  Total input paths to process : 1
2020-11-15 13:52:58  [ main:1264 ] - [ INFO ]  number of splits:1
2020-11-15 13:52:58  [ main:1333 ] - [ INFO ]  Submitting tokens for job: job_local1308023190_0001
2020-11-15 13:52:58  [ main:1515 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-15 13:52:58  [ main:1515 ] - [ INFO ]  Running job: job_local1308023190_0001
2020-11-15 13:52:58  [ Thread-18:1516 ] - [ INFO ]  OutputCommitter set in config null
2020-11-15 13:52:58  [ Thread-18:1519 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:52:58  [ Thread-18:1521 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-15 13:52:58  [ Thread-18:1586 ] - [ INFO ]  Waiting for map tasks
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1586 ] - [ INFO ]  Starting task: attempt_local1308023190_0001_m_000000_0
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1601 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1605 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1605 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1608 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/word_count/input.txt:0+120
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1660 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1660 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1660 ] - [ INFO ]  soft limit at 83886080
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1660 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1660 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1662 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1908 ] - [ INFO ]  
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1910 ] - [ INFO ]  Starting flush of map output
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1910 ] - [ INFO ]  Spilling map output
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1910 ] - [ INFO ]  bufstart = 0; bufend = 280; bufvoid = 104857600
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1910 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214240(104856960); length = 157/6553600
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1918 ] - [ INFO ]  Finished spill 0
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1921 ] - [ INFO ]  Task:attempt_local1308023190_0001_m_000000_0 is done. And is in the process of committing
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1942 ] - [ INFO ]  map
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1942 ] - [ INFO ]  Task 'attempt_local1308023190_0001_m_000000_0' done.
2020-11-15 13:52:58  [ LocalJobRunner Map Task Executor #0:1942 ] - [ INFO ]  Finishing task: attempt_local1308023190_0001_m_000000_0
2020-11-15 13:52:58  [ Thread-18:1943 ] - [ INFO ]  map task executor complete.
2020-11-15 13:52:58  [ Thread-18:1945 ] - [ INFO ]  Waiting for reduce tasks
2020-11-15 13:52:58  [ pool-6-thread-1:1945 ] - [ INFO ]  Starting task: attempt_local1308023190_0001_r_000000_0
2020-11-15 13:52:58  [ pool-6-thread-1:1948 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:52:58  [ pool-6-thread-1:1949 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:52:58  [ pool-6-thread-1:1949 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:52:58  [ pool-6-thread-1:1952 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1359da15
2020-11-15 13:52:59  [ pool-6-thread-1:1962 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-15 13:52:59  [ EventFetcher for fetching Map Completion Events:1964 ] - [ INFO ]  attempt_local1308023190_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-15 13:52:59  [ localfetcher#1:1981 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1308023190_0001_m_000000_0 decomp: 362 len: 366 to MEMORY
2020-11-15 13:52:59  [ localfetcher#1:1985 ] - [ INFO ]  Read 362 bytes from map-output for attempt_local1308023190_0001_m_000000_0
2020-11-15 13:52:59  [ localfetcher#1:1986 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 362, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->362
2020-11-15 13:52:59  [ EventFetcher for fetching Map Completion Events:1987 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-15 13:52:59  [ pool-6-thread-1:1987 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:52:59  [ pool-6-thread-1:1987 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-15 13:52:59  [ pool-6-thread-1:1991 ] - [ INFO ]  Merging 1 sorted segments
2020-11-15 13:52:59  [ pool-6-thread-1:1992 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 357 bytes
2020-11-15 13:52:59  [ pool-6-thread-1:1992 ] - [ INFO ]  Merged 1 segments, 362 bytes to disk to satisfy reduce memory limit
2020-11-15 13:52:59  [ pool-6-thread-1:1993 ] - [ INFO ]  Merging 1 files, 366 bytes from disk
2020-11-15 13:52:59  [ pool-6-thread-1:1993 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-15 13:52:59  [ pool-6-thread-1:1993 ] - [ INFO ]  Merging 1 sorted segments
2020-11-15 13:52:59  [ pool-6-thread-1:1993 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 357 bytes
2020-11-15 13:52:59  [ pool-6-thread-1:1994 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:52:59  [ pool-6-thread-1:2033 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-15 13:52:59  [ pool-6-thread-1:2233 ] - [ INFO ]  Task:attempt_local1308023190_0001_r_000000_0 is done. And is in the process of committing
2020-11-15 13:52:59  [ pool-6-thread-1:2244 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:52:59  [ pool-6-thread-1:2244 ] - [ INFO ]  Task attempt_local1308023190_0001_r_000000_0 is allowed to commit now
2020-11-15 13:52:59  [ pool-6-thread-1:2489 ] - [ INFO ]  Saved output of task 'attempt_local1308023190_0001_r_000000_0' to hdfs://master:9000/user/root/mr/word_count/output.txt/_temporary/0/task_local1308023190_0001_r_000000
2020-11-15 13:52:59  [ pool-6-thread-1:2491 ] - [ INFO ]  reduce > reduce
2020-11-15 13:52:59  [ pool-6-thread-1:2491 ] - [ INFO ]  Task 'attempt_local1308023190_0001_r_000000_0' done.
2020-11-15 13:52:59  [ pool-6-thread-1:2491 ] - [ INFO ]  Finishing task: attempt_local1308023190_0001_r_000000_0
2020-11-15 13:52:59  [ Thread-18:2492 ] - [ INFO ]  reduce task executor complete.
2020-11-15 13:52:59  [ main:2519 ] - [ INFO ]  Job job_local1308023190_0001 running in uber mode : false
2020-11-15 13:52:59  [ main:2520 ] - [ INFO ]   map 100% reduce 100%
2020-11-15 13:53:00  [ main:3526 ] - [ INFO ]  Job job_local1308023190_0001 completed successfully
2020-11-15 13:53:00  [ main:3533 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1100
		FILE: Number of bytes written=569306
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=240
		HDFS: Number of bytes written=41
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=12
		Map output records=40
		Map output bytes=280
		Map output materialized bytes=366
		Input split bytes=117
		Combine input records=0
		Combine output records=0
		Reduce input groups=8
		Reduce shuffle bytes=366
		Reduce input records=40
		Reduce output records=8
		Spilled Records=80
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=6
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=120
	File Output Format Counters 
		Bytes Written=41
2020-11-15 13:54:48  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-15 13:54:49  [ main:962 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-15 13:54:49  [ main:963 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-15 13:54:50  [ main:1145 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-15 13:54:50  [ main:1150 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-15 13:54:50  [ main:1164 ] - [ INFO ]  Total input paths to process : 1
2020-11-15 13:54:50  [ main:1243 ] - [ INFO ]  number of splits:1
2020-11-15 13:54:50  [ main:1305 ] - [ INFO ]  Submitting tokens for job: job_local1416320526_0001
2020-11-15 13:54:50  [ main:1395 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-15 13:54:50  [ main:1395 ] - [ INFO ]  Running job: job_local1416320526_0001
2020-11-15 13:54:50  [ Thread-18:1395 ] - [ INFO ]  OutputCommitter set in config null
2020-11-15 13:54:50  [ Thread-18:1398 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:54:50  [ Thread-18:1399 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-15 13:54:50  [ Thread-18:1433 ] - [ INFO ]  Waiting for map tasks
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1433 ] - [ INFO ]  Starting task: attempt_local1416320526_0001_m_000000_0
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1452 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1456 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1456 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1458 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/word_count/input.txt:0+120
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1514 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1514 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1514 ] - [ INFO ]  soft limit at 83886080
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1514 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1514 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1516 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1664 ] - [ INFO ]  
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1667 ] - [ INFO ]  Starting flush of map output
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1668 ] - [ INFO ]  Spilling map output
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1668 ] - [ INFO ]  bufstart = 0; bufend = 280; bufvoid = 104857600
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1668 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214240(104856960); length = 157/6553600
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1689 ] - [ INFO ]  Finished spill 0
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1696 ] - [ INFO ]  Task:attempt_local1416320526_0001_m_000000_0 is done. And is in the process of committing
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1729 ] - [ INFO ]  map
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1729 ] - [ INFO ]  Task 'attempt_local1416320526_0001_m_000000_0' done.
2020-11-15 13:54:50  [ LocalJobRunner Map Task Executor #0:1729 ] - [ INFO ]  Finishing task: attempt_local1416320526_0001_m_000000_0
2020-11-15 13:54:50  [ Thread-18:1729 ] - [ INFO ]  map task executor complete.
2020-11-15 13:54:50  [ Thread-18:1731 ] - [ INFO ]  Waiting for reduce tasks
2020-11-15 13:54:50  [ pool-6-thread-1:1732 ] - [ INFO ]  Starting task: attempt_local1416320526_0001_r_000000_0
2020-11-15 13:54:50  [ pool-6-thread-1:1737 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:54:50  [ pool-6-thread-1:1737 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:54:50  [ pool-6-thread-1:1737 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:54:50  [ pool-6-thread-1:1739 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b78fec3
2020-11-15 13:54:50  [ pool-6-thread-1:1747 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-15 13:54:50  [ EventFetcher for fetching Map Completion Events:1749 ] - [ INFO ]  attempt_local1416320526_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-15 13:54:50  [ localfetcher#1:1769 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1416320526_0001_m_000000_0 decomp: 362 len: 366 to MEMORY
2020-11-15 13:54:50  [ localfetcher#1:1774 ] - [ INFO ]  Read 362 bytes from map-output for attempt_local1416320526_0001_m_000000_0
2020-11-15 13:54:50  [ localfetcher#1:1775 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 362, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->362
2020-11-15 13:54:50  [ EventFetcher for fetching Map Completion Events:1775 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-15 13:54:50  [ pool-6-thread-1:1776 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:54:50  [ pool-6-thread-1:1776 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-15 13:54:50  [ pool-6-thread-1:1780 ] - [ INFO ]  Merging 1 sorted segments
2020-11-15 13:54:50  [ pool-6-thread-1:1780 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 357 bytes
2020-11-15 13:54:50  [ pool-6-thread-1:1781 ] - [ INFO ]  Merged 1 segments, 362 bytes to disk to satisfy reduce memory limit
2020-11-15 13:54:50  [ pool-6-thread-1:1781 ] - [ INFO ]  Merging 1 files, 366 bytes from disk
2020-11-15 13:54:50  [ pool-6-thread-1:1782 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-15 13:54:50  [ pool-6-thread-1:1782 ] - [ INFO ]  Merging 1 sorted segments
2020-11-15 13:54:50  [ pool-6-thread-1:1782 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 357 bytes
2020-11-15 13:54:50  [ pool-6-thread-1:1782 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:54:50  [ pool-6-thread-1:1807 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-15 13:54:50  [ pool-6-thread-1:1964 ] - [ INFO ]  Task:attempt_local1416320526_0001_r_000000_0 is done. And is in the process of committing
2020-11-15 13:54:50  [ pool-6-thread-1:1981 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:54:50  [ pool-6-thread-1:1982 ] - [ INFO ]  Task attempt_local1416320526_0001_r_000000_0 is allowed to commit now
2020-11-15 13:54:50  [ pool-6-thread-1:2018 ] - [ INFO ]  Saved output of task 'attempt_local1416320526_0001_r_000000_0' to hdfs://master:9000/user/root/mr/word_count/output.txt/_temporary/0/task_local1416320526_0001_r_000000
2020-11-15 13:54:50  [ pool-6-thread-1:2019 ] - [ INFO ]  reduce > reduce
2020-11-15 13:54:50  [ pool-6-thread-1:2019 ] - [ INFO ]  Task 'attempt_local1416320526_0001_r_000000_0' done.
2020-11-15 13:54:50  [ pool-6-thread-1:2019 ] - [ INFO ]  Finishing task: attempt_local1416320526_0001_r_000000_0
2020-11-15 13:54:50  [ Thread-18:2019 ] - [ INFO ]  reduce task executor complete.
2020-11-15 13:54:51  [ main:2397 ] - [ INFO ]  Job job_local1416320526_0001 running in uber mode : false
2020-11-15 13:54:51  [ main:2399 ] - [ INFO ]   map 100% reduce 100%
2020-11-15 13:54:51  [ main:2399 ] - [ INFO ]  Job job_local1416320526_0001 completed successfully
2020-11-15 13:54:51  [ main:2407 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=1100
		FILE: Number of bytes written=569306
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=240
		HDFS: Number of bytes written=41
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=12
		Map output records=40
		Map output bytes=280
		Map output materialized bytes=366
		Input split bytes=117
		Combine input records=0
		Combine output records=0
		Reduce input groups=8
		Reduce shuffle bytes=366
		Reduce input records=40
		Reduce output records=8
		Spilled Records=80
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=10
		Total committed heap usage (bytes)=641728512
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=120
	File Output Format Counters 
		Bytes Written=41
2020-11-15 13:56:52  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-15 13:56:53  [ main:1098 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-15 13:56:53  [ main:1100 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-15 13:56:53  [ main:1344 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-15 13:56:53  [ main:1350 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-15 13:56:53  [ main:1370 ] - [ INFO ]  Total input paths to process : 1
2020-11-15 13:56:53  [ main:1456 ] - [ INFO ]  number of splits:1
2020-11-15 13:56:53  [ main:1613 ] - [ INFO ]  Submitting tokens for job: job_local159759063_0001
2020-11-15 13:56:53  [ main:1728 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-15 13:56:53  [ main:1728 ] - [ INFO ]  Running job: job_local159759063_0001
2020-11-15 13:56:53  [ Thread-18:1729 ] - [ INFO ]  OutputCommitter set in config null
2020-11-15 13:56:53  [ Thread-18:1732 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:56:53  [ Thread-18:1733 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-15 13:56:53  [ Thread-18:1773 ] - [ INFO ]  Waiting for map tasks
2020-11-15 13:56:53  [ LocalJobRunner Map Task Executor #0:1774 ] - [ INFO ]  Starting task: attempt_local159759063_0001_m_000000_0
2020-11-15 13:56:53  [ LocalJobRunner Map Task Executor #0:1789 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:56:53  [ LocalJobRunner Map Task Executor #0:1795 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:56:53  [ LocalJobRunner Map Task Executor #0:1795 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:56:53  [ LocalJobRunner Map Task Executor #0:1797 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/avro/avro.txt:0+326
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:1920 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:1921 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:1921 ] - [ INFO ]  soft limit at 83886080
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:1921 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:1921 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:1933 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2052 ] - [ INFO ]  
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2053 ] - [ INFO ]  Starting flush of map output
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2053 ] - [ INFO ]  Spilling map output
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2053 ] - [ INFO ]  bufstart = 0; bufend = 448; bufvoid = 104857600
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2053 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214344(104857376); length = 53/6553600
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2064 ] - [ INFO ]  Starting flush of map output
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2064 ] - [ INFO ]  (RESET) equator 0 kv 26214396(104857584) kvi 26214340(104857360)
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2064 ] - [ INFO ]  Spilling map output
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2064 ] - [ INFO ]  bufstart = 0; bufend = 448; bufvoid = 104857600
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2064 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214344(104857376); length = 53/6553600
2020-11-15 13:56:54  [ LocalJobRunner Map Task Executor #0:2065 ] - [ INFO ]  Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@430a43a6
java.lang.RuntimeException: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.ConsumerBO.<init>()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:134)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
	at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:146)
	at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1688)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1637)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1489)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)
	at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:2019)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:797)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.ConsumerBO.<init>()
	at java.lang.Class.getConstructor0(Class.java:3082)
	at java.lang.Class.getDeclaredConstructor(Class.java:2178)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:128)
	... 19 more
2020-11-15 13:56:54  [ Thread-18:2068 ] - [ INFO ]  map task executor complete.
2020-11-15 13:56:54  [ Thread-18:2078 ] - [ WARN ]  job_local159759063_0001
java.lang.Exception: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.ConsumerBO.<init>()
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.ConsumerBO.<init>()
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:134)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:42)
	at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:146)
	at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1688)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1637)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1489)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.NoSuchMethodException: com.satan.hadoop.model.result.ConsumerBO.<init>()
	at java.lang.Class.getConstructor0(Class.java:3082)
	at java.lang.Class.getDeclaredConstructor(Class.java:2178)
	at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:128)
	... 18 more
2020-11-15 13:56:54  [ main:2734 ] - [ INFO ]  Job job_local159759063_0001 running in uber mode : false
2020-11-15 13:56:54  [ main:2735 ] - [ INFO ]   map 0% reduce 0%
2020-11-15 13:56:54  [ main:2737 ] - [ INFO ]  Job job_local159759063_0001 failed with state FAILED due to: NA
2020-11-15 13:56:54  [ main:2746 ] - [ INFO ]  Counters: 11
	Map-Reduce Framework
		Map input records=14
		Map output records=14
		Map output bytes=448
		Map output materialized bytes=0
		Input split bytes=115
		Combine input records=0
		Combine output records=0
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
	File Input Format Counters 
		Bytes Read=326
2020-11-15 13:57:45  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-15 13:57:46  [ main:683 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-15 13:57:46  [ main:684 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-15 13:57:46  [ main:869 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-15 13:57:46  [ main:874 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-15 13:57:46  [ main:888 ] - [ INFO ]  Total input paths to process : 1
2020-11-15 13:57:46  [ main:968 ] - [ INFO ]  number of splits:1
2020-11-15 13:57:47  [ main:1033 ] - [ INFO ]  Submitting tokens for job: job_local1574115557_0001
2020-11-15 13:57:47  [ main:1142 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-15 13:57:47  [ main:1143 ] - [ INFO ]  Running job: job_local1574115557_0001
2020-11-15 13:57:47  [ Thread-18:1143 ] - [ INFO ]  OutputCommitter set in config null
2020-11-15 13:57:47  [ Thread-18:1146 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:57:47  [ Thread-18:1148 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-15 13:57:47  [ Thread-18:1185 ] - [ INFO ]  Waiting for map tasks
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1185 ] - [ INFO ]  Starting task: attempt_local1574115557_0001_m_000000_0
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1206 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1211 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1211 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1213 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/data/avro/avro.txt:0+326
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  soft limit at 83886080
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1263 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1265 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1341 ] - [ INFO ]  
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1342 ] - [ INFO ]  Starting flush of map output
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1342 ] - [ INFO ]  Spilling map output
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1343 ] - [ INFO ]  bufstart = 0; bufend = 448; bufvoid = 104857600
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1343 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214344(104857376); length = 53/6553600
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1352 ] - [ INFO ]  Finished spill 0
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1357 ] - [ INFO ]  Task:attempt_local1574115557_0001_m_000000_0 is done. And is in the process of committing
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1373 ] - [ INFO ]  map
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1374 ] - [ INFO ]  Task 'attempt_local1574115557_0001_m_000000_0' done.
2020-11-15 13:57:47  [ LocalJobRunner Map Task Executor #0:1374 ] - [ INFO ]  Finishing task: attempt_local1574115557_0001_m_000000_0
2020-11-15 13:57:47  [ Thread-18:1374 ] - [ INFO ]  map task executor complete.
2020-11-15 13:57:47  [ Thread-18:1376 ] - [ INFO ]  Waiting for reduce tasks
2020-11-15 13:57:47  [ pool-6-thread-1:1376 ] - [ INFO ]  Starting task: attempt_local1574115557_0001_r_000000_0
2020-11-15 13:57:47  [ pool-6-thread-1:1382 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 13:57:47  [ pool-6-thread-1:1384 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 13:57:47  [ pool-6-thread-1:1385 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 13:57:47  [ pool-6-thread-1:1388 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@57ce0d4
2020-11-15 13:57:47  [ pool-6-thread-1:1403 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-15 13:57:47  [ EventFetcher for fetching Map Completion Events:1405 ] - [ INFO ]  attempt_local1574115557_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-15 13:57:47  [ localfetcher#1:1453 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local1574115557_0001_m_000000_0 decomp: 104 len: 108 to MEMORY
2020-11-15 13:57:47  [ localfetcher#1:1466 ] - [ INFO ]  Read 104 bytes from map-output for attempt_local1574115557_0001_m_000000_0
2020-11-15 13:57:47  [ localfetcher#1:1469 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 104, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->104
2020-11-15 13:57:47  [ EventFetcher for fetching Map Completion Events:1472 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-15 13:57:47  [ pool-6-thread-1:1475 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:57:47  [ pool-6-thread-1:1476 ] - [ INFO ]  finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
2020-11-15 13:57:47  [ pool-6-thread-1:1487 ] - [ INFO ]  Merging 1 sorted segments
2020-11-15 13:57:47  [ pool-6-thread-1:1487 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 99 bytes
2020-11-15 13:57:47  [ pool-6-thread-1:1488 ] - [ INFO ]  Merged 1 segments, 104 bytes to disk to satisfy reduce memory limit
2020-11-15 13:57:47  [ pool-6-thread-1:1489 ] - [ INFO ]  Merging 1 files, 108 bytes from disk
2020-11-15 13:57:47  [ pool-6-thread-1:1489 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-15 13:57:47  [ pool-6-thread-1:1489 ] - [ INFO ]  Merging 1 sorted segments
2020-11-15 13:57:47  [ pool-6-thread-1:1490 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 99 bytes
2020-11-15 13:57:47  [ pool-6-thread-1:1490 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:57:48  [ pool-6-thread-1:2068 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-15 13:57:48  [ main:2144 ] - [ INFO ]  Job job_local1574115557_0001 running in uber mode : false
2020-11-15 13:57:48  [ main:2146 ] - [ INFO ]   map 100% reduce 0%
2020-11-15 13:57:48  [ pool-6-thread-1:2255 ] - [ INFO ]  Task:attempt_local1574115557_0001_r_000000_0 is done. And is in the process of committing
2020-11-15 13:57:48  [ pool-6-thread-1:2265 ] - [ INFO ]  1 / 1 copied.
2020-11-15 13:57:48  [ pool-6-thread-1:2265 ] - [ INFO ]  Task attempt_local1574115557_0001_r_000000_0 is allowed to commit now
2020-11-15 13:57:48  [ pool-6-thread-1:2299 ] - [ INFO ]  Saved output of task 'attempt_local1574115557_0001_r_000000_0' to hdfs://master:9000/user/root/mr/data/avro/result/_temporary/0/task_local1574115557_0001_r_000000
2020-11-15 13:57:48  [ pool-6-thread-1:2300 ] - [ INFO ]  reduce > reduce
2020-11-15 13:57:48  [ pool-6-thread-1:2300 ] - [ INFO ]  Task 'attempt_local1574115557_0001_r_000000_0' done.
2020-11-15 13:57:48  [ pool-6-thread-1:2300 ] - [ INFO ]  Finishing task: attempt_local1574115557_0001_r_000000_0
2020-11-15 13:57:48  [ Thread-18:2301 ] - [ INFO ]  reduce task executor complete.
2020-11-15 13:57:49  [ main:3147 ] - [ INFO ]   map 100% reduce 100%
2020-11-15 13:57:49  [ main:3148 ] - [ INFO ]  Job job_local1574115557_0001 completed successfully
2020-11-15 13:57:49  [ main:3154 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=584
		FILE: Number of bytes written=569448
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=652
		HDFS: Number of bytes written=226
		HDFS: Number of read operations=17
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=6
	Map-Reduce Framework
		Map input records=14
		Map output records=14
		Map output bytes=448
		Map output materialized bytes=108
		Input split bytes=115
		Combine input records=14
		Combine output records=3
		Reduce input groups=3
		Reduce shuffle bytes=108
		Reduce input records=3
		Reduce output records=3
		Spilled Records=6
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=326
	File Output Format Counters 
		Bytes Written=226
2020-11-15 20:39:01  [ main:0 ] - [ WARN ]  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2020-11-15 20:39:01  [ main:818 ] - [ INFO ]  session.id is deprecated. Instead, use dfs.metrics.session-id
2020-11-15 20:39:01  [ main:819 ] - [ INFO ]  Initializing JVM Metrics with processName=JobTracker, sessionId=
2020-11-15 20:39:02  [ main:1005 ] - [ WARN ]  Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
2020-11-15 20:39:02  [ main:1009 ] - [ WARN ]  No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
2020-11-15 20:39:02  [ main:1065 ] - [ INFO ]  Total input paths to process : 2
2020-11-15 20:39:02  [ main:1114 ] - [ INFO ]  number of splits:2
2020-11-15 20:39:02  [ main:1188 ] - [ INFO ]  Submitting tokens for job: job_local2025906751_0001
2020-11-15 20:39:02  [ main:1294 ] - [ INFO ]  The url to track the job: http://localhost:8080/
2020-11-15 20:39:02  [ main:1295 ] - [ INFO ]  Running job: job_local2025906751_0001
2020-11-15 20:39:02  [ Thread-18:1295 ] - [ INFO ]  OutputCommitter set in config null
2020-11-15 20:39:02  [ Thread-18:1300 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 20:39:02  [ Thread-18:1302 ] - [ INFO ]  OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
2020-11-15 20:39:02  [ Thread-18:1344 ] - [ INFO ]  Waiting for map tasks
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1345 ] - [ INFO ]  Starting task: attempt_local2025906751_0001_m_000000_0
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1362 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1368 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1368 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1370 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/word_count/input/input.txt:0+120
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  soft limit at 83886080
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1420 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1422 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1530 ] - [ INFO ]  
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1533 ] - [ INFO ]  Starting flush of map output
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1533 ] - [ INFO ]  Spilling map output
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1534 ] - [ INFO ]  bufstart = 0; bufend = 280; bufvoid = 104857600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1534 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214240(104856960); length = 157/6553600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1548 ] - [ INFO ]  Finished spill 0
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1556 ] - [ INFO ]  Task:attempt_local2025906751_0001_m_000000_0 is done. And is in the process of committing
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1576 ] - [ INFO ]  map
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1576 ] - [ INFO ]  Task 'attempt_local2025906751_0001_m_000000_0' done.
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1576 ] - [ INFO ]  Finishing task: attempt_local2025906751_0001_m_000000_0
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1577 ] - [ INFO ]  Starting task: attempt_local2025906751_0001_m_000001_0
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1579 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1580 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1580 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1582 ] - [ INFO ]  Processing split: hdfs://master:9000/user/root/mr/word_count/input/input1.txt:0+26
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1654 ] - [ INFO ]  (EQUATOR) 0 kvi 26214396(104857584)
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1654 ] - [ INFO ]  mapreduce.task.io.sort.mb: 100
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1654 ] - [ INFO ]  soft limit at 83886080
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1655 ] - [ INFO ]  bufstart = 0; bufvoid = 104857600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1655 ] - [ INFO ]  kvstart = 26214396; length = 6553600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1655 ] - [ INFO ]  Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1680 ] - [ INFO ]  
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1680 ] - [ INFO ]  Starting flush of map output
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1680 ] - [ INFO ]  Spilling map output
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1680 ] - [ INFO ]  bufstart = 0; bufend = 46; bufvoid = 104857600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1681 ] - [ INFO ]  kvstart = 26214396(104857584); kvend = 26214380(104857520); length = 17/6553600
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1682 ] - [ INFO ]  Finished spill 0
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1683 ] - [ INFO ]  Task:attempt_local2025906751_0001_m_000001_0 is done. And is in the process of committing
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1696 ] - [ INFO ]  map
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1697 ] - [ INFO ]  Task 'attempt_local2025906751_0001_m_000001_0' done.
2020-11-15 20:39:02  [ LocalJobRunner Map Task Executor #0:1697 ] - [ INFO ]  Finishing task: attempt_local2025906751_0001_m_000001_0
2020-11-15 20:39:02  [ Thread-18:1697 ] - [ INFO ]  map task executor complete.
2020-11-15 20:39:02  [ Thread-18:1699 ] - [ INFO ]  Waiting for reduce tasks
2020-11-15 20:39:02  [ pool-6-thread-1:1699 ] - [ INFO ]  Starting task: attempt_local2025906751_0001_r_000000_0
2020-11-15 20:39:02  [ pool-6-thread-1:1703 ] - [ INFO ]  File Output Committer Algorithm version is 1
2020-11-15 20:39:02  [ pool-6-thread-1:1703 ] - [ INFO ]  ProcfsBasedProcessTree currently is supported only on Linux.
2020-11-15 20:39:02  [ pool-6-thread-1:1703 ] - [ INFO ]   Using ResourceCalculatorProcessTree : null
2020-11-15 20:39:02  [ pool-6-thread-1:1706 ] - [ INFO ]  Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@58ecc1c
2020-11-15 20:39:02  [ pool-6-thread-1:1714 ] - [ INFO ]  MergerManager: memoryLimit=2672505600, maxSingleShuffleLimit=668126400, mergeThreshold=1763853824, ioSortFactor=10, memToMemMergeOutputsThreshold=10
2020-11-15 20:39:02  [ EventFetcher for fetching Map Completion Events:1716 ] - [ INFO ]  attempt_local2025906751_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
2020-11-15 20:39:02  [ localfetcher#1:1735 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local2025906751_0001_m_000000_0 decomp: 362 len: 366 to MEMORY
2020-11-15 20:39:02  [ localfetcher#1:1743 ] - [ INFO ]  Read 362 bytes from map-output for attempt_local2025906751_0001_m_000000_0
2020-11-15 20:39:02  [ localfetcher#1:1745 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 362, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->362
2020-11-15 20:39:02  [ localfetcher#1:1750 ] - [ INFO ]  localfetcher#1 about to shuffle output of map attempt_local2025906751_0001_m_000001_0 decomp: 58 len: 62 to MEMORY
2020-11-15 20:39:02  [ localfetcher#1:1751 ] - [ INFO ]  Read 58 bytes from map-output for attempt_local2025906751_0001_m_000001_0
2020-11-15 20:39:02  [ localfetcher#1:1751 ] - [ INFO ]  closeInMemoryFile -> map-output of size: 58, inMemoryMapOutputs.size() -> 2, commitMemory -> 362, usedMemory ->420
2020-11-15 20:39:02  [ EventFetcher for fetching Map Completion Events:1752 ] - [ INFO ]  EventFetcher is interrupted.. Returning
2020-11-15 20:39:02  [ pool-6-thread-1:1753 ] - [ INFO ]  2 / 2 copied.
2020-11-15 20:39:02  [ pool-6-thread-1:1753 ] - [ INFO ]  finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs
2020-11-15 20:39:02  [ pool-6-thread-1:1763 ] - [ INFO ]  Merging 2 sorted segments
2020-11-15 20:39:02  [ pool-6-thread-1:1763 ] - [ INFO ]  Down to the last merge-pass, with 2 segments left of total size: 407 bytes
2020-11-15 20:39:02  [ pool-6-thread-1:1766 ] - [ INFO ]  Merged 2 segments, 420 bytes to disk to satisfy reduce memory limit
2020-11-15 20:39:02  [ pool-6-thread-1:1767 ] - [ INFO ]  Merging 1 files, 422 bytes from disk
2020-11-15 20:39:02  [ pool-6-thread-1:1768 ] - [ INFO ]  Merging 0 segments, 0 bytes from memory into reduce
2020-11-15 20:39:02  [ pool-6-thread-1:1768 ] - [ INFO ]  Merging 1 sorted segments
2020-11-15 20:39:02  [ pool-6-thread-1:1768 ] - [ INFO ]  Down to the last merge-pass, with 1 segments left of total size: 413 bytes
2020-11-15 20:39:02  [ pool-6-thread-1:1769 ] - [ INFO ]  2 / 2 copied.
2020-11-15 20:39:02  [ pool-6-thread-1:1809 ] - [ INFO ]  mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
2020-11-15 20:39:03  [ pool-6-thread-1:1991 ] - [ INFO ]  Task:attempt_local2025906751_0001_r_000000_0 is done. And is in the process of committing
2020-11-15 20:39:03  [ pool-6-thread-1:2005 ] - [ INFO ]  2 / 2 copied.
2020-11-15 20:39:03  [ pool-6-thread-1:2005 ] - [ INFO ]  Task attempt_local2025906751_0001_r_000000_0 is allowed to commit now
2020-11-15 20:39:03  [ pool-6-thread-1:2039 ] - [ INFO ]  Saved output of task 'attempt_local2025906751_0001_r_000000_0' to hdfs://master:9000/user/root/mr/word_count/output/_temporary/0/task_local2025906751_0001_r_000000
2020-11-15 20:39:03  [ pool-6-thread-1:2040 ] - [ INFO ]  reduce > reduce
2020-11-15 20:39:03  [ pool-6-thread-1:2040 ] - [ INFO ]  Task 'attempt_local2025906751_0001_r_000000_0' done.
2020-11-15 20:39:03  [ pool-6-thread-1:2040 ] - [ INFO ]  Finishing task: attempt_local2025906751_0001_r_000000_0
2020-11-15 20:39:03  [ Thread-18:2041 ] - [ INFO ]  reduce task executor complete.
2020-11-15 20:39:03  [ main:2301 ] - [ INFO ]  Job job_local2025906751_0001 running in uber mode : false
2020-11-15 20:39:03  [ main:2303 ] - [ INFO ]   map 100% reduce 100%
2020-11-15 20:39:03  [ main:2305 ] - [ INFO ]  Job job_local2025906751_0001 completed successfully
2020-11-15 20:39:03  [ main:2316 ] - [ INFO ]  Counters: 35
	File System Counters
		FILE: Number of bytes read=2373
		FILE: Number of bytes written=854395
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=412
		HDFS: Number of bytes written=77
		HDFS: Number of read operations=28
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=8
	Map-Reduce Framework
		Map input records=14
		Map output records=45
		Map output bytes=326
		Map output materialized bytes=428
		Input split bytes=247
		Combine input records=0
		Combine output records=0
		Reduce input groups=13
		Reduce shuffle bytes=428
		Reduce input records=45
		Reduce output records=13
		Spilled Records=90
		Shuffled Maps =2
		Failed Shuffles=0
		Merged Map outputs=2
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=1263009792
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=146
	File Output Format Counters 
		Bytes Written=77
